{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dbd8b-23f4-42f3-a332-4d7438f5f656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e78e629b-e202-465b-8128-cee4474d0cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre‑encoding semua sampel …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28000/28000 [00:02<00:00, 9476.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "════ FOLD 1/5 ════\n",
      "[Fold 1][Epoch 1] TL: 0.8875 | VL: 0.6864 | TA: 0.6675 | VA: 0.7498\n",
      "[Fold 1][Epoch 2] TL: 0.5396 | VL: 0.5824 | TA: 0.8171 | VA: 0.7941\n",
      "[Fold 1][Epoch 3] TL: 0.4140 | VL: 0.3921 | TA: 0.8582 | VA: 0.8677\n",
      "[Fold 1][Epoch 4] TL: 0.3408 | VL: 0.4136 | TA: 0.8824 | VA: 0.8623\n",
      "[Fold 1][Epoch 5] TL: 0.2933 | VL: 0.3507 | TA: 0.9011 | VA: 0.8834\n",
      "[Fold 1][Epoch 6] TL: 0.2580 | VL: 0.3499 | TA: 0.9139 | VA: 0.8854\n",
      "[Fold 1][Epoch 7] TL: 0.2210 | VL: 0.2894 | TA: 0.9258 | VA: 0.9109\n",
      "[Fold 1][Epoch 8] TL: 0.1973 | VL: 0.2977 | TA: 0.9332 | VA: 0.9073\n",
      "[Fold 1][Epoch 9] TL: 0.1732 | VL: 0.2568 | TA: 0.9413 | VA: 0.9202\n",
      "[Fold 1][Epoch 10] TL: 0.1483 | VL: 0.2657 | TA: 0.9505 | VA: 0.9189\n",
      "[Fold 1][Epoch 11] TL: 0.1300 | VL: 0.2606 | TA: 0.9575 | VA: 0.9229\n",
      "[Fold 1][Epoch 12] TL: 0.1163 | VL: 0.2181 | TA: 0.9612 | VA: 0.9395\n",
      "[Fold 1][Epoch 13] TL: 0.1003 | VL: 0.2223 | TA: 0.9667 | VA: 0.9387\n",
      "[Fold 1][Epoch 14] TL: 0.0894 | VL: 0.2448 | TA: 0.9717 | VA: 0.9373\n",
      "[Fold 1][Epoch 15] TL: 0.0779 | VL: 0.2433 | TA: 0.9749 | VA: 0.9386\n",
      "[Fold 1][Epoch 16] TL: 0.0661 | VL: 0.2307 | TA: 0.9801 | VA: 0.9427\n",
      "[Fold 1][Epoch 17] TL: 0.0594 | VL: 0.2341 | TA: 0.9820 | VA: 0.9430\n",
      "[Fold 1][Epoch 18] TL: 0.0528 | VL: 0.2413 | TA: 0.9844 | VA: 0.9439\n",
      "[Fold 1][Epoch 19] TL: 0.0495 | VL: 0.2382 | TA: 0.9854 | VA: 0.9452\n",
      "[Fold 1][Epoch 20] TL: 0.0462 | VL: 0.2387 | TA: 0.9867 | VA: 0.9459\n",
      "\n",
      "════ FOLD 2/5 ════\n",
      "[Fold 2][Epoch 1] TL: 0.9181 | VL: 0.6789 | TA: 0.6498 | VA: 0.7725\n",
      "[Fold 2][Epoch 2] TL: 0.5301 | VL: 0.5187 | TA: 0.8158 | VA: 0.8193\n",
      "[Fold 2][Epoch 3] TL: 0.3969 | VL: 0.3665 | TA: 0.8664 | VA: 0.8800\n",
      "[Fold 2][Epoch 4] TL: 0.3285 | VL: 0.3186 | TA: 0.8883 | VA: 0.8902\n",
      "[Fold 2][Epoch 5] TL: 0.2902 | VL: 0.3271 | TA: 0.9012 | VA: 0.8875\n",
      "[Fold 2][Epoch 6] TL: 0.2522 | VL: 0.2966 | TA: 0.9140 | VA: 0.9032\n",
      "[Fold 2][Epoch 7] TL: 0.2227 | VL: 0.3100 | TA: 0.9257 | VA: 0.9002\n",
      "[Fold 2][Epoch 8] TL: 0.1983 | VL: 0.3083 | TA: 0.9332 | VA: 0.8993\n",
      "[Fold 2][Epoch 9] TL: 0.1739 | VL: 0.3315 | TA: 0.9423 | VA: 0.9002\n",
      "[Fold 2][Epoch 10] TL: 0.1568 | VL: 0.2508 | TA: 0.9491 | VA: 0.9229\n",
      "[Fold 2][Epoch 11] TL: 0.1397 | VL: 0.2310 | TA: 0.9530 | VA: 0.9254\n",
      "[Fold 2][Epoch 12] TL: 0.1200 | VL: 0.2567 | TA: 0.9596 | VA: 0.9259\n",
      "[Fold 2][Epoch 13] TL: 0.1018 | VL: 0.2224 | TA: 0.9667 | VA: 0.9354\n",
      "[Fold 2][Epoch 14] TL: 0.0895 | VL: 0.2155 | TA: 0.9713 | VA: 0.9407\n",
      "[Fold 2][Epoch 15] TL: 0.0778 | VL: 0.2183 | TA: 0.9746 | VA: 0.9384\n",
      "[Fold 2][Epoch 16] TL: 0.0681 | VL: 0.2326 | TA: 0.9787 | VA: 0.9373\n",
      "[Fold 2][Epoch 17] TL: 0.0603 | VL: 0.2181 | TA: 0.9813 | VA: 0.9427\n",
      "[Fold 2][Epoch 18] TL: 0.0540 | VL: 0.2250 | TA: 0.9832 | VA: 0.9409\n",
      "[Fold 2][Epoch 19] TL: 0.0502 | VL: 0.2233 | TA: 0.9850 | VA: 0.9414\n",
      "[Fold 2][Epoch 20] TL: 0.0476 | VL: 0.2231 | TA: 0.9854 | VA: 0.9409\n",
      "\n",
      "════ FOLD 3/5 ════\n",
      "[Fold 3][Epoch 1] TL: 0.8889 | VL: 0.6174 | TA: 0.6654 | VA: 0.7804\n",
      "[Fold 3][Epoch 2] TL: 0.5205 | VL: 0.4370 | TA: 0.8205 | VA: 0.8568\n",
      "[Fold 3][Epoch 3] TL: 0.3983 | VL: 0.3984 | TA: 0.8629 | VA: 0.8680\n",
      "[Fold 3][Epoch 4] TL: 0.3367 | VL: 0.3825 | TA: 0.8856 | VA: 0.8705\n",
      "[Fold 3][Epoch 5] TL: 0.2876 | VL: 0.3371 | TA: 0.9031 | VA: 0.8891\n",
      "[Fold 3][Epoch 6] TL: 0.2517 | VL: 0.2841 | TA: 0.9151 | VA: 0.9050\n",
      "[Fold 3][Epoch 7] TL: 0.2249 | VL: 0.2427 | TA: 0.9257 | VA: 0.9207\n",
      "[Fold 3][Epoch 8] TL: 0.1977 | VL: 0.2346 | TA: 0.9337 | VA: 0.9250\n",
      "[Fold 3][Epoch 9] TL: 0.1717 | VL: 0.2379 | TA: 0.9430 | VA: 0.9220\n",
      "[Fold 3][Epoch 10] TL: 0.1532 | VL: 0.1990 | TA: 0.9486 | VA: 0.9404\n",
      "[Fold 3][Epoch 11] TL: 0.1334 | VL: 0.2214 | TA: 0.9558 | VA: 0.9329\n",
      "[Fold 3][Epoch 12] TL: 0.1169 | VL: 0.1985 | TA: 0.9619 | VA: 0.9416\n",
      "[Fold 3][Epoch 13] TL: 0.1020 | VL: 0.1880 | TA: 0.9657 | VA: 0.9457\n",
      "[Fold 3][Epoch 14] TL: 0.0899 | VL: 0.1851 | TA: 0.9708 | VA: 0.9459\n",
      "[Fold 3][Epoch 15] TL: 0.0766 | VL: 0.1927 | TA: 0.9756 | VA: 0.9496\n",
      "[Fold 3][Epoch 16] TL: 0.0671 | VL: 0.2018 | TA: 0.9795 | VA: 0.9500\n",
      "[Fold 3][Epoch 17] TL: 0.0585 | VL: 0.1939 | TA: 0.9824 | VA: 0.9514\n",
      "[Fold 3][Epoch 18] TL: 0.0526 | VL: 0.2010 | TA: 0.9845 | VA: 0.9511\n",
      "[Fold 3][Epoch 19] TL: 0.0485 | VL: 0.1957 | TA: 0.9863 | VA: 0.9530\n",
      "[Fold 3][Epoch 20] TL: 0.0464 | VL: 0.1939 | TA: 0.9868 | VA: 0.9523\n",
      "\n",
      "════ FOLD 4/5 ════\n",
      "[Fold 4][Epoch 1] TL: 0.8846 | VL: 0.5518 | TA: 0.6669 | VA: 0.8118\n",
      "[Fold 4][Epoch 2] TL: 0.5330 | VL: 0.5640 | TA: 0.8139 | VA: 0.8066\n",
      "[Fold 4][Epoch 3] TL: 0.3972 | VL: 0.4142 | TA: 0.8618 | VA: 0.8623\n",
      "[Fold 4][Epoch 4] TL: 0.3360 | VL: 0.3110 | TA: 0.8874 | VA: 0.8977\n",
      "[Fold 4][Epoch 5] TL: 0.2884 | VL: 0.3054 | TA: 0.9021 | VA: 0.8991\n",
      "[Fold 4][Epoch 6] TL: 0.2630 | VL: 0.2773 | TA: 0.9131 | VA: 0.9091\n",
      "[Fold 4][Epoch 7] TL: 0.2277 | VL: 0.2851 | TA: 0.9234 | VA: 0.9041\n",
      "[Fold 4][Epoch 8] TL: 0.1973 | VL: 0.3273 | TA: 0.9324 | VA: 0.8952\n",
      "[Fold 4][Epoch 9] TL: 0.1811 | VL: 0.2636 | TA: 0.9393 | VA: 0.9114\n",
      "[Fold 4][Epoch 10] TL: 0.1561 | VL: 0.2301 | TA: 0.9478 | VA: 0.9275\n",
      "[Fold 4][Epoch 11] TL: 0.1338 | VL: 0.2546 | TA: 0.9556 | VA: 0.9241\n",
      "[Fold 4][Epoch 12] TL: 0.1186 | VL: 0.2012 | TA: 0.9610 | VA: 0.9421\n",
      "[Fold 4][Epoch 13] TL: 0.1006 | VL: 0.2075 | TA: 0.9679 | VA: 0.9386\n",
      "[Fold 4][Epoch 14] TL: 0.0891 | VL: 0.2126 | TA: 0.9718 | VA: 0.9407\n",
      "[Fold 4][Epoch 15] TL: 0.0777 | VL: 0.1889 | TA: 0.9752 | VA: 0.9487\n",
      "[Fold 4][Epoch 16] TL: 0.0676 | VL: 0.1926 | TA: 0.9787 | VA: 0.9477\n",
      "[Fold 4][Epoch 17] TL: 0.0589 | VL: 0.2014 | TA: 0.9819 | VA: 0.9482\n",
      "[Fold 4][Epoch 18] TL: 0.0542 | VL: 0.2005 | TA: 0.9842 | VA: 0.9459\n",
      "[Fold 4][Epoch 19] TL: 0.0494 | VL: 0.1995 | TA: 0.9851 | VA: 0.9475\n",
      "[Fold 4][Epoch 20] TL: 0.0472 | VL: 0.1968 | TA: 0.9864 | VA: 0.9487\n",
      "\n",
      "════ FOLD 5/5 ════\n",
      "[Fold 5][Epoch 1] TL: 0.8815 | VL: 0.6597 | TA: 0.6710 | VA: 0.7573\n",
      "[Fold 5][Epoch 2] TL: 0.5205 | VL: 0.4242 | TA: 0.8213 | VA: 0.8616\n",
      "[Fold 5][Epoch 3] TL: 0.4003 | VL: 0.4271 | TA: 0.8659 | VA: 0.8521\n",
      "[Fold 5][Epoch 4] TL: 0.3403 | VL: 0.3646 | TA: 0.8849 | VA: 0.8777\n",
      "[Fold 5][Epoch 5] TL: 0.2904 | VL: 0.3400 | TA: 0.9010 | VA: 0.8859\n",
      "[Fold 5][Epoch 6] TL: 0.2568 | VL: 0.2629 | TA: 0.9135 | VA: 0.9104\n",
      "[Fold 5][Epoch 7] TL: 0.2271 | VL: 0.2670 | TA: 0.9221 | VA: 0.9154\n",
      "[Fold 5][Epoch 8] TL: 0.2031 | VL: 0.2787 | TA: 0.9311 | VA: 0.9118\n",
      "[Fold 5][Epoch 9] TL: 0.1765 | VL: 0.2160 | TA: 0.9405 | VA: 0.9323\n",
      "[Fold 5][Epoch 10] TL: 0.1528 | VL: 0.2384 | TA: 0.9494 | VA: 0.9305\n",
      "[Fold 5][Epoch 11] TL: 0.1368 | VL: 0.2232 | TA: 0.9540 | VA: 0.9359\n",
      "[Fold 5][Epoch 12] TL: 0.1169 | VL: 0.2490 | TA: 0.9614 | VA: 0.9302\n",
      "[Fold 5][Epoch 13] TL: 0.1018 | VL: 0.2178 | TA: 0.9655 | VA: 0.9425\n",
      "[Fold 5][Epoch 14] TL: 0.0868 | VL: 0.2134 | TA: 0.9703 | VA: 0.9425\n",
      "[Fold 5][Epoch 15] TL: 0.0742 | VL: 0.2277 | TA: 0.9760 | VA: 0.9466\n",
      "[Fold 5][Epoch 16] TL: 0.0646 | VL: 0.2319 | TA: 0.9798 | VA: 0.9429\n",
      "[Fold 5][Epoch 17] TL: 0.0590 | VL: 0.2123 | TA: 0.9817 | VA: 0.9475\n",
      "[Fold 5][Epoch 18] TL: 0.0511 | VL: 0.2207 | TA: 0.9848 | VA: 0.9463\n",
      "[Fold 5][Epoch 19] TL: 0.0484 | VL: 0.2178 | TA: 0.9858 | VA: 0.9486\n",
      "[Fold 5][Epoch 20] TL: 0.0463 | VL: 0.2190 | TA: 0.9863 | VA: 0.9473\n"
     ]
    }
   ],
   "source": [
    "# encoder_only_kfold_tuning.py – versi 1x training per fold, tuning dilakukan per fold\n",
    "\n",
    "import os, glob, gc, copy, math, random, time, json\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ====== KONFIGURASI ======\n",
    "DATA_DIR    = \"/workspace/SPLIT_BEATS_NPY/train\"\n",
    "LABEL_MAP   = {'N':0,'L':1,'R':2,'V':3,'Q':4}\n",
    "SEED        = 42\n",
    "N_SPLITS    = 5\n",
    "EPOCHS      = 20\n",
    "MAX_LEN     = 512\n",
    "EMB_DIM     = 512\n",
    "N_HEADS     = 8\n",
    "FF_DIM      = 2048\n",
    "N_LAYERS    = 12\n",
    "GRID = [{\"lr\": 2e-5, \"batch_size\": 32}]  # hanya 1 kombinasi agar train 1x/fold\n",
    "OUTPUT_BASE = \"/workspace/HASIL_ENCODER_Tuned/HASIL_1\"\n",
    "\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_ID, CLS_ID  = 256, 257\n",
    "VOCAB_SIZE      = 258\n",
    "cls_names       = list(LABEL_MAP.keys())\n",
    "\n",
    "# ====== UTIL ======\n",
    "def signal_to_ids(sig):\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    ids  = np.concatenate(([CLS_ID], norm))[:MAX_LEN]\n",
    "    mask = np.ones_like(ids)\n",
    "    if len(ids) < MAX_LEN:\n",
    "        pad_len = MAX_LEN - len(ids)\n",
    "        ids  = np.concatenate((ids,  np.full(pad_len, PAD_ID)))\n",
    "        mask = np.concatenate((mask, np.zeros(pad_len)))\n",
    "    return ids, mask\n",
    "\n",
    "# ====== MODEL DEFINITION ======\n",
    "class EncoderOnlyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_ID)\n",
    "        self.pos_emb   = nn.Parameter(torch.zeros(1, MAX_LEN, EMB_DIM))\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=EMB_DIM, nhead=N_HEADS, dim_feedforward=FF_DIM, dropout=0.1, batch_first=True)\n",
    "        self.encoder  = nn.TransformerEncoder(enc_layer, num_layers=N_LAYERS)\n",
    "        self.fc       = nn.Linear(EMB_DIM, len(LABEL_MAP))\n",
    "        nn.init.normal_(self.pos_emb, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.token_emb(input_ids) + self.pos_emb\n",
    "        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
    "        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1e-9)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ====== LOAD DATA ======\n",
    "files, labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    for f in glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\")):\n",
    "        files.append(f); labels.append(idx)\n",
    "files, labels = np.array(files), np.array(labels)\n",
    "\n",
    "print(\"Pre‑encoding semua sampel …\")\n",
    "all_ids, all_mask = [], []\n",
    "for f in tqdm(files):\n",
    "    ids, msk = signal_to_ids(np.load(f))\n",
    "    all_ids.append(ids); all_mask.append(msk)\n",
    "all_ids  = torch.tensor(all_ids,  dtype=torch.long)\n",
    "all_mask = torch.tensor(all_mask, dtype=torch.long)\n",
    "labels_t = torch.tensor(labels,   dtype=torch.long)\n",
    "\n",
    "# ====== K-FOLD TRAINING ======\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "rows_all = []; fold_val_losses = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(all_ids, labels), 1):\n",
    "    print(f\"\\n════ FOLD {fold}/{N_SPLITS} ════\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    \n",
    "    best_loss = math.inf; best_state = None; best_history = None\n",
    "    for hp in GRID:\n",
    "        lr, bs = hp['lr'], hp['batch_size']\n",
    "        train_ds = TensorDataset(all_ids[tr], all_mask[tr], labels_t[tr])\n",
    "        val_ds   = TensorDataset(all_ids[va], all_mask[va], labels_t[va])\n",
    "        train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "        val_loader   = DataLoader(val_ds, batch_size=bs)\n",
    "\n",
    "        model = EncoderOnlyClassifier().to(DEVICE)\n",
    "        optim = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, EPOCHS)\n",
    "\n",
    "        history = []\n",
    "        for epoch in range(1, EPOCHS + 1):\n",
    "            model.train(); tot_loss = 0; correct = 0; total = 0\n",
    "            for ids, msk, lbl in train_loader:\n",
    "                ids, msk, lbl = ids.to(DEVICE), msk.to(DEVICE), lbl.to(DEVICE)\n",
    "                optim.zero_grad(); out = model(ids, msk)\n",
    "                loss = F.cross_entropy(out, lbl)\n",
    "                loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); optim.step()\n",
    "                tot_loss += loss.item() * len(lbl)\n",
    "                correct += (out.argmax(1) == lbl).sum().item(); total += len(lbl)\n",
    "            scheduler.step()\n",
    "            train_loss = tot_loss / total\n",
    "            train_acc  = correct / total\n",
    "\n",
    "            model.eval(); val_loss, correct, total = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for ids, msk, lbl in val_loader:\n",
    "                    ids, msk, lbl = ids.to(DEVICE), msk.to(DEVICE), lbl.to(DEVICE)\n",
    "                    out = model(ids, msk)\n",
    "                    val_loss += F.cross_entropy(out, lbl, reduction='sum').item()\n",
    "                    correct += (out.argmax(1) == lbl).sum().item(); total += len(lbl)\n",
    "            val_loss /= total\n",
    "            val_acc  = correct / total\n",
    "\n",
    "            history.append({\"epoch\": epoch, \"train_loss\": train_loss, \"val_loss\": val_loss, \"train_acc\": train_acc, \"val_acc\": val_acc})\n",
    "            print(f\"[Fold {fold}][Epoch {epoch}] TL: {train_loss:.4f} | VL: {val_loss:.4f} | TA: {train_acc:.4f} | VA: {val_acc:.4f}\")\n",
    "\n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                best_history = history.copy()\n",
    "\n",
    "    # Simpan hasil terbaik fold ini\n",
    "    out_dir = os.path.join(OUTPUT_BASE, f\"fold{fold}\"); os.makedirs(out_dir, exist_ok=True)\n",
    "    torch.save(best_state, os.path.join(out_dir, \"best_model.pt\"))\n",
    "    with open(os.path.join(out_dir, \"model_config.json\"), \"w\") as f:\n",
    "        json.dump({\"emb_dim\": EMB_DIM, \"n_layers\": N_LAYERS, \"n_heads\": N_HEADS, \"ff_dim\": FF_DIM, \"max_len\": MAX_LEN,\n",
    "                   \"vocab_size\": VOCAB_SIZE, \"pad_id\": PAD_ID, \"cls_id\": CLS_ID, \"label_map\": LABEL_MAP}, f, indent=2)\n",
    "    with open(os.path.join(out_dir, \"vocab.txt\"), \"w\") as f:\n",
    "        f.writelines([f\"{i}\\n\" for i in range(256)] + [\"[PAD]\\n\", \"[CLS]\\n\"])\n",
    "    pd.DataFrame(best_history).to_csv(os.path.join(out_dir, \"history_epoch.csv\"), index=False)\n",
    "\n",
    "    # Evaluasi dan simpan confusion matrix\n",
    "    model.load_state_dict(best_state); model.eval()\n",
    "    val_loader = DataLoader(val_ds, batch_size=bs)\n",
    "    logits, y_true = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids, msk, lbl in val_loader:\n",
    "            ids, msk = ids.to(DEVICE), msk.to(DEVICE)\n",
    "            logits.append(model(ids, msk).cpu()); y_true.append(lbl)\n",
    "    preds = torch.cat(logits).argmax(1).numpy(); y_true = torch.cat(y_true).numpy()\n",
    "    cm = confusion_matrix(y_true, preds, labels=list(range(len(cls_names))))\n",
    "    plt.figure(figsize=(6,5)); plt.imshow(cm, cmap='Blues'); plt.title(f\"Confusion Fold {fold}\")\n",
    "    plt.xticks(range(len(cls_names)), cls_names); plt.yticks(range(len(cls_names)), cls_names)\n",
    "    for r in range(len(cm)):\n",
    "        for c in range(len(cm)):\n",
    "            plt.text(c, r, cm[r,c], ha='center', va='center')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_fold.png\")); plt.close()\n",
    "\n",
    "    # Simpan metrik per kelas\n",
    "    for i, cls in enumerate(cls_names):\n",
    "        TP = cm[i,i]; FN = cm[i].sum()-TP; FP = cm[:,i].sum()-TP; TN = cm.sum()-TP-FN-FP\n",
    "        ACC = (TP+TN)/cm.sum(); REC = TP/(TP+FN+1e-8); SPEC = TN/(TN+FP+1e-8); F1 = 2*TP/(2*TP+FP+FN+1e-8)\n",
    "        rows_all.append({\"fold\": fold, \"kelas\": cls, \"akurasi\": round(ACC,4), \"f1\": round(F1,4), \"recall\": round(REC,4), \"spesifisitas\": round(SPEC,4)})\n",
    "    fold_val_losses.append((fold, best_loss))\n",
    "\n",
    "# Simpan metrik akhir\n",
    "out_df = pd.DataFrame(rows_all)\n",
    "out_df.to_csv(os.path.join(OUTPUT_BASE, \"final_summary_perkelas.csv\"), index=False)\n",
    "best_fold = sorted(fold_val_losses, key=lambda x: x[1])[0][0]\n",
    "out_df[out_df.fold == best_fold].to_csv(os.path.join(OUTPUT_BASE, \"final_summary_best_only.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6ab71a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:505: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\NestedTensorImpl.cpp:182.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] Accuracy: 0.8546 | F1-macro: 0.8555 | F1-weighted: 0.8555\n",
      "\n",
      "[TEST] Evaluasi selesai.\n",
      "- Confusion matrix: D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\HASIL_Encoder_Beats_tuning\\HASIL_Encoder_Beats_tuning\\fold3\\TEST_EVAL\\confmat_test.png\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_io.TextIOWrapper' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 212\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[TEST] Evaluasi selesai.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Confusion matrix: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfmat_test.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 212\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m- CM normalized: \u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43mos.path.join(OUTPUT_DIR, \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfmat_test_normalized.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m)}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m )\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Per-kelas CSV : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_per_class_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- Ringkasan JSON: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics_summary_test.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: '_io.TextIOWrapper' object is not callable"
     ]
    }
   ],
   "source": [
    "# test_encoder_only.py\n",
    "# Uji model EncoderOnlyClassifier pada set uji, hasil: confusion matrix & metrik per-kelas\n",
    "import os, glob, json, math, copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ========== KONFIGURASI ========== \n",
    "MODEL_DIR     = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\HASIL_Encoder_Beats_tuning\\HASIL_Encoder_Beats_tuning\\fold3\"   # ganti ke fold terbaik Anda\n",
    "TEST_DIR      = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\DATA_UJI_INFERENCE\\Beats_TEST\"                # root data uji, berisi subfolder kelas\n",
    "OUTPUT_DIR    = os.path.join(MODEL_DIR, \"TEST_EVAL\")             # hasil evaluasi uji\n",
    "BATCH_SIZE    = 64\n",
    "DEVICE        = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPS           = 1e-8\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ========== BACA KONFIGURASI MODEL ========== \n",
    "config_path = os.path.join(MODEL_DIR, \"model_config.json\")\n",
    "state_path  = os.path.join(MODEL_DIR, \"best_model.pt\")\n",
    "\n",
    "assert os.path.isfile(config_path), f\"Tidak menemukan model_config.json di {config_path}\"\n",
    "assert os.path.isfile(state_path),  f\"Tidak menemukan best_model.pt di {state_path}\"\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    CFG = json.load(f)\n",
    "\n",
    "EMB_DIM    = CFG[\"emb_dim\"]\n",
    "N_LAYERS   = CFG[\"n_layers\"]\n",
    "N_HEADS    = CFG[\"n_heads\"]\n",
    "FF_DIM     = CFG[\"ff_dim\"]\n",
    "MAX_LEN    = CFG[\"max_len\"]\n",
    "VOCAB_SIZE = CFG[\"vocab_size\"]\n",
    "PAD_ID     = CFG[\"pad_id\"]\n",
    "CLS_ID     = CFG[\"cls_id\"]\n",
    "LABEL_MAP  = CFG[\"label_map\"]  # dict nama->id seperti {'N':0,'L':1,'R':2,'V':3,'Q':4}\n",
    "\n",
    "# urutan nama kelas untuk konsistensi tampilan & confusion_matrix\n",
    "CLS_NAMES = list(LABEL_MAP.keys())\n",
    "NUM_CLASSES = len(LABEL_MAP)\n",
    "\n",
    "# ========== UTIL: ENCODING SAMA DENGAN TRAIN ==========\n",
    "def signal_to_ids(sig: np.ndarray):\n",
    "    sig = np.asarray(sig).astype(np.float32)\n",
    "    # Normalisasi ke [0,255] sesuai train\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + EPS) * 255).astype(np.int64)\n",
    "    ids  = np.concatenate(([CLS_ID], norm))[:MAX_LEN]\n",
    "    mask = np.ones_like(ids, dtype=np.int64)\n",
    "    if ids.shape[0] < MAX_LEN:\n",
    "        pad_len = MAX_LEN - ids.shape[0]\n",
    "        ids  = np.concatenate((ids,  np.full(pad_len, PAD_ID, dtype=np.int64)))\n",
    "        mask = np.concatenate((mask, np.zeros(pad_len, dtype=np.int64)))\n",
    "    return ids, mask\n",
    "\n",
    "# ========== DEFINISI MODEL HARUS IDENTIK ==========\n",
    "class EncoderOnlyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_ID)\n",
    "        self.pos_emb   = nn.Parameter(torch.zeros(1, MAX_LEN, EMB_DIM))\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=EMB_DIM, nhead=N_HEADS, dim_feedforward=FF_DIM,\n",
    "            dropout=0.1, batch_first=True\n",
    "        )\n",
    "        self.encoder  = nn.TransformerEncoder(enc_layer, num_layers=N_LAYERS)\n",
    "        self.fc       = nn.Linear(EMB_DIM, NUM_CLASSES)\n",
    "        # Positional embedding inisialisasi spt training\n",
    "        nn.init.normal_(self.pos_emb, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.token_emb(input_ids) + self.pos_emb\n",
    "        x = self.encoder(x, src_key_padding_mask=~attention_mask.bool())\n",
    "        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1e-9)\n",
    "        return self.fc(x)\n",
    "\n",
    "# ========== MUAT MODEL ==========\n",
    "model = EncoderOnlyClassifier().to(DEVICE)\n",
    "state = torch.load(state_path, map_location=DEVICE)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "\n",
    "# ========== SIAPKAN DATA UJI ==========\n",
    "test_files, test_labels = [], []\n",
    "for cls_name, cls_id in LABEL_MAP.items():\n",
    "    pattern = os.path.join(TEST_DIR, cls_name, \"*.npy\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    test_files.extend(files)\n",
    "    test_labels.extend([cls_id] * len(files))\n",
    "\n",
    "test_files = np.array(test_files)\n",
    "test_labels = np.array(test_labels, dtype=np.int64)\n",
    "\n",
    "assert len(test_files) > 0, f\"Tidak ada berkas .npy ditemukan di {TEST_DIR} (cek struktur TEST_DIR/<kelas>/*.npy).\"\n",
    "\n",
    "# Pre-encode\n",
    "all_ids, all_mask = [], []\n",
    "for f in test_files:\n",
    "    sig = np.load(f)\n",
    "    ids, msk = signal_to_ids(sig)\n",
    "    all_ids.append(ids); all_mask.append(msk)\n",
    "\n",
    "all_ids  = torch.tensor(np.stack(all_ids),  dtype=torch.long)\n",
    "all_mask = torch.tensor(np.stack(all_mask), dtype=torch.long)\n",
    "y_true_t = torch.tensor(test_labels,       dtype=torch.long)\n",
    "\n",
    "test_ds = TensorDataset(all_ids, all_mask, y_true_t)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# ========== INFERENSI ==========\n",
    "logits_list, y_true_list = [], []\n",
    "with torch.no_grad():\n",
    "    for ids, msk, lbl in test_loader:\n",
    "        ids, msk = ids.to(DEVICE), msk.to(DEVICE)\n",
    "        out = model(ids, msk)\n",
    "        logits_list.append(out.cpu())\n",
    "        y_true_list.append(lbl)\n",
    "\n",
    "logits = torch.cat(logits_list, dim=0)\n",
    "y_true = torch.cat(y_true_list, dim=0).numpy()\n",
    "y_pred = logits.argmax(dim=1).numpy()\n",
    "\n",
    "# ========== METRIK GLOBAL ==========\n",
    "acc  = accuracy_score(y_true, y_pred)\n",
    "f1_m = f1_score(y_true, y_pred, average=\"macro\")\n",
    "f1_w = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"[TEST] Accuracy: {acc:.4f} | F1-macro: {f1_m:.4f} | F1-weighted: {f1_w:.4f}\")\n",
    "\n",
    "# ========== CONFUSION MATRIX ==========\n",
    "labels_order = list(range(NUM_CLASSES))  # pastikan urutan konsisten dengan CLS_NAMES\n",
    "cm = confusion_matrix(y_true, y_pred, labels=labels_order)\n",
    "\n",
    "# Simpan numerik CM\n",
    "pd.DataFrame(cm, index=CLS_NAMES, columns=CLS_NAMES).to_csv(os.path.join(OUTPUT_DIR, \"confusion_matrix_numeric.csv\"))\n",
    "\n",
    "# Plot CM (angka putih pada diagonal, hitam di luar diagonal)\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.imshow(cm, cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix - TEST SET (Encoder)\", pad=12)\n",
    "plt.xticks(range(NUM_CLASSES), CLS_NAMES)\n",
    "plt.yticks(range(NUM_CLASSES), CLS_NAMES)\n",
    "for r in range(NUM_CLASSES):\n",
    "    for c in range(NUM_CLASSES):\n",
    "        color = \"white\" if r == c else \"black\"\n",
    "        plt.text(c, r, str(cm[r, c]), ha=\"center\", va=\"center\", fontsize=10, color=color)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confmat_test.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# (Opsional) Confusion Matrix ter-normalisasi baris\n",
    "row_sums = cm.sum(axis=1, keepdims=True) + EPS\n",
    "cm_norm = cm / row_sums\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.imshow(cm_norm, cmap=\"Blues\")\n",
    "plt.title(\"Normalized Confusion Matrix - TEST SET (Encoder)\", pad=12)\n",
    "plt.xticks(range(NUM_CLASSES), CLS_NAMES)\n",
    "plt.yticks(range(NUM_CLASSES), CLS_NAMES)\n",
    "for r in range(NUM_CLASSES):\n",
    "    for c in range(NUM_CLASSES):\n",
    "        val = cm_norm[r, c]\n",
    "        txt = f\"{val:.2f}\"\n",
    "        color = \"white\" if (r == c and val > 0.5) else \"black\"\n",
    "        plt.text(c, r, txt, ha=\"center\", va=\"center\", fontsize=9, color=color)\n",
    "plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confmat_test_normalized.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ========== METRIK PER-KELAS ==========\n",
    "rows = []\n",
    "total = cm.sum()\n",
    "for i, cls in enumerate(CLS_NAMES):\n",
    "    TP = cm[i, i]\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    TN = total - TP - FN - FP\n",
    "\n",
    "    ACC  = (TP + TN) / (total + EPS)\n",
    "    REC  = TP / (TP + FN + EPS)            # Sensitivitas/Recall\n",
    "    SPEC = TN / (TN + FP + EPS)            # Spesifisitas\n",
    "    F1   = (2 * TP) / (2 * TP + FP + FN + EPS)\n",
    "\n",
    "    rows.append({\n",
    "        \"kelas\": cls,\n",
    "        \"akurasi\": round(ACC, 6),\n",
    "        \"recall\": round(REC, 6),\n",
    "        \"spesifisitas\": round(SPEC, 6),\n",
    "        \"f1\": round(F1, 6),\n",
    "        \"support\": int(cm[i, :].sum())\n",
    "    })\n",
    "\n",
    "per_class_df = pd.DataFrame(rows)\n",
    "per_class_df.to_csv(os.path.join(OUTPUT_DIR, \"metrics_per_class_test.csv\"), index=False)\n",
    "\n",
    "# ========== RINGKASAN GLOBAL ==========\n",
    "summary = {\n",
    "    \"accuracy\": round(float(acc), 6),\n",
    "    \"f1_macro\": round(float(f1_m), 6),\n",
    "    \"f1_weighted\": round(float(f1_w), 6),\n",
    "    \"num_samples\": int(len(y_true)),\n",
    "    \"classes\": CLS_NAMES\n",
    "}\n",
    "with open(os.path.join(OUTPUT_DIR, \"metrics_summary_test.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n[TEST] Evaluasi selesai.\")\n",
    "print(f\"- Confusion matrix: {os.path.join(OUTPUT_DIR, 'confmat_test.png')}\")\n",
    "print(f(\"- CM normalized: {os.path.join(OUTPUT_DIR, 'confmat_test_normalized.png')}\") )\n",
    "print(f\"- Per-kelas CSV : {os.path.join(OUTPUT_DIR, 'metrics_per_class_test.csv')}\")\n",
    "print(f\"- Ringkasan JSON: {os.path.join(OUTPUT_DIR, 'metrics_summary_test.json')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
