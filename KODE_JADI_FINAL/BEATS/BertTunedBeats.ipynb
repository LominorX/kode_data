{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa7caf0b-08fc-4133-888c-404d3c51a018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Collecting transformers==4.48.2\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m192.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m883.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m691.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.2)\n",
      "  Downloading huggingface_hub-0.34.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.48.2)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m580.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.2)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.48.2)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m854.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m509.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m221.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m189.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m246.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m214.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m223.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m114.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m181.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m160.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m164.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m268.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.1-py3-none-any.whl (558 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m169.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m237.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m199.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m99.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m292.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m241.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m144.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m138.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m172.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m176.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, kiwisolver, joblib, hf-xet, fonttools, cycler, scipy, pandas, huggingface-hub, contourpy, tokenizers, scikit-learn, matplotlib, transformers, seaborn, accelerate\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "Successfully installed accelerate-0.26.0 contourpy-1.3.2 cycler-0.12.1 fonttools-4.59.0 hf-xet-1.1.5 huggingface-hub-0.34.1 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.10.3 numpy-1.26.4 pandas-2.3.1 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.7.1 scipy-1.15.3 seaborn-0.13.2 threadpoolctl-3.6.0 tokenizers-0.21.2 tqdm-4.67.1 transformers-4.48.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 torch transformers==4.48.2 scikit-learn accelerate==0.26.0 matplotlib tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c655138-6d10-4eb4-a4e5-e42ec99139f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11200' max='11200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11200/11200 3:24:47, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.031100</td>\n",
       "      <td>0.257011</td>\n",
       "      <td>0.936830</td>\n",
       "      <td>0.936762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.172500</td>\n",
       "      <td>0.119348</td>\n",
       "      <td>0.966518</td>\n",
       "      <td>0.966491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.101566</td>\n",
       "      <td>0.976339</td>\n",
       "      <td>0.976328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.071000</td>\n",
       "      <td>0.096027</td>\n",
       "      <td>0.978571</td>\n",
       "      <td>0.978635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.087305</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>0.981199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033100</td>\n",
       "      <td>0.085461</td>\n",
       "      <td>0.982589</td>\n",
       "      <td>0.982615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.073698</td>\n",
       "      <td>0.986384</td>\n",
       "      <td>0.986406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.017100</td>\n",
       "      <td>0.079244</td>\n",
       "      <td>0.984375</td>\n",
       "      <td>0.984430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.012300</td>\n",
       "      <td>0.077762</td>\n",
       "      <td>0.986161</td>\n",
       "      <td>0.986152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>0.097904</td>\n",
       "      <td>0.983929</td>\n",
       "      <td>0.983964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.107749</td>\n",
       "      <td>0.985268</td>\n",
       "      <td>0.985304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.104388</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.985919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.109282</td>\n",
       "      <td>0.985045</td>\n",
       "      <td>0.985033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.115877</td>\n",
       "      <td>0.985491</td>\n",
       "      <td>0.985485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.107839</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.985947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.101310</td>\n",
       "      <td>0.987277</td>\n",
       "      <td>0.987285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.103243</td>\n",
       "      <td>0.986161</td>\n",
       "      <td>0.986185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.105821</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.985941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.110812</td>\n",
       "      <td>0.986607</td>\n",
       "      <td>0.986614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.105019</td>\n",
       "      <td>0.986830</td>\n",
       "      <td>0.986832</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 2/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11200' max='11200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11200/11200 3:33:16, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.033400</td>\n",
       "      <td>0.294480</td>\n",
       "      <td>0.926339</td>\n",
       "      <td>0.926432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178000</td>\n",
       "      <td>0.132416</td>\n",
       "      <td>0.961830</td>\n",
       "      <td>0.962055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.105600</td>\n",
       "      <td>0.096253</td>\n",
       "      <td>0.976562</td>\n",
       "      <td>0.976551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.070100</td>\n",
       "      <td>0.085349</td>\n",
       "      <td>0.978125</td>\n",
       "      <td>0.978092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.076562</td>\n",
       "      <td>0.983259</td>\n",
       "      <td>0.983326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.036700</td>\n",
       "      <td>0.082515</td>\n",
       "      <td>0.981250</td>\n",
       "      <td>0.981314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.092580</td>\n",
       "      <td>0.981920</td>\n",
       "      <td>0.981895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016200</td>\n",
       "      <td>0.081709</td>\n",
       "      <td>0.984821</td>\n",
       "      <td>0.984859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.103208</td>\n",
       "      <td>0.983482</td>\n",
       "      <td>0.983552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.119599</td>\n",
       "      <td>0.980580</td>\n",
       "      <td>0.980592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.097442</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.985964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.004800</td>\n",
       "      <td>0.103827</td>\n",
       "      <td>0.985045</td>\n",
       "      <td>0.985067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.095493</td>\n",
       "      <td>0.986607</td>\n",
       "      <td>0.986639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.089641</td>\n",
       "      <td>0.988616</td>\n",
       "      <td>0.988644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.096858</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0.987077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.089913</td>\n",
       "      <td>0.988393</td>\n",
       "      <td>0.988412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.091802</td>\n",
       "      <td>0.987946</td>\n",
       "      <td>0.987974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.101702</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0.987084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.095188</td>\n",
       "      <td>0.988170</td>\n",
       "      <td>0.988195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095315</td>\n",
       "      <td>0.987946</td>\n",
       "      <td>0.987972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 3/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11200' max='11200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11200/11200 3:44:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.028300</td>\n",
       "      <td>0.264968</td>\n",
       "      <td>0.934152</td>\n",
       "      <td>0.933981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.113348</td>\n",
       "      <td>0.969866</td>\n",
       "      <td>0.969951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.115500</td>\n",
       "      <td>0.100008</td>\n",
       "      <td>0.975223</td>\n",
       "      <td>0.975107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.075400</td>\n",
       "      <td>0.071134</td>\n",
       "      <td>0.984152</td>\n",
       "      <td>0.984140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.054100</td>\n",
       "      <td>0.081393</td>\n",
       "      <td>0.983259</td>\n",
       "      <td>0.983204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.034400</td>\n",
       "      <td>0.081561</td>\n",
       "      <td>0.982812</td>\n",
       "      <td>0.982784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.079095</td>\n",
       "      <td>0.983929</td>\n",
       "      <td>0.983918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.068228</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0.987035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>0.077799</td>\n",
       "      <td>0.987277</td>\n",
       "      <td>0.987273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>0.105048</td>\n",
       "      <td>0.984152</td>\n",
       "      <td>0.984145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.084880</td>\n",
       "      <td>0.987054</td>\n",
       "      <td>0.987028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.084067</td>\n",
       "      <td>0.988393</td>\n",
       "      <td>0.988384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.088766</td>\n",
       "      <td>0.987277</td>\n",
       "      <td>0.987265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.074549</td>\n",
       "      <td>0.988170</td>\n",
       "      <td>0.988178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.085339</td>\n",
       "      <td>0.988393</td>\n",
       "      <td>0.988370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>0.988839</td>\n",
       "      <td>0.988813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.076452</td>\n",
       "      <td>0.989732</td>\n",
       "      <td>0.989727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.075340</td>\n",
       "      <td>0.989509</td>\n",
       "      <td>0.989503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.075315</td>\n",
       "      <td>0.989732</td>\n",
       "      <td>0.989729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074671</td>\n",
       "      <td>0.990848</td>\n",
       "      <td>0.990847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 4/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11200' max='11200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11200/11200 3:54:29, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.038900</td>\n",
       "      <td>0.281838</td>\n",
       "      <td>0.925446</td>\n",
       "      <td>0.925766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.172400</td>\n",
       "      <td>0.127670</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.957334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.102400</td>\n",
       "      <td>0.095084</td>\n",
       "      <td>0.974777</td>\n",
       "      <td>0.974703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068900</td>\n",
       "      <td>0.080191</td>\n",
       "      <td>0.981920</td>\n",
       "      <td>0.981889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.088493</td>\n",
       "      <td>0.978795</td>\n",
       "      <td>0.978778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.099646</td>\n",
       "      <td>0.979241</td>\n",
       "      <td>0.979303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.082234</td>\n",
       "      <td>0.983705</td>\n",
       "      <td>0.983742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.084427</td>\n",
       "      <td>0.984152</td>\n",
       "      <td>0.984147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011400</td>\n",
       "      <td>0.080616</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.985729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.096763</td>\n",
       "      <td>0.984152</td>\n",
       "      <td>0.984166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.099256</td>\n",
       "      <td>0.984821</td>\n",
       "      <td>0.984813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.090219</td>\n",
       "      <td>0.985938</td>\n",
       "      <td>0.985950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.096812</td>\n",
       "      <td>0.986384</td>\n",
       "      <td>0.986368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.087495</td>\n",
       "      <td>0.986384</td>\n",
       "      <td>0.986392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.091277</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>0.987503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.080723</td>\n",
       "      <td>0.989286</td>\n",
       "      <td>0.989305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.105260</td>\n",
       "      <td>0.986830</td>\n",
       "      <td>0.986834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.093508</td>\n",
       "      <td>0.988170</td>\n",
       "      <td>0.988169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.088467</td>\n",
       "      <td>0.988170</td>\n",
       "      <td>0.988173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.086786</td>\n",
       "      <td>0.988393</td>\n",
       "      <td>0.988395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 5/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11200' max='11200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11200/11200 3:32:11, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.046600</td>\n",
       "      <td>0.280701</td>\n",
       "      <td>0.927232</td>\n",
       "      <td>0.926773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.131724</td>\n",
       "      <td>0.962946</td>\n",
       "      <td>0.963078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101500</td>\n",
       "      <td>0.068365</td>\n",
       "      <td>0.981920</td>\n",
       "      <td>0.981924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.077420</td>\n",
       "      <td>0.982812</td>\n",
       "      <td>0.982836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>0.079958</td>\n",
       "      <td>0.982812</td>\n",
       "      <td>0.982883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.033600</td>\n",
       "      <td>0.069245</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.985724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.065993</td>\n",
       "      <td>0.986607</td>\n",
       "      <td>0.986622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.020300</td>\n",
       "      <td>0.069511</td>\n",
       "      <td>0.986384</td>\n",
       "      <td>0.986393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.014500</td>\n",
       "      <td>0.064821</td>\n",
       "      <td>0.986384</td>\n",
       "      <td>0.986364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.005600</td>\n",
       "      <td>0.070009</td>\n",
       "      <td>0.988839</td>\n",
       "      <td>0.988842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.005700</td>\n",
       "      <td>0.072624</td>\n",
       "      <td>0.989062</td>\n",
       "      <td>0.989065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.083176</td>\n",
       "      <td>0.988170</td>\n",
       "      <td>0.988175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.073456</td>\n",
       "      <td>0.988616</td>\n",
       "      <td>0.988611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.070040</td>\n",
       "      <td>0.990402</td>\n",
       "      <td>0.990386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.066640</td>\n",
       "      <td>0.990848</td>\n",
       "      <td>0.990849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.067509</td>\n",
       "      <td>0.990625</td>\n",
       "      <td>0.990628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.072200</td>\n",
       "      <td>0.990402</td>\n",
       "      <td>0.990394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.079983</td>\n",
       "      <td>0.990625</td>\n",
       "      <td>0.990625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078306</td>\n",
       "      <td>0.990848</td>\n",
       "      <td>0.990848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.075534</td>\n",
       "      <td>0.991295</td>\n",
       "      <td>0.991293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Mean ± Std Final Metrics (Test Set per Fold) =====\n",
      "            ACC           F1          REC         SPEC\n",
      "N  0.993±0.002  0.983±0.004  0.981±0.004  0.996±0.001\n",
      "L  0.997±0.000  0.991±0.001  0.991±0.003  0.998±0.001\n",
      "R  0.998±0.001  0.995±0.002  0.995±0.002  0.999±0.001\n",
      "V  0.992±0.002  0.980±0.004  0.983±0.006  0.994±0.001\n",
      "Q  0.998±0.001  0.995±0.001  0.994±0.002  0.999±0.000\n"
     ]
    }
   ],
   "source": [
    "# [IMPORT & SETUP SAMA SEPERTI SEBELUMNYA]\n",
    "import os, glob, random, gc, json\n",
    "import numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, set_seed, TrainerCallback)\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "DATA_DIR    = \"/workspace/SPLIT_BEATS_NPY/train\"\n",
    "LABEL_MAP   = {'N':0,'L':1,'R':2,'V':3,'Q':4}\n",
    "MODEL_NAME  = \"bert-base-uncased\"\n",
    "SEED        = 42\n",
    "N_SPLITS    = 5\n",
    "EPOCHS      = 20\n",
    "MAX_LEN     = 512\n",
    "OUTPUT_BASE = \"/workspace/HASIL_BERT_tuned/HASIL_1\"\n",
    "\n",
    "HYPERPARAMS = {\n",
    "    \"lr\": 2e-5,\n",
    "    \"batch_size\": 32,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"warmup_ratio\": 0.1\n",
    "}\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cls_names = list(LABEL_MAP.keys())\n",
    "\n",
    "# === UTIL ===\n",
    "def signal_to_text(sig):\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, preds),\n",
    "        \"f1_macro\": f1_score(labels, preds, average='macro')\n",
    "    }\n",
    "\n",
    "# === CALLBACK FOR LOGGING METRICS ===\n",
    "class MetricsLoggerCallback(TrainerCallback):\n",
    "    def __init__(self, trainer_ref, train_ds, val_ds):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer_ref\n",
    "        self.train_ds = train_ds\n",
    "        self.val_ds = val_ds\n",
    "        self.history = []\n",
    "\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        epoch = state.epoch\n",
    "\n",
    "        train_preds = self.trainer.predict(self.train_ds)\n",
    "        train_labels = train_preds.label_ids\n",
    "        train_logits = train_preds.predictions\n",
    "        train_acc = accuracy_score(train_labels, np.argmax(train_logits, axis=1))\n",
    "\n",
    "        val_preds = self.trainer.predict(self.val_ds)\n",
    "        val_labels = val_preds.label_ids\n",
    "        val_logits = val_preds.predictions\n",
    "        val_acc = accuracy_score(val_labels, np.argmax(val_logits, axis=1))\n",
    "        val_loss = val_preds.metrics[\"test_loss\"]\n",
    "\n",
    "        # Get train loss manually\n",
    "        train_loss = None\n",
    "        for log in reversed(self.trainer.state.log_history):\n",
    "            if 'loss' in log and 'epoch' in log and log['epoch'] == epoch:\n",
    "                train_loss = log['loss']\n",
    "                break\n",
    "\n",
    "        self.history.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"train_acc\": train_acc,\n",
    "            \"val_acc\": val_acc\n",
    "        })\n",
    "\n",
    "# === LOAD & TOKENIZE DATA ===\n",
    "files, labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    for f in glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\")):\n",
    "        files.append(f); labels.append(idx)\n",
    "files, labels = np.array(files), np.array(labels)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "all_ids, all_mask = [], []\n",
    "for f in files:\n",
    "    txt = signal_to_text(np.load(f))\n",
    "    enc = tokenizer(txt, padding=\"max_length\", truncation=True,\n",
    "                    max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    all_ids.append(enc[\"input_ids\"].squeeze(0))\n",
    "    all_mask.append(enc[\"attention_mask\"].squeeze(0))\n",
    "all_ids  = torch.stack(all_ids)\n",
    "all_mask = torch.stack(all_mask)\n",
    "labels_t = torch.tensor(labels)\n",
    "\n",
    "# === 5-FOLD TRAINING ===\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "summary_per_fold = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(skf.split(all_ids, labels), 1):\n",
    "    print(f\"\\n=== Fold {fold}/{N_SPLITS} ===\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    X_train, X_test = all_ids[train_idx], all_ids[test_idx]\n",
    "    M_train, M_test = all_mask[train_idx], all_mask[test_idx]\n",
    "    Y_train, Y_test = labels_t[train_idx], labels_t[test_idx]\n",
    "\n",
    "    # Split train into train/val (80/20)\n",
    "    tr_idx, val_idx = train_test_split(np.arange(len(X_train)), test_size=0.2, stratify=Y_train, random_state=SEED)\n",
    "    Xtr, Xval = X_train[tr_idx], X_train[val_idx]\n",
    "    Mtr, Mval = M_train[tr_idx], M_train[val_idx]\n",
    "    Ytr, Yval = Y_train[tr_idx], Y_train[val_idx]\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(Xtr, Mtr, Ytr)\n",
    "    val_ds   = torch.utils.data.TensorDataset(Xval, Mval, Yval)\n",
    "    test_ds  = torch.utils.data.TensorDataset(X_test, M_test, Y_test)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        ids, msk, lbl = map(torch.stack, zip(*batch))\n",
    "        return {\"input_ids\": ids, \"attention_mask\": msk, \"labels\": lbl}\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(LABEL_MAP)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    fold_dir = os.path.join(OUTPUT_BASE, f\"fold{fold}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=fold_dir,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=HYPERPARAMS[\"lr\"],\n",
    "        weight_decay=HYPERPARAMS[\"weight_decay\"],\n",
    "        warmup_ratio=HYPERPARAMS[\"warmup_ratio\"],\n",
    "        per_device_train_batch_size=HYPERPARAMS[\"batch_size\"],\n",
    "        per_device_eval_batch_size=HYPERPARAMS[\"batch_size\"],\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1_macro\",\n",
    "        greater_is_better=True,\n",
    "        logging_strategy=\"epoch\",\n",
    "        seed=SEED,\n",
    "        report_to=[]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        data_collator=collate_fn,\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    cb = MetricsLoggerCallback(trainer, train_ds, val_ds)\n",
    "    trainer.add_callback(cb)\n",
    "\n",
    "    trainer.train()\n",
    "    model.eval()\n",
    "\n",
    "    # Simpan log per epoch\n",
    "    hist_df = pd.DataFrame(cb.history)\n",
    "    hist_df.to_csv(os.path.join(fold_dir, \"history_epoch.csv\"), index=False)\n",
    "\n",
    "    # Evaluasi di test set\n",
    "    test_pred = trainer.predict(test_ds)\n",
    "    y_true = Y_test.cpu().numpy()\n",
    "    y_pred = np.argmax(test_pred.predictions, axis=1)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(cls_names))))\n",
    "    per_cls = {}\n",
    "    for i, cls in enumerate(cls_names):\n",
    "        TP = cm[i, i]\n",
    "        FN = cm[i].sum() - TP\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc = (TP + TN) / cm.sum()\n",
    "        rec = TP / (TP + FN + 1e-8)\n",
    "        spec = TN / (TN + FP + 1e-8)\n",
    "        f1 = f1_score(y_true, y_pred, labels=[i], average='macro')\n",
    "        per_cls[cls] = {'ACC': acc, 'F1': f1, 'REC': rec, 'SPEC': spec}\n",
    "\n",
    "    # Confusion matrix gambar\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - Fold {fold}\")\n",
    "    plt.xticks(range(len(cls_names)), cls_names)\n",
    "    plt.yticks(range(len(cls_names)), cls_names)\n",
    "    for r in range(len(cm)):\n",
    "        for c in range(len(cm)):\n",
    "            plt.text(c, r, cm[r,c], ha='center', va='center', color='black', fontsize=9)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fold_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    pd.DataFrame(per_cls).T.to_csv(os.path.join(fold_dir, \"metrics_per_class.csv\"))\n",
    "    summary_per_fold.append(per_cls)\n",
    "\n",
    "# === SUMMARY ===\n",
    "summary = {c: {m: [] for m in ['ACC','F1','REC','SPEC']} for c in cls_names}\n",
    "for fm in summary_per_fold:\n",
    "    for c in cls_names:\n",
    "        for m in summary[c]:\n",
    "            summary[c][m].append(fm[c][m])\n",
    "\n",
    "df_summary = pd.DataFrame({\n",
    "    c: {m: f\"{np.mean(v):.3f}±{np.std(v):.3f}\" for m, v in summary[c].items()}\n",
    "    for c in cls_names\n",
    "}).T\n",
    "\n",
    "print(\"\\n===== Mean ± Std Final Metrics (Test Set per Fold) =====\\n\", df_summary)\n",
    "df_summary.to_csv(os.path.join(OUTPUT_BASE, \"final_summary_best_only.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68695cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 Hasil evaluasi disimpan di: D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\Bert beat tuned\\fold2\\checkpoint-7840\\test_metrics_perclass.csv\n",
      "   Accuracy    Recall  Specificity  F1-Score\n",
      "N  0.992714  0.976429     0.996786  0.981688\n",
      "L  0.996714  0.991429     0.998036  0.991783\n",
      "R  0.997714  0.990714     0.999464  0.994265\n",
      "V  0.991143  0.990000     0.991429  0.978123\n",
      "Q  0.997429  0.990714     0.999107  0.993553\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "DATA_DIR    = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\DATA\\output_coba\\SPLIT_BEATS_NPY\\Beats_TEST\"\n",
    "MODEL_PATH  = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\Bert beat tuned\\fold2\\checkpoint-7840\"\n",
    "LABEL_MAP   = {'N': 0, 'L': 1, 'R': 2, 'V': 3, 'Q': 4}\n",
    "MAX_LEN     = 512\n",
    "MODEL_NAME  = \"bert-base-uncased\"\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cls_names   = list(LABEL_MAP.keys())\n",
    "\n",
    "# === FUNGSI PEMROSESAN SINYAL ===\n",
    "def signal_to_text(sig):\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "# === LOAD TEST DATA ===\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "files, labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    cls_folder = os.path.join(DATA_DIR, cls)\n",
    "    for f in glob.glob(os.path.join(cls_folder, \"*.npy\")):\n",
    "        files.append(f)\n",
    "        labels.append(idx)\n",
    "files, labels = np.array(files), np.array(labels)\n",
    "\n",
    "all_ids, all_mask = [], []\n",
    "for f in files:\n",
    "    txt = signal_to_text(np.load(f))\n",
    "    enc = tokenizer(txt, padding=\"max_length\", truncation=True,\n",
    "                    max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    all_ids.append(enc[\"input_ids\"].squeeze(0))\n",
    "    all_mask.append(enc[\"attention_mask\"].squeeze(0))\n",
    "all_ids  = torch.stack(all_ids)\n",
    "all_mask = torch.stack(all_mask)\n",
    "labels_t = torch.tensor(labels)\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === INFERENSI ===\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(all_ids), 32):\n",
    "        ids = all_ids[i:i+32].to(DEVICE)\n",
    "        msk = all_mask[i:i+32].to(DEVICE)\n",
    "        outputs = model(input_ids=ids, attention_mask=msk)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "\n",
    "y_true = labels_t.numpy()\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# === CONFUSION MATRIX DENGAN WARNA PUTIH UNTUK DIAGONAL ===\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(cls_names))))\n",
    "plt.figure(figsize=(6, 6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                 xticklabels=cls_names, yticklabels=cls_names, cbar=False)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        color = \"white\" if i == j else \"black\"\n",
    "        ax.text(j + 0.5, i + 0.5, format(cm[i, j], \"d\"),\n",
    "                ha=\"center\", va=\"center\", color=color, fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - TEST SET (Fold 2)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_PATH, \"confusion_matrix_test.png\"))\n",
    "plt.show()\n",
    "\n",
    "# === METRIK PER KELAS ===\n",
    "per_cls = {}\n",
    "for i, cls in enumerate(cls_names):\n",
    "    TP = cm[i, i]\n",
    "    FN = cm[i].sum() - TP\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    TN = cm.sum() - (TP + FN + FP)\n",
    "    acc  = (TP + TN) / cm.sum()\n",
    "    rec  = TP / (TP + FN + 1e-8)\n",
    "    spec = TN / (TN + FP + 1e-8)\n",
    "    f1   = f1_score(y_true, y_pred, labels=[i], average='macro')\n",
    "    per_cls[cls] = {'Accuracy': acc, 'Recall': rec, 'Specificity': spec, 'F1-Score': f1}\n",
    "\n",
    "# === SIMPAN HASIL ===\n",
    "df = pd.DataFrame(per_cls).T\n",
    "df_path = os.path.join(MODEL_PATH, \"test_metrics_perclass.csv\")\n",
    "df.to_csv(df_path)\n",
    "print(f\"\\n📁 Hasil evaluasi disimpan di: {df_path}\")\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
