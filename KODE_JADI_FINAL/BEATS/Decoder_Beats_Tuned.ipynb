{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df2f2d9-1a45-4398-bd3c-e6d50122c70e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Collecting transformers==4.48.2\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m971.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.2)\n",
      "  Downloading huggingface_hub-0.33.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.48.2)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m507.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.2)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.48.2)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.8)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m315.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m288.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m275.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m692.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.4-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.3/515.3 kB\u001b[0m \u001b[31m433.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m277.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m349.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m392.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m264.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, safetensors, regex, numpy, hf-xet, huggingface-hub, tokenizers, transformers, accelerate\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "Successfully installed accelerate-0.26.0 hf-xet-1.1.5 huggingface-hub-0.33.4 numpy-1.26.4 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.2 tqdm-4.67.1 transformers-4.48.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 torch transformers==4.48.2 scikit-learn accelerate==0.26.0 matplotlib tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792f6cca-3e61-48b8-9d96-82d9c560858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0+cu121\n",
      "12.1\n",
      "True\n",
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aeedf4c-d3c5-4a21-a89a-2a73f62e6144",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-encoding: 100%|██████████| 28000/28000 [00:03<00:00, 9139.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FOLD 1/2 ====\n",
      "\n",
      "⏰ Fold 1 | Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6076, Train Acc: 0.2178\n",
      "Val Loss: 1.5765, Val Acc: 0.2924\n",
      "\n",
      "⏰ Fold 1 | Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5544, Train Acc: 0.3308\n",
      "Val Loss: 1.5302, Val Acc: 0.3850\n",
      "\n",
      "⏰ Fold 1 | Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5153, Train Acc: 0.3888\n",
      "Val Loss: 1.4956, Val Acc: 0.4054\n",
      "\n",
      "⏰ Fold 1 | Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4849, Train Acc: 0.4126\n",
      "Val Loss: 1.4675, Val Acc: 0.4266\n",
      "\n",
      "⏰ Fold 1 | Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4611, Train Acc: 0.4265\n",
      "Val Loss: 1.4463, Val Acc: 0.4335\n",
      "\n",
      "⏰ Fold 1 | Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4423, Train Acc: 0.4331\n",
      "Val Loss: 1.4296, Val Acc: 0.4379\n",
      "\n",
      "⏰ Fold 1 | Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4269, Train Acc: 0.4410\n",
      "Val Loss: 1.4165, Val Acc: 0.4418\n",
      "\n",
      "⏰ Fold 1 | Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4144, Train Acc: 0.4441\n",
      "Val Loss: 1.4046, Val Acc: 0.4484\n",
      "\n",
      "⏰ Fold 1 | Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4045, Train Acc: 0.4505\n",
      "Val Loss: 1.3953, Val Acc: 0.4486\n",
      "\n",
      "⏰ Fold 1 | Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3953, Train Acc: 0.4534\n",
      "Val Loss: 1.3879, Val Acc: 0.4564\n",
      "\n",
      "⏰ Fold 1 | Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3887, Train Acc: 0.4563\n",
      "Val Loss: 1.3817, Val Acc: 0.4552\n",
      "\n",
      "⏰ Fold 1 | Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3835, Train Acc: 0.4561\n",
      "Val Loss: 1.3772, Val Acc: 0.4621\n",
      "\n",
      "⏰ Fold 1 | Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3788, Train Acc: 0.4616\n",
      "Val Loss: 1.3733, Val Acc: 0.4610\n",
      "\n",
      "⏰ Fold 1 | Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3760, Train Acc: 0.4634\n",
      "Val Loss: 1.3703, Val Acc: 0.4614\n",
      "\n",
      "⏰ Fold 1 | Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3730, Train Acc: 0.4623\n",
      "Val Loss: 1.3682, Val Acc: 0.4620\n",
      "\n",
      "⏰ Fold 1 | Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3714, Train Acc: 0.4647\n",
      "Val Loss: 1.3668, Val Acc: 0.4640\n",
      "\n",
      "⏰ Fold 1 | Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3698, Train Acc: 0.4631\n",
      "Val Loss: 1.3658, Val Acc: 0.4644\n",
      "\n",
      "⏰ Fold 1 | Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3687, Train Acc: 0.4651\n",
      "Val Loss: 1.3653, Val Acc: 0.4641\n",
      "\n",
      "⏰ Fold 1 | Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3690, Train Acc: 0.4656\n",
      "Val Loss: 1.3650, Val Acc: 0.4641\n",
      "\n",
      "⏰ Fold 1 | Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3681, Train Acc: 0.4640\n",
      "Val Loss: 1.3650, Val Acc: 0.4646\n",
      "✔️ Fold 1 Training Completed.\n",
      "\n",
      "==== FOLD 2/2 ====\n",
      "\n",
      "⏰ Fold 2 | Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.6144, Train Acc: 0.2543\n",
      "Val Loss: 1.5638, Val Acc: 0.3259\n",
      "\n",
      "⏰ Fold 2 | Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5343, Train Acc: 0.3849\n",
      "Val Loss: 1.5059, Val Acc: 0.4156\n",
      "\n",
      "⏰ Fold 2 | Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4859, Train Acc: 0.4284\n",
      "Val Loss: 1.4655, Val Acc: 0.4401\n",
      "\n",
      "⏰ Fold 2 | Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4509, Train Acc: 0.4405\n",
      "Val Loss: 1.4347, Val Acc: 0.4438\n",
      "\n",
      "⏰ Fold 2 | Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4239, Train Acc: 0.4461\n",
      "Val Loss: 1.4124, Val Acc: 0.4503\n",
      "\n",
      "⏰ Fold 2 | Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.4040, Train Acc: 0.4523\n",
      "Val Loss: 1.3947, Val Acc: 0.4541\n",
      "\n",
      "⏰ Fold 2 | Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3883, Train Acc: 0.4560\n",
      "Val Loss: 1.3815, Val Acc: 0.4570\n",
      "\n",
      "⏰ Fold 2 | Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3759, Train Acc: 0.4580\n",
      "Val Loss: 1.3703, Val Acc: 0.4571\n",
      "\n",
      "⏰ Fold 2 | Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3650, Train Acc: 0.4606\n",
      "Val Loss: 1.3620, Val Acc: 0.4631\n",
      "\n",
      "⏰ Fold 2 | Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3574, Train Acc: 0.4633\n",
      "Val Loss: 1.3551, Val Acc: 0.4649\n",
      "\n",
      "⏰ Fold 2 | Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3508, Train Acc: 0.4649\n",
      "Val Loss: 1.3496, Val Acc: 0.4654\n",
      "\n",
      "⏰ Fold 2 | Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3468, Train Acc: 0.4632\n",
      "Val Loss: 1.3453, Val Acc: 0.4666\n",
      "\n",
      "⏰ Fold 2 | Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3424, Train Acc: 0.4664\n",
      "Val Loss: 1.3420, Val Acc: 0.4674\n",
      "\n",
      "⏰ Fold 2 | Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3394, Train Acc: 0.4675\n",
      "Val Loss: 1.3396, Val Acc: 0.4679\n",
      "\n",
      "⏰ Fold 2 | Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3350, Train Acc: 0.4696\n",
      "Val Loss: 1.3363, Val Acc: 0.4679\n",
      "\n",
      "⏰ Fold 2 | Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3334, Train Acc: 0.4660\n",
      "Val Loss: 1.3356, Val Acc: 0.4683\n",
      "\n",
      "⏰ Fold 2 | Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3330, Train Acc: 0.4688\n",
      "Val Loss: 1.3351, Val Acc: 0.4683\n",
      "\n",
      "⏰ Fold 2 | Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3323, Train Acc: 0.4701\n",
      "Val Loss: 1.3349, Val Acc: 0.4679\n",
      "\n",
      "⏰ Fold 2 | Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.3325, Train Acc: 0.4674\n",
      "Val Loss: 1.3348, Val Acc: 0.4680\n",
      "✔️ Fold 2 Training Completed.\n",
      "\n",
      "✔️ All training folds completed.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, gc, copy, math, random, time, json\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR    = \"/workspace/SPLIT_BEATS_NPY/train\"\n",
    "LABEL_MAP   = {'N':0,'L':1,'R':2,'V':3,'Q':4}\n",
    "SEED        = 42\n",
    "N_SPLITS    = 2\n",
    "EPOCHS      = 20\n",
    "BATCH_SIZE  = 32\n",
    "MAX_LEN     = 512\n",
    "EMB_DIM     = 512\n",
    "N_HEADS     = 8\n",
    "FF_DIM      = 2048\n",
    "N_LAYERS    = 12\n",
    "LR          = 2e-5\n",
    "OUTPUT_BASE = \"/workspace/HASIL_DECODER/HASIL_2\"\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_ID, CLS_ID  = 256, 257\n",
    "VOCAB_SIZE      = 258\n",
    "cls_names       = list(LABEL_MAP.keys())\n",
    "\n",
    "def signal_to_ids(sig: np.ndarray):\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    ids  = np.concatenate(([CLS_ID], norm))[:MAX_LEN]\n",
    "    mask = np.ones_like(ids, dtype=int)\n",
    "    if len(ids) < MAX_LEN:\n",
    "        pad_len = MAX_LEN - len(ids)\n",
    "        ids  = np.concatenate((ids,  np.full(pad_len, PAD_ID)))\n",
    "        mask = np.concatenate((mask, np.zeros(pad_len)))\n",
    "    return ids, mask\n",
    "\n",
    "# Load data\n",
    "files, labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    for f in glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\")):\n",
    "        files.append(f); labels.append(idx)\n",
    "files, labels = np.array(files), np.array(labels)\n",
    "\n",
    "all_ids, all_mask = [], []\n",
    "for f in tqdm(files, desc=\"Pre-encoding\"):\n",
    "    ids, msk = signal_to_ids(np.load(f))\n",
    "    all_ids.append(ids);  all_mask.append(msk)\n",
    "all_ids  = torch.tensor(all_ids,  dtype=torch.long)\n",
    "all_mask = torch.tensor(all_mask, dtype=torch.long)\n",
    "labels_t = torch.tensor(labels,   dtype=torch.long)\n",
    "\n",
    "class DecoderOnlyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_ID)\n",
    "        self.pos_emb   = nn.Parameter(torch.zeros(1, MAX_LEN, EMB_DIM))\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model=EMB_DIM, nhead=N_HEADS,\n",
    "                                               dim_feedforward=FF_DIM, dropout=0.1,\n",
    "                                               batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=N_LAYERS)\n",
    "        self.fc      = nn.Linear(EMB_DIM, len(LABEL_MAP))\n",
    "        nn.init.normal_(self.pos_emb, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        tgt = self.token_emb(input_ids) + self.pos_emb\n",
    "        memory = torch.zeros_like(tgt).to(tgt.device)\n",
    "        x = self.decoder(tgt, memory, tgt_key_padding_mask=~attention_mask.bool())\n",
    "        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1e-9)\n",
    "        return self.fc(x)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "rows_all = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(all_ids, labels), 1):\n",
    "    print(f\"\\n==== FOLD {fold}/{N_SPLITS} ====\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    train_ds = TensorDataset(all_ids[tr], all_mask[tr], labels_t[tr])\n",
    "    val_ds   = TensorDataset(all_ids[va], all_mask[va], labels_t[va])\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = DecoderOnlyClassifier().to(DEVICE)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.fc.weight.requires_grad = True\n",
    "    model.fc.bias.requires_grad = True\n",
    "\n",
    "    optim = torch.optim.AdamW(model.fc.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, EPOCHS)\n",
    "\n",
    "    best_state, best_loss = None, math.inf\n",
    "    out_dir = os.path.join(OUTPUT_BASE, f\"fold{fold}\"); os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"\\n⏰ Fold {fold} | Epoch {epoch}/{EPOCHS}\")\n",
    "        model.train(); total_loss = 0.0; correct = 0; total = 0\n",
    "        for ids, msk, lbl in tqdm(train_loader, leave=False):\n",
    "            ids, msk, lbl = ids.to(DEVICE), msk.to(DEVICE), lbl.to(DEVICE)\n",
    "            optim.zero_grad(); out = model(ids, msk)\n",
    "            loss = F.cross_entropy(out, lbl)\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step(); total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == lbl).sum().item()\n",
    "            total += lbl.size(0)\n",
    "        train_acc = correct / total\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval(); val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for ids, msk, lbl in val_loader:\n",
    "                ids, msk, lbl = ids.to(DEVICE), msk.to(DEVICE), lbl.to(DEVICE)\n",
    "                out = model(ids, msk)\n",
    "                val_loss += F.cross_entropy(out, lbl, reduction='sum').item()\n",
    "                correct_val += (out.argmax(1) == lbl).sum().item()\n",
    "                total_val += lbl.size(0)\n",
    "        val_loss /= len(val_ds)\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        print(f\"Train Loss: {total_loss/len(train_loader):.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss, best_state = val_loss, copy.deepcopy(model.state_dict())\n",
    "\n",
    "    torch.save(best_state, os.path.join(out_dir, \"best_model.pt\"))\n",
    "    print(f\"✔️ Fold {fold} Training Completed.\")\n",
    "\n",
    "print(\"\\n✔️ All training folds completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a9a662-0d65-48d7-ac9d-9895365660c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pre-encoding: 100%|██████████| 28000/28000 [00:03<00:00, 8625.28it/s]\n",
      "/tmp/ipykernel_3606/511644460.py:52: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  all_ids  = torch.tensor(all_ids,  dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== FOLD 1/2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.9732 | Train Acc: 0.6345 | Val Loss: 0.7783 | Val Acc: 0.7276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 0.6535 | Train Acc: 0.7701 | Val Loss: 0.6036 | Val Acc: 0.7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 0.5097 | Train Acc: 0.8248 | Val Loss: 0.5122 | Val Acc: 0.8212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 0.4292 | Train Acc: 0.8549 | Val Loss: 0.4704 | Val Acc: 0.8454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 0.3623 | Train Acc: 0.8747 | Val Loss: 0.4226 | Val Acc: 0.8602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 0.3090 | Train Acc: 0.8956 | Val Loss: 0.3885 | Val Acc: 0.8745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 0.2729 | Train Acc: 0.9086 | Val Loss: 0.3619 | Val Acc: 0.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 0.2472 | Train Acc: 0.9132 | Val Loss: 0.3321 | Val Acc: 0.8898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 | Train Loss: 0.2103 | Train Acc: 0.9281 | Val Loss: 0.3327 | Val Acc: 0.8940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 | Train Loss: 0.1903 | Train Acc: 0.9355 | Val Loss: 0.2941 | Val Acc: 0.9106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20 | Train Loss: 0.1657 | Train Acc: 0.9422 | Val Loss: 0.2691 | Val Acc: 0.9181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20 | Train Loss: 0.1442 | Train Acc: 0.9518 | Val Loss: 0.3067 | Val Acc: 0.9126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20 | Train Loss: 0.1224 | Train Acc: 0.9596 | Val Loss: 0.3003 | Val Acc: 0.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20 | Train Loss: 0.1116 | Train Acc: 0.9631 | Val Loss: 0.2877 | Val Acc: 0.9200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20 | Train Loss: 0.0949 | Train Acc: 0.9693 | Val Loss: 0.2622 | Val Acc: 0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20 | Train Loss: 0.0846 | Train Acc: 0.9722 | Val Loss: 0.2777 | Val Acc: 0.9272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20 | Train Loss: 0.0752 | Train Acc: 0.9763 | Val Loss: 0.2858 | Val Acc: 0.9250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20 | Train Loss: 0.0693 | Train Acc: 0.9785 | Val Loss: 0.2774 | Val Acc: 0.9278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20 | Train Loss: 0.0645 | Train Acc: 0.9799 | Val Loss: 0.2700 | Val Acc: 0.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20 | Train Loss: 0.0620 | Train Acc: 0.9814 | Val Loss: 0.2736 | Val Acc: 0.9279\n",
      "✔️ Fold 1 Training Completed. Best Val Loss: 0.2622\n",
      "\n",
      "==== FOLD 2/2 ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 1.0045 | Train Acc: 0.6151 | Val Loss: 0.7820 | Val Acc: 0.7213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 | Train Loss: 0.6696 | Train Acc: 0.7643 | Val Loss: 0.5353 | Val Acc: 0.8091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 | Train Loss: 0.5254 | Train Acc: 0.8199 | Val Loss: 0.4505 | Val Acc: 0.8525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 | Train Loss: 0.4300 | Train Acc: 0.8542 | Val Loss: 0.3985 | Val Acc: 0.8624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 | Train Loss: 0.3789 | Train Acc: 0.8714 | Val Loss: 0.4161 | Val Acc: 0.8605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 | Train Loss: 0.3309 | Train Acc: 0.8884 | Val Loss: 0.4066 | Val Acc: 0.8702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 | Train Loss: 0.2993 | Train Acc: 0.8981 | Val Loss: 0.3939 | Val Acc: 0.8679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 | Train Loss: 0.2572 | Train Acc: 0.9139 | Val Loss: 0.3108 | Val Acc: 0.9007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 46/438 [00:27<03:52,  1.69it/s]"
     ]
    }
   ],
   "source": [
    "import os, glob, gc, copy, math, random, time, json\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "DATA_DIR    = \"/workspace/SPLIT_BEATS_NPY/train\"\n",
    "LABEL_MAP   = {'N':0,'L':1,'R':2,'V':3,'Q':4}\n",
    "SEED        = 42\n",
    "N_SPLITS    = 2\n",
    "EPOCHS      = 20\n",
    "BATCH_SIZE  = 32\n",
    "MAX_LEN     = 512\n",
    "EMB_DIM     = 512\n",
    "N_HEADS     = 8\n",
    "FF_DIM      = 2048\n",
    "N_LAYERS    = 12\n",
    "LR          = 2e-5\n",
    "OUTPUT_BASE = \"/workspace/HASIL_DECODER/HASIL_TUNING_FULL_2\"\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)\n",
    "\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PAD_ID, CLS_ID  = 256, 257\n",
    "VOCAB_SIZE      = 258\n",
    "cls_names       = list(LABEL_MAP.keys())\n",
    "\n",
    "def signal_to_ids(sig: np.ndarray):\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    ids  = np.concatenate(([CLS_ID], norm))[:MAX_LEN]\n",
    "    mask = np.ones_like(ids, dtype=int)\n",
    "    if len(ids) < MAX_LEN:\n",
    "        pad_len = MAX_LEN - len(ids)\n",
    "        ids  = np.concatenate((ids,  np.full(pad_len, PAD_ID)))\n",
    "        mask = np.concatenate((mask, np.zeros(pad_len)))\n",
    "    return ids, mask\n",
    "\n",
    "# Load data\n",
    "files, labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    for f in glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\")):\n",
    "        files.append(f); labels.append(idx)\n",
    "files, labels = np.array(files), np.array(labels)\n",
    "\n",
    "all_ids, all_mask = [], []\n",
    "for f in tqdm(files, desc=\"Pre-encoding\"):\n",
    "    ids, msk = signal_to_ids(np.load(f))\n",
    "    all_ids.append(ids);  all_mask.append(msk)\n",
    "all_ids  = torch.tensor(all_ids,  dtype=torch.long)\n",
    "all_mask = torch.tensor(all_mask, dtype=torch.long)\n",
    "labels_t = torch.tensor(labels,   dtype=torch.long)\n",
    "\n",
    "class DecoderOnlyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(VOCAB_SIZE, EMB_DIM, padding_idx=PAD_ID)\n",
    "        self.pos_emb   = nn.Parameter(torch.zeros(1, MAX_LEN, EMB_DIM))\n",
    "        dec_layer = nn.TransformerDecoderLayer(d_model=EMB_DIM, nhead=N_HEADS,\n",
    "                                               dim_feedforward=FF_DIM, dropout=0.1,\n",
    "                                               batch_first=True)\n",
    "        self.decoder = nn.TransformerDecoder(dec_layer, num_layers=N_LAYERS)\n",
    "        self.fc      = nn.Linear(EMB_DIM, len(LABEL_MAP))\n",
    "        nn.init.normal_(self.pos_emb, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        tgt = self.token_emb(input_ids) + self.pos_emb\n",
    "        memory = torch.zeros_like(tgt).to(tgt.device)\n",
    "        x = self.decoder(tgt, memory, tgt_key_padding_mask=~attention_mask.bool())\n",
    "        x = (x * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True).clamp(min=1e-9)\n",
    "        return self.fc(x)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "rows_all = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(all_ids, labels), 1):\n",
    "    print(f\"\\n==== FOLD {fold}/{N_SPLITS} ====\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    train_ds = TensorDataset(all_ids[tr], all_mask[tr], labels_t[tr])\n",
    "    val_ds   = TensorDataset(all_ids[va], all_mask[va], labels_t[va])\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "    model = DecoderOnlyClassifier().to(DEVICE)\n",
    "\n",
    "    # FULL TUNING: semua parameter di-train\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, EPOCHS)\n",
    "\n",
    "    best_state, best_loss = None, math.inf\n",
    "    out_dir = os.path.join(OUTPUT_BASE, f\"fold{fold}\"); os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train(); total_loss = 0.0; correct = 0; total = 0\n",
    "        for ids, msk, lbl in tqdm(train_loader, leave=False):\n",
    "            ids, msk, lbl = ids.to(DEVICE), msk.to(DEVICE), lbl.to(DEVICE)\n",
    "            optim.zero_grad(); out = model(ids, msk)\n",
    "            loss = F.cross_entropy(out, lbl)\n",
    "            loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optim.step(); total_loss += loss.item()\n",
    "            correct += (out.argmax(1) == lbl).sum().item()\n",
    "            total += lbl.size(0)\n",
    "        train_acc = correct / total\n",
    "        scheduler.step()\n",
    "\n",
    "        model.eval(); val_loss, correct_val, total_val = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for ids, msk, lbl in val_loader:\n",
    "                ids, msk, lbl = ids.to(DEVICE), msk.to(DEVICE), lbl.to(DEVICE)\n",
    "                out = model(ids, msk)\n",
    "                val_loss += F.cross_entropy(out, lbl, reduction='sum').item()\n",
    "                correct_val += (out.argmax(1) == lbl).sum().item()\n",
    "                total_val += lbl.size(0)\n",
    "        val_loss /= len(val_ds)\n",
    "        val_acc = correct_val / total_val\n",
    "\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} | Train Loss: {total_loss/len(train_loader):.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': total_loss/len(train_loader),\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        })\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "            best_loss, best_state = val_loss, copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Simpan model & history\n",
    "    torch.save(best_state, os.path.join(out_dir, \"best_model.pt\"))\n",
    "    pd.DataFrame(history).to_csv(os.path.join(out_dir, \"history.csv\"), index=False)\n",
    "    print(f\"✔️ Fold {fold} Training Completed. Best Val Loss: {best_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✔️ All training folds completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
