{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a866645-25f8-4e67-bb9b-775c812f9ef0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.26.4)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: transformers==4.48.2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.48.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.6.1)\n",
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers==4.48.2) (4.67.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from accelerate==0.26.0) (6.1.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers==4.48.2) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.48.2) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.48.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.48.2) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers==4.48.2) (2025.1.31)\n",
      "Downloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.2.1\n",
      "    Uninstalling accelerate-1.2.1:\n",
      "      Successfully uninstalled accelerate-1.2.1\n",
      "Successfully installed accelerate-0.26.0\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 torch transformers==4.48.2 scikit-learn accelerate==0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3050eb10-1b48-4870-9603-26edc57f1ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\asus\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib tqdm pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eff340b0-fbee-4e8e-9327-0199a0d33dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Distribusi file per kelas:\n",
      "Kelas  Jumlah File\n",
      "    L         5600\n",
      "    N         5600\n",
      "    Q         5600\n",
      "    R         5600\n",
      "    V         5600\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, glob\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ═══ GANTI PATH DI BAWAH INI ═══\n",
    "DATA_DIR = r\"/workspace/SPLIT_BEATS_NPY/train\"   # folder utama data train (.npy)\n",
    "\n",
    "# ── Hitung file per kelas ────────────────────────────────────────\n",
    "counts = OrderedDict()\n",
    "for cls in sorted(os.listdir(DATA_DIR)):\n",
    "    cls_path = os.path.join(DATA_DIR, cls)\n",
    "    if not os.path.isdir(cls_path):\n",
    "        continue\n",
    "    n_files = len(glob.glob(os.path.join(cls_path, \"*.npy\")))\n",
    "    counts[cls] = n_files\n",
    "\n",
    "# ── Tampilkan tabel distribusi ───────────────────────────────────\n",
    "df = pd.DataFrame(\n",
    "    {\"Kelas\": list(counts.keys()), \"Jumlah File\": list(counts.values())}\n",
    ")\n",
    "print(\"\\n📊 Distribusi file per kelas:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# ── (Opsional) plot bar chart ────────────────────────────────────\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.bar(df[\"Kelas\"], df[\"Jumlah File\"])\n",
    "plt.xlabel(\"Kelas\")\n",
    "plt.ylabel(\"Jumlah Beat (.npy)\")\n",
    "plt.title(\"Distribusi Beat per Kelas\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132a58a-9b0d-4e5c-ac66-7f74d047b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d01ea2a4-9ef3-49ee-a1d6-e41201f7be95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28000 samples across 5 classes\n",
      "Tokenizing all samples …\n",
      "\n",
      "════ FOLD 1/5 ════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='14000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/14000 1:27:09, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.373100</td>\n",
       "      <td>0.158208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>0.114419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.121262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.061400</td>\n",
       "      <td>0.108887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.077716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.088786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.092626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.096708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020100</td>\n",
       "      <td>0.109399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.102393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.109293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.091127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.088607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.093086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.128879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.114646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.116530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.117407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.113984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ACC     F1    REC   SPEC\n",
      "N  0.988  0.969  0.962  0.994\n",
      "L  0.994  0.986  0.982  0.997\n",
      "R  0.997  0.993  0.998  0.997\n",
      "V  0.988  0.970  0.981  0.990\n",
      "Q  0.998  0.994  0.988  1.000\n",
      "\n",
      "════ FOLD 2/5 ════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='14000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/14000 1:27:08, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.117349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.166446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.089371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.066200</td>\n",
       "      <td>0.068708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.047500</td>\n",
       "      <td>0.058401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.037000</td>\n",
       "      <td>0.069460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.070006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.076681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.060158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>0.085657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.072019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.049539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.065443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.062891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.078472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.072718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.077410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.076201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.074560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ACC     F1    REC   SPEC\n",
      "N  0.996  0.989  0.987  0.998\n",
      "L  0.998  0.994  0.994  0.998\n",
      "R  0.998  0.996  0.996  0.999\n",
      "V  0.994  0.986  0.988  0.996\n",
      "Q  0.998  0.996  0.996  0.999\n",
      "\n",
      "════ FOLD 3/5 ════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='14000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/14000 1:27:13, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.451800</td>\n",
       "      <td>0.291807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.136920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.119806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.067800</td>\n",
       "      <td>0.077794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.051500</td>\n",
       "      <td>0.077270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.043600</td>\n",
       "      <td>0.076804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029600</td>\n",
       "      <td>0.104580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>0.088432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.019700</td>\n",
       "      <td>0.085150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.080287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.110402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.098580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.096214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.006200</td>\n",
       "      <td>0.098212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.108167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.109724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.105122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.100343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.092085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.093225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ACC     F1    REC   SPEC\n",
      "N  0.990  0.975  0.973  0.994\n",
      "L  0.995  0.988  0.986  0.998\n",
      "R  0.997  0.992  0.991  0.998\n",
      "V  0.989  0.973  0.981  0.991\n",
      "Q  0.996  0.990  0.987  0.998\n",
      "\n",
      "════ FOLD 4/5 ════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='14000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/14000 1:27:12, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.470100</td>\n",
       "      <td>0.209857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.157600</td>\n",
       "      <td>0.150282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.107900</td>\n",
       "      <td>0.151978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.090102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.064800</td>\n",
       "      <td>0.105156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.075402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.034800</td>\n",
       "      <td>0.091769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.107881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.023800</td>\n",
       "      <td>0.087137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017600</td>\n",
       "      <td>0.113653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.098042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.013600</td>\n",
       "      <td>0.099542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.103334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.091696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.097115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.132639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.104629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.104732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.106750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.108690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ACC     F1    REC   SPEC\n",
      "N  0.992  0.979  0.979  0.995\n",
      "L  0.995  0.987  0.979  0.999\n",
      "R  0.995  0.988  0.995  0.995\n",
      "V  0.989  0.974  0.975  0.993\n",
      "Q  0.995  0.988  0.989  0.997\n",
      "\n",
      "════ FOLD 5/5 ════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14000' max='14000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14000/14000 1:27:14, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.595700</td>\n",
       "      <td>0.265342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.132383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.129600</td>\n",
       "      <td>0.130108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.102100</td>\n",
       "      <td>0.112193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.083800</td>\n",
       "      <td>0.101353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.074500</td>\n",
       "      <td>0.126879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.087028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.090367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.082622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.094863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.027100</td>\n",
       "      <td>0.100341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.091080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.090116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.013700</td>\n",
       "      <td>0.088489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.081084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.012100</td>\n",
       "      <td>0.086035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.008100</td>\n",
       "      <td>0.526182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.510486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.689733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.698055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ACC     F1    REC   SPEC\n",
      "N  0.993  0.982  0.987  0.994\n",
      "L  0.997  0.992  0.988  0.999\n",
      "R  0.998  0.995  0.996  0.999\n",
      "V  0.992  0.980  0.980  0.995\n",
      "Q  0.998  0.994  0.991  0.999\n",
      "\n",
      "════ MEAN±STD METRICS ════\n",
      "            ACC           F1          REC         SPEC\n",
      "N  0.992±0.003  0.979±0.007  0.978±0.010  0.995±0.001\n",
      "L  0.996±0.001  0.989±0.003  0.986±0.005  0.998±0.001\n",
      "R  0.997±0.001  0.993±0.003  0.995±0.002  0.998±0.001\n",
      "V  0.991±0.002  0.977±0.006  0.981±0.004  0.993±0.002\n",
      "Q  0.997±0.001  0.992±0.003  0.990±0.003  0.999±0.001\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random, numpy as np, torch, pandas as pd, matplotlib.pyplot as plt, gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from transformers import (BertTokenizer, BertForSequenceClassification,\n",
    "                          Trainer, TrainingArguments, set_seed)\n",
    "\n",
    "# ═════════════ KONFIGURASI ══════════════════════════════════════════════\n",
    "DATA_DIR   = \"/workspace/SPLIT_BEATS_NPY/train\"\n",
    "LABEL_MAP  = {'N':0,'L':1,'R':2,'V':3,'Q':4}\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "SEED       = 42\n",
    "N_SPLITS   = 5\n",
    "EPOCHS     = 20\n",
    "BATCH_SIZE = 32\n",
    "MAX_LEN    = 512\n",
    "OUTPUT_BASE= \"/workspace/HASIL_BERT/HASIL_5\"   # semua hasil per‑fold disini\n",
    "\n",
    "set_seed(SEED)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cls_names = list(LABEL_MAP.keys())\n",
    "\n",
    "# ═════════════ UTIL ═════════════════════════════════════════════════════\n",
    "\n",
    "def signal_to_text(sig: np.ndarray) -> str:\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "# ═════════════ LOAD DATA ════════════════════════════════════════════════\n",
    "files, labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    for f in glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\")):\n",
    "        files.append(f); labels.append(idx)\n",
    "files, labels = np.array(files), np.array(labels)\n",
    "print(f\"Loaded {len(files)} samples across {len(LABEL_MAP)} classes\")\n",
    "\n",
    "# ═════════════ TOKENISASI SEMUA DATA ════════════════════════════════════\n",
    "print(\"Tokenizing all samples …\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "all_ids, all_mask = [], []\n",
    "for f in files:\n",
    "    txt = signal_to_text(np.load(f))\n",
    "    enc = tokenizer(txt, padding=\"max_length\", truncation=True,\n",
    "                    max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    all_ids.append(enc[\"input_ids\"].squeeze(0))\n",
    "    all_mask.append(enc[\"attention_mask\"].squeeze(0))\n",
    "all_ids  = torch.stack(all_ids)\n",
    "all_mask = torch.stack(all_mask)\n",
    "labels_t = torch.tensor(labels)\n",
    "\n",
    "# ═════════════ K‑FOLD TRAINING ═════════════════════════════════════════=\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "fold_metrics, fold_predictions = [], {}\n",
    "\n",
    "for fold,(tr,va) in enumerate(skf.split(all_ids, labels),1):\n",
    "    print(f\"\\n════ FOLD {fold}/{N_SPLITS} ════\")\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "    train_ds = torch.utils.data.TensorDataset(all_ids[tr], all_mask[tr], labels_t[tr])\n",
    "    val_ds   = torch.utils.data.TensorDataset(all_ids[va], all_mask[va], labels_t[va])\n",
    "\n",
    "    def collate(batch):\n",
    "        ids, msk, lbl = map(torch.stack, zip(*batch))\n",
    "        return {\"input_ids\": ids, \"attention_mask\": msk, \"labels\": lbl}\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_MAP)).to(DEVICE)\n",
    "\n",
    "    out_dir = os.path.join(OUTPUT_BASE, f\"fold{fold}\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir            = out_dir,\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        per_device_eval_batch_size  = BATCH_SIZE,\n",
    "        num_train_epochs      = EPOCHS,\n",
    "        evaluation_strategy   = \"epoch\",\n",
    "        save_strategy         = \"epoch\",     # ⬅️ simpan model tiap epoch\n",
    "        save_total_limit      = 1,\n",
    "        load_best_model_at_end= True,\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "        seed                 = SEED,\n",
    "        logging_strategy     = \"epoch\",\n",
    "        report_to            = []\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(model=model, args=args,\n",
    "                      train_dataset=train_ds,\n",
    "                      eval_dataset=val_ds,\n",
    "                      data_collator=collate)\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # simpan best model & tokenizer (jaga-jaga)\n",
    "    trainer.save_model(out_dir)      # menyimpan pytorch_model.bin & config.json\n",
    "    tokenizer.save_pretrained(out_dir)\n",
    "\n",
    "    # ═════════════ EVALUASI ═════════════════════════════════════════════\n",
    "    preds = torch.argmax(torch.from_numpy(trainer.predict(val_ds).predictions), dim=1).numpy()\n",
    "    y_true = labels_t[va].cpu().numpy()\n",
    "    fold_predictions[fold] = {\"y_true\": y_true, \"y_pred\": preds}\n",
    "\n",
    "    cm = confusion_matrix(y_true, preds, labels=list(range(len(cls_names))))\n",
    "\n",
    "    per_cls = {}\n",
    "    for i,cls in enumerate(cls_names):\n",
    "        TP = cm[i,i]; FN = cm[i].sum()-TP; FP = cm[:,i].sum()-TP; TN = cm.sum()-(TP+FN+FP)\n",
    "        acc  = (TP+TN)/cm.sum(); rec = TP/(TP+FN+1e-8); spec = TN/(TN+FP+1e-8); f1 = f1_score(y_true, preds, labels=[i], average='macro')\n",
    "        per_cls[cls] = {'ACC':acc,'F1':f1,'REC':rec,'SPEC':spec}\n",
    "    fold_metrics.append(per_cls)\n",
    "\n",
    "    # simpan confusion matrix\n",
    "    plt.figure(figsize=(7,6))\n",
    "    plt.imshow(cm, cmap='Blues'); plt.title(f\"Confusion Fold {fold}\")\n",
    "    plt.xticks(range(len(cls_names)), cls_names); plt.yticks(range(len(cls_names)), cls_names)\n",
    "    for r in range(len(cm)):\n",
    "        for c in range(len(cm)):\n",
    "            plt.text(c, r, cm[r,c], ha='center', va='center', color='black')\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"confusion_fold.png\")); plt.close()\n",
    "\n",
    "    # print tabel metrik\n",
    "    print(pd.DataFrame(per_cls).T.round(3))\n",
    "\n",
    "    # kosongkan memori\n",
    "    model.cpu(); del trainer; torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# ═════════════ RINGKASAN AKHIR ══════════════════════════════════════════\n",
    "summary = {c:{m:[] for m in ['ACC','F1','REC','SPEC']} for c in cls_names}\n",
    "for fm in fold_metrics:\n",
    "    for c in cls_names:\n",
    "        for m in summary[c]:\n",
    "            summary[c][m].append(fm[c][m])\n",
    "\n",
    "df_summary = pd.DataFrame({c:{m:f\"{np.mean(v):.3f}±{np.std(v):.3f}\" for m,v in summary[c].items()} for c in cls_names}).T\n",
    "print(\"\\n════ MEAN±STD METRICS ════\\n\", df_summary)\n",
    "\n",
    "df_summary.to_csv(os.path.join(OUTPUT_BASE, \"final_summary.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f82bcdc8-e85f-4d61-93b1-2338e2dc04dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/workspace/HASIL/HASIL_3/fold5/tokenizer_config.json',\n",
       " '/workspace/HASIL/HASIL_3/fold5/special_tokens_map.json',\n",
       " '/workspace/HASIL/HASIL_3/fold5/vocab.txt',\n",
       " '/workspace/HASIL/HASIL_3/fold5/added_tokens.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setelah trainer.train() tiap fold dan sebelum model dihapus:\n",
    "model.save_pretrained(output_dir)          # simpan pytorch_model.bin + config.json\n",
    "tokenizer.save_pretrained(output_dir)      # simpan tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "385f0b24-89c2-4337-977c-2eecd6d4a37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    fold kelas  akurasi      f1  recall  spesifisitas\n",
      "0      1     N   0.9877  0.9690  0.9616        0.9942\n",
      "1      1     L   0.9943  0.9857  0.9821        0.9973\n",
      "2      1     R   0.9971  0.9929  0.9982        0.9969\n",
      "3      1     V   0.9880  0.9704  0.9812        0.9897\n",
      "4      1     Q   0.9975  0.9937  0.9884        0.9998\n",
      "5      2     N   0.9957  0.9893  0.9875        0.9978\n",
      "6      2     L   0.9975  0.9938  0.9937        0.9984\n",
      "7      2     R   0.9984  0.9960  0.9955        0.9991\n",
      "8      2     V   0.9943  0.9858  0.9884        0.9958\n",
      "9      2     Q   0.9984  0.9960  0.9955        0.9991\n",
      "10     3     N   0.9902  0.9754  0.9732        0.9944\n",
      "11     3     L   0.9952  0.9879  0.9857        0.9975\n",
      "12     3     R   0.9970  0.9924  0.9911        0.9984\n",
      "13     3     V   0.9891  0.9730  0.9812        0.9911\n",
      "14     3     Q   0.9961  0.9902  0.9875        0.9982\n",
      "15     4     N   0.9918  0.9794  0.9786        0.9951\n",
      "16     4     L   0.9948  0.9869  0.9786        0.9989\n",
      "17     4     R   0.9952  0.9880  0.9946        0.9953\n",
      "18     4     V   0.9895  0.9737  0.9750        0.9931\n",
      "19     4     Q   0.9952  0.9880  0.9893        0.9967\n",
      "20     5     N   0.9929  0.9822  0.9875        0.9942\n",
      "21     5     L   0.9968  0.9919  0.9884        0.9989\n",
      "22     5     R   0.9980  0.9951  0.9955        0.9987\n",
      "23     5     V   0.9920  0.9799  0.9804        0.9949\n",
      "24     5     Q   0.9975  0.9937  0.9911        0.9991\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Gabungkan semua hasil fold ke dalam satu DataFrame                                                   \n",
    "rows = []\n",
    "for i, fold_data in enumerate(fold_metrics, 1):\n",
    "    for cls, metrics in fold_data.items():\n",
    "        row = {\n",
    "            'fold': i,\n",
    "            'kelas': cls,\n",
    "            'akurasi': round(metrics['ACC'], 4),\n",
    "            'f1': round(metrics['F1'], 4),\n",
    "            'recall': round(metrics['REC'], 4),\n",
    "            'spesifisitas': round(metrics['SPEC'], 4)\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "# Buat DataFrame\n",
    "df_all_folds = pd.DataFrame(rows)\n",
    "\n",
    "# Simpan ke file CSV\n",
    "df_all_folds.to_csv(\"/workspace/HASIL_BERT/HASIL_5/final_summary.csv\", index=False)\n",
    "print(df_all_folds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef91aefe-3f30-4e7d-ba94-aa7134fe8f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold terbaik adalah FOLD 2 dengan rata-rata akurasi: 0.9969\n",
      "\n",
      "Detail metrik pada Fold terbaik:\n",
      "   fold kelas  akurasi      f1  recall  spesifisitas\n",
      "5     2     N   0.9957  0.9893  0.9875        0.9978\n",
      "6     2     L   0.9975  0.9938  0.9937        0.9984\n",
      "7     2     R   0.9984  0.9960  0.9955        0.9991\n",
      "8     2     V   0.9943  0.9858  0.9884        0.9958\n",
      "9     2     Q   0.9984  0.9960  0.9955        0.9991\n"
     ]
    }
   ],
   "source": [
    "# Hitung rata-rata akurasi per fold\n",
    "avg_acc_per_fold = df_all_folds.groupby(\"fold\")[\"akurasi\"].mean()\n",
    "\n",
    "# Cari fold dengan rata-rata akurasi tertinggi\n",
    "best_fold = avg_acc_per_fold.idxmax()\n",
    "best_acc = avg_acc_per_fold.max()\n",
    "\n",
    "print(f\"Fold terbaik adalah FOLD {best_fold} dengan rata-rata akurasi: {best_acc:.4f}\")\n",
    "\n",
    "# Tampilkan seluruh metrik pada fold terbaik\n",
    "best_fold_df = df_all_folds[df_all_folds[\"fold\"] == best_fold]\n",
    "print(\"\\nDetail metrik pada Fold terbaik:\")\n",
    "print(best_fold_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81e3811d-7dac-490b-af0d-4afb753009e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a502330b-34e0-4af9-bc7c-2826263e1355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Menampilkan confusion matrix untuk Fold 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Ambil prediksi dan label asli untuk fold terbaik (asumsi kamu simpan sebelumnya)\n",
    "# Misalnya disimpan dalam dict bernama fold_predictions\n",
    "# Format: fold_predictions[fold] = {\"y_true\": [...], \"y_pred\": [...]}\n",
    "# Jika belum punya, kamu perlu menyimpan y_true dan y_pred per fold saat training\n",
    "\n",
    "# Cek fold terbaik\n",
    "best_fold = df_all_folds.groupby(\"fold\")[\"akurasi\"].mean().idxmax()\n",
    "print(f\"Menampilkan confusion matrix untuk Fold {best_fold}\")\n",
    "\n",
    "# Ambil data y_true dan y_pred dari fold terbaik\n",
    "y_true = fold_predictions[best_fold][\"y_true\"]\n",
    "y_pred = fold_predictions[best_fold][\"y_pred\"]\n",
    "class_names = [\"N\", \"L\", \"R\", \"V\", \"Q\"]\n",
    "\n",
    "# Buat confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f\"Confusion Matrix – Fold {best_fold}\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eddc6ddd-7fb0-4349-ba07-538159fb0997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Fold 1 Model ===\n",
      "           precision    recall  f1-score\n",
      "N           0.980132  0.951429  0.965567\n",
      "L           0.970423  0.984286  0.977305\n",
      "R           0.988547  0.986429  0.987487\n",
      "V           0.941917  0.961429  0.951573\n",
      "Q           0.988530  0.985000  0.986762\n",
      "accuracy    0.973714  0.973714  0.973714\n",
      "macro avg   0.973910  0.973714  0.973739\n",
      "\n",
      "=== Evaluating Fold 2 Model ===\n",
      "           precision    recall  f1-score\n",
      "N           0.966138  0.957857  0.961980\n",
      "L           0.967156  0.988571  0.977746\n",
      "R           0.990007  0.990714  0.990361\n",
      "V           0.959971  0.959286  0.959628\n",
      "Q           0.989138  0.975714  0.982380\n",
      "accuracy    0.974429  0.974429  0.974429\n",
      "macro avg   0.974482  0.974429  0.974419\n",
      "\n",
      "=== Evaluating Fold 3 Model ===\n",
      "           precision    recall  f1-score\n",
      "N           0.971719  0.957143  0.964376\n",
      "L           0.986438  0.987143  0.986790\n",
      "R           0.986506  0.992143  0.989316\n",
      "V           0.961675  0.967857  0.964756\n",
      "Q           0.987170  0.989286  0.988227\n",
      "accuracy    0.978714  0.978714  0.978714\n",
      "macro avg   0.978702  0.978714  0.978693\n",
      "\n",
      "=== Evaluating Fold 4 Model ===\n",
      "           precision    recall  f1-score\n",
      "N           0.975273  0.957857  0.966486\n",
      "L           0.979315  0.980714  0.980014\n",
      "R           0.992811  0.986429  0.989609\n",
      "V           0.949721  0.971429  0.960452\n",
      "Q           0.985000  0.985000  0.985000\n",
      "accuracy    0.976286  0.976286  0.976286\n",
      "macro avg   0.976424  0.976286  0.976312\n",
      "\n",
      "=== Evaluating Fold 5 Model ===\n",
      "           precision    recall  f1-score\n",
      "N           0.981036  0.960714  0.970769\n",
      "L           0.987152  0.987857  0.987504\n",
      "R           0.986496  0.991429  0.988956\n",
      "V           0.955758  0.972143  0.963881\n",
      "Q           0.991410  0.989286  0.990347\n",
      "accuracy    0.980286  0.980286  0.980286\n",
      "macro avg   0.980370  0.980286  0.980291\n",
      "\n",
      "✅ Selesai evaluasi semua fold terhadap data test.\n"
     ]
    }
   ],
   "source": [
    "import os, torch, numpy as np, glob\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# ════ KONFIGURASI =====================================================================\n",
    "TEST_DIR    = \"/workspace/SPLIT_BEATS_NPY/test\"\n",
    "MODEL_DIR   = \"/workspace/HASIL_BERT/HASIL_4\"\n",
    "LABEL_MAP   = {'N': 0, 'L': 1, 'R': 2, 'V': 3, 'Q': 4}\n",
    "MODEL_NAME  = \"bert-base-uncased\"\n",
    "MAX_LEN     = 512\n",
    "BATCH_SIZE  = 32\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cls_names   = list(LABEL_MAP.keys())\n",
    "\n",
    "# ════ TOKENIZER =======================================================================\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def signal_to_text(sig: np.ndarray) -> str:\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "# ════ LOAD TEST DATA ==================================================================\n",
    "test_files, test_labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    for f in glob.glob(os.path.join(TEST_DIR, cls, \"*.npy\")):\n",
    "        test_files.append(f)\n",
    "        test_labels.append(idx)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "texts = [signal_to_text(np.load(f)) for f in test_files]\n",
    "encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n",
    "input_ids = encodings['input_ids']\n",
    "attention_mask = encodings['attention_mask']\n",
    "\n",
    "# ════ UJI SETIAP FOLD =================================================================\n",
    "results_per_fold = []\n",
    "\n",
    "for fold in range(1, 6):\n",
    "    print(f\"\\n=== Evaluating Fold {fold} Model ===\")\n",
    "\n",
    "    # Load model\n",
    "    model_path = os.path.join(MODEL_DIR, f\"fold{fold}\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Predict in batches\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), BATCH_SIZE):\n",
    "            ids = input_ids[i:i+BATCH_SIZE].to(DEVICE)\n",
    "            att = attention_mask[i:i+BATCH_SIZE].to(DEVICE)\n",
    "            outputs = model(ids, attention_mask=att)\n",
    "            logits = outputs.logits\n",
    "            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "            preds.extend(batch_preds)\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    report = classification_report(test_labels, preds, target_names=cls_names, output_dict=True)\n",
    "    df = pd.DataFrame(report).T\n",
    "    df.to_csv(os.path.join(model_path, f\"test_report_fold{fold}.csv\"))\n",
    "    results_per_fold.append(df)\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(test_labels, preds)\n",
    "    plt.figure(figsize=(7,6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cls_names, yticklabels=cls_names)\n",
    "    plt.title(f\"Confusion Matrix Fold {fold} (Test Set)\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(model_path, f\"conf_matrix_test_fold{fold}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(df[['precision', 'recall', 'f1-score']].iloc[:-1])  # Print class-wise results\n",
    "\n",
    "print(\"\\n✅ Selesai evaluasi semua fold terhadap data test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abc40a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0712 06:04:49.698000 16340 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model dari: D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\HASIL_BERT\\HASIL_5\\fold2\n",
      "[INFO] Jumlah data test: 7000\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           N     0.9870    0.9764    0.9817      1400\n",
      "           L     0.9914    0.9900    0.9907      1400\n",
      "           R     0.9993    0.9950    0.9971      1400\n",
      "           V     0.9658    0.9879    0.9767      1400\n",
      "           Q     0.9935    0.9871    0.9903      1400\n",
      "\n",
      "    accuracy                         0.9873      7000\n",
      "   macro avg     0.9874    0.9873    0.9873      7000\n",
      "weighted avg     0.9874    0.9873    0.9873      7000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Metrik disimpan di: D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_AKHIR_RECORD\\BEATS\\EVALUASI_BERT_3\n",
      "   Accuracy  Recall  Specificity      F1\n",
      "N    0.9927  0.9764       0.9968  0.9817\n",
      "L    0.9963  0.9900       0.9979  0.9907\n",
      "R    0.9989  0.9950       0.9998  0.9971\n",
      "V    0.9906  0.9879       0.9912  0.9767\n",
      "Q    0.9961  0.9871       0.9984  0.9903\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np, pandas as pd, torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "MODEL_BASE = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\HASIL_BERT\\HASIL_5\"\n",
    "FOLD_NAME  = \"fold2\"  # << GANTI sesuai fold yang ingin kamu pakai\n",
    "TEST_DIR   = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\DATA\\output_coba\\SPLIT_BEATS_NPY\\Beats_TEST\"\n",
    "OUTPUT_EVAL_DIR = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_AKHIR_RECORD\\BEATS\\EVALUASI_BERT_3\"\n",
    "os.makedirs(OUTPUT_EVAL_DIR, exist_ok=True)\n",
    "\n",
    "LABEL_MAP  = {'N':0, 'L':1, 'R':2, 'V':3, 'Q':4}\n",
    "IDX2LABEL  = {v: k for k, v in LABEL_MAP.items()}\n",
    "cls_names  = list(LABEL_MAP.keys())\n",
    "MAX_LEN    = 512\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === UTILITAS ===\n",
    "def signal_to_text(sig: np.ndarray) -> str:\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "def load_test_data():\n",
    "    files, labels = [], []\n",
    "    for cls, idx in LABEL_MAP.items():\n",
    "        folder_path = os.path.join(TEST_DIR, cls)\n",
    "        if not os.path.exists(folder_path):\n",
    "            print(f\"[WARNING] Folder tidak ditemukan: {folder_path}\")\n",
    "            continue\n",
    "        file_list = glob.glob(os.path.join(folder_path, \"*.npy\"))\n",
    "        files.extend(file_list)\n",
    "        labels.extend([idx] * len(file_list))\n",
    "    return files, labels\n",
    "\n",
    "# === LOAD MODEL DAN TOKENIZER ===\n",
    "model_path = os.path.join(MODEL_BASE, FOLD_NAME)\n",
    "print(f\"[INFO] Loading model dari: {model_path}\")\n",
    "model = BertForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# === LOAD DATA TEST ===\n",
    "test_files, test_labels = load_test_data()\n",
    "print(f\"[INFO] Jumlah data test: {len(test_files)}\")\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for i, (file, label) in enumerate(zip(test_files, test_labels)):\n",
    "    try:\n",
    "        signal = np.load(file)\n",
    "        txt = signal_to_text(signal)\n",
    "        enc = tokenizer(txt, padding=\"max_length\", truncation=True,\n",
    "                        max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(**enc)\n",
    "            pred = output.logits.argmax(dim=1).item()\n",
    "        y_true.append(label)\n",
    "        y_pred.append(pred)\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Gagal memproses file {file}: {e}\")\n",
    "\n",
    "# === EVALUASI AKHIR ===\n",
    "if len(y_true) == 0:\n",
    "    print(\"[ERROR] Tidak ada data yang berhasil diprediksi.\")\n",
    "else:\n",
    "    print(\"\\n=== Classification Report ===\")\n",
    "    print(classification_report(y_true, y_pred, labels=list(LABEL_MAP.values()), target_names=cls_names, digits=4))\n",
    "\n",
    "    # CONFUSION MATRIX\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(LABEL_MAP.values()))\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - TEST SET\")\n",
    "    plt.xticks(range(len(cls_names)), cls_names)\n",
    "    plt.yticks(range(len(cls_names)), cls_names)\n",
    "    for r in range(len(cm)):\n",
    "        for c in range(len(cm)):\n",
    "            plt.text(c, r, cm[r, c], ha='center', va='center', color='black')\n",
    "    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(os.path.join(OUTPUT_EVAL_DIR, \"confusion_matrix_test.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # SIMPAN METRIK PER KELAS\n",
    "    df_metrics = pd.DataFrame(columns=[\"Accuracy\", \"Recall\", \"Specificity\", \"F1\"])\n",
    "    for i, cls in enumerate(cls_names):\n",
    "        TP = cm[i, i]\n",
    "        FN = cm[i].sum() - TP\n",
    "        FP = cm[:, i].sum() - TP\n",
    "        TN = cm.sum() - (TP + FN + FP)\n",
    "        acc  = (TP + TN) / cm.sum()\n",
    "        rec  = TP / (TP + FN + 1e-8)\n",
    "        spec = TN / (TN + FP + 1e-8)\n",
    "        f1   = f1_score(np.array(y_true) == i, np.array(y_pred) == i)\n",
    "        df_metrics.loc[cls] = [acc, rec, spec, f1]\n",
    "\n",
    "    df_metrics = df_metrics.round(4)\n",
    "    df_metrics.to_csv(os.path.join(OUTPUT_EVAL_DIR, \"test_metrics.csv\"))\n",
    "    print(f\"\\n✅ Metrik disimpan di: {OUTPUT_EVAL_DIR}\")\n",
    "    print(df_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e822098",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 62\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_true, y_pred\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Evaluasi model beats dan rhythm\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m y_true_beats, y_pred_beats   \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_BEATS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_BEATS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_map_beats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrev_map_beats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m y_true_rythm, y_pred_rythm   \u001b[38;5;241m=\u001b[39m evaluate_model(MODEL_RYTHM, DATA_RYTHM, label_map_rythm, rev_map_rythm)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Gabungkan hasil\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model_path, data_path, local_map, rev_map)\u001b[0m\n\u001b[0;32m     48\u001b[0m enc \u001b[38;5;241m=\u001b[39m tokenizer(txt, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m                 max_length\u001b[38;5;241m=\u001b[39mMAX_LEN, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 51\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     53\u001b[0m global_true \u001b[38;5;241m=\u001b[39m label_map_global[rev_map[idx]]\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1665\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1657\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1660\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1661\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1662\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1663\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1677\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1679\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1142\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:695\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    684\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    685\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    686\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m         output_attentions,\n\u001b[0;32m    693\u001b[0m     )\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 695\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    706\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    584\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 585\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    594\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:515\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    507\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 515\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    438\u001b[0m )\n\u001b[1;32m--> 440\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_prob\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "MODEL_BEATS  = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\BEATS\\HASIL_BERT\\HASIL_5\\fold2\"\n",
    "MODEL_RYTHM  = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\RYTHM\\HASIL_Final_BERT_RYTHM\\HASIL_Final_BERT_RYTHM\\fold5\\model\"\n",
    "DATA_BEATS   = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\DATA\\output_coba\\SPLIT_BEATS_NPY\\Beats_TEST\"\n",
    "DATA_RYTHM   = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\SPLIT_SLIDING_FINAL\\SPLIT_SLIDING_FINAL\\test\"\n",
    "OUTPUT_DIR   = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_AKHIR_RECORD\\GABUNGAN\\BERT\"\n",
    "MAX_LEN      = 512\n",
    "DEVICE       = torch.device(\"cpu\")  # paksa CPU agar tidak error\n",
    "\n",
    "# Label global gabungan beats + rhythm\n",
    "label_map_global = {'N':0, 'L':1, 'R':2, 'V':3, 'Q':4, 'AFIB':5, 'VT':6, 'VFL':7}\n",
    "idx_to_label     = {v:k for k,v in label_map_global.items()}\n",
    "label_map_beats  = {'N':0, 'L':1, 'R':2, 'V':3, 'Q':4}\n",
    "label_map_rythm  = {'AFIB':0, 'VT':1, 'VFL':2}\n",
    "rev_map_beats    = {v:k for k,v in label_map_beats.items()}\n",
    "rev_map_rythm    = {v:k for k,v in label_map_rythm.items()}\n",
    "\n",
    "# Fungsi konversi sinyal ke bentuk token teks\n",
    "def signal_to_text(sig: np.ndarray) -> str:\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "# Evaluasi model\n",
    "def evaluate_model(model_path, data_path, local_map, rev_map):\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path).to(DEVICE)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for cls, idx in local_map.items():\n",
    "        folder = os.path.join(data_path, cls)\n",
    "        if not os.path.exists(folder):\n",
    "            continue\n",
    "        files = glob.glob(os.path.join(folder, \"*.npy\"))\n",
    "        for file in files:\n",
    "            try:\n",
    "                sig = np.load(file)\n",
    "                txt = signal_to_text(sig)\n",
    "                enc = tokenizer(txt, padding=\"max_length\", truncation=True,\n",
    "                                max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    output = model(**enc)\n",
    "                    pred = output.logits.argmax(dim=1).item()\n",
    "                global_true = label_map_global[rev_map[idx]]\n",
    "                global_pred = label_map_global[rev_map[pred]]\n",
    "                y_true.append(global_true)\n",
    "                y_pred.append(global_pred)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Gagal memproses file {file}: {e}\")\n",
    "    return y_true, y_pred\n",
    "\n",
    "# Evaluasi model beats dan rhythm\n",
    "y_true_beats, y_pred_beats   = evaluate_model(MODEL_BEATS, DATA_BEATS, label_map_beats, rev_map_beats)\n",
    "y_true_rythm, y_pred_rythm   = evaluate_model(MODEL_RYTHM, DATA_RYTHM, label_map_rythm, rev_map_rythm)\n",
    "\n",
    "# Gabungkan hasil\n",
    "y_true = y_true_beats + y_true_rythm\n",
    "y_pred = y_pred_beats + y_pred_rythm\n",
    "\n",
    "# Simpan hasil ke folder\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Classification report\n",
    "report = classification_report(y_true, y_pred, target_names=list(label_map_global.keys()), digits=4, output_dict=True)\n",
    "df_report = pd.DataFrame(report).transpose()\n",
    "df_report.to_csv(os.path.join(OUTPUT_DIR, \"classification_report_gabungan.csv\"))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(label_map_global))))\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(cm, cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Gabungan Beats & Rhythm\")\n",
    "plt.xticks(ticks=list(range(len(label_map_global))), labels=label_map_global.keys())\n",
    "plt.yticks(ticks=list(range(len(label_map_global))), labels=label_map_global.keys())\n",
    "for i in range(len(cm)):\n",
    "    for j in range(len(cm)):\n",
    "        plt.text(j, i, cm[i, j], ha='center', va='center', color='black')\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix_gabungan.png\"))\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d403b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === PATH FILE EXCEL ===\n",
    "beats_path  = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\hasil_Bert_beats.xlsx\"\n",
    "rythm_path  = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\hasil_Bert_rythm (1).xlsx\"\n",
    "\n",
    "# === BACA DATA ===\n",
    "beats_df = pd.read_excel(beats_path)\n",
    "rythm_df = pd.read_excel(rythm_path)\n",
    "\n",
    "# === GRAFIK BEATS ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(beats_df[\"Epoch\"], beats_df[\"Training Loss\"], marker='o', label=\"Train Loss\", color='blue')\n",
    "plt.plot(beats_df[\"Epoch\"], beats_df[\"Validation Loss\"], marker='o', label=\"Validation Loss\", color='orange')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Validation Loss - BERT (Beats)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\grafik_loss_bert_beats.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# === GRAFIK RYTHM ===\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(rythm_df[\"Epoch\"], rythm_df[\"Training Loss\"], marker='o', label=\"Train Loss\", color='green')\n",
    "plt.plot(rythm_df[\"Epoch\"], rythm_df[\"Validation Loss\"], marker='o', label=\"Validation Loss\", color='red')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Train vs Validation Loss - BERT (Rhythm)\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\grafik_loss_bert_rythm.png\", dpi=300)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
