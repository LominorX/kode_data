{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b160b7-3492-4812-b27b-36fce1d29922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: transformers==4.48.2 in /usr/local/lib/python3.10/dist-packages (4.48.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
      "Requirement already satisfied: accelerate==0.26.0 in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.34.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2025.7.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.5.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 torch transformers==4.48.2 scikit-learn accelerate==0.26.0 matplotlib tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db2c2f31-7e88-406b-bdc6-48b8d0ef1ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\nOnly a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:36\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_model\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_balanced_memory, infer_auto_device_map\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.26.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SAFE_MODEL_NAME,\n\u001b[1;32m     29\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[1;32m     30\u001b[0m     SAMPLER_NAME,\n\u001b[1;32m     31\u001b[0m     SCALER_NAME,\n\u001b[1;32m     32\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     33\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     34\u001b[0m     get_pretty_name,\n\u001b[1;32m     35\u001b[0m     is_tpu_available,\n\u001b[1;32m     36\u001b[0m     is_xpu_available,\n\u001b[1;32m     37\u001b[0m     save,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py:152\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    154\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    155\u001b[0m     _filter_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     prepare_tpu,\n\u001b[1;32m    161\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/fsdp_utils.py:25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, FSDP_PYTORCH_VERSION) \u001b[38;5;129;01mand\u001b[39;00m is_torch_distributed_available():\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist_cp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_planner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultLoadPlanner, DefaultSavePlanner\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     TensorStorageMetadata,\n\u001b[1;32m      3\u001b[0m     BytesStorageMetadata,\n\u001b[1;32m      4\u001b[0m     ChunkStorageMetadata,\n\u001b[1;32m      5\u001b[0m     Metadata,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_dict_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_state_dict, load\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_dict_saver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_state_dict, save\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplanner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoadPlanner\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_planner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultLoadPlanner\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _DistWrapper, _all_gather_keys\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/default_planner.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m narrow_tensor_by_index\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTensor\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplanner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     SavePlanner,\n\u001b[1;32m     19\u001b[0m     LoadPlanner,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     WriteItemType,\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/ops/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/ops/embedding_ops.py:5\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpSchema, OutputSharding\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_prop_rule\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/op_schema.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplacement_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTensorSpec\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_mesh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceMesh\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/placement_types.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functional_collectives\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfuncol\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_c10d\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mc10d\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_functional_collectives.py:647\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_running_with_deploy():\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Library MUST be defined at module scope or it doesn't work\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# Creating a \"DEF\" Library always crashes torch::deploy so we create our Library instances here\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m#   guarded against running inside it\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     c10_lib \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc10d_functional\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDEF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     c10_lib_impl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mLibrary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc10d_functional\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/library.py:63\u001b[0m, in \u001b[0;36mLibrary.__init__\u001b[0;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[1;32m     62\u001b[0m filename, lineno \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mfilename, frame\u001b[38;5;241m.\u001b[39mlineno\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineno\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns \u001b[38;5;241m=\u001b[39m ns\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:47\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     38\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:47\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileConfig, GenerationConfig, GenerationMixin\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attention_forward\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.peft because of the following error (look up to see its traceback):\nOnly a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1806\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1808\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\nOnly a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241"
     ]
    }
   ],
   "source": [
    "import os, torch, numpy as np, glob\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === KONFIGURASI ====================================================================\n",
    "DATA_DIR    = \"/workspace/SPLIT_RHYTHM_NPY/train\"\n",
    "MODEL_BASE  = \"/workspace/HASIL_BERT_RHYTHM_Default/HASIL_UjiCoba\"\n",
    "LABEL_MAP   = {'AFIB': 0, 'VFL': 1, 'VT': 2}\n",
    "MODEL_NAME  = \"bert-base-uncased\"\n",
    "MAX_LEN     = 512\n",
    "EPOCHS      = 2\n",
    "BATCH_SIZE  = 16\n",
    "SEED        = 42\n",
    "\n",
    "os.makedirs(MODEL_BASE, exist_ok=True)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "cls_names = list(LABEL_MAP.keys())\n",
    "\n",
    "# === Dataset Custom ==================================================================\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, files, labels):\n",
    "        self.texts = [self.signal_to_text(np.load(f)) for f in files]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def signal_to_text(sig):\n",
    "        norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "        return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum(axis=1) - np.diag(cm)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    spec = tn / (tn + fp + 1e-8)\n",
    "    return spec.mean()\n",
    "\n",
    "# === Load Data =========================================================================\n",
    "all_files, all_labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    files = glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\"))\n",
    "    all_files.extend(files)\n",
    "    all_labels.extend([idx] * len(files))\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "final_summary = []\n",
    "\n",
    "# === Training per Fold ==================================================================\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_files, all_labels), 1):\n",
    "    print(f\"\\n==== Training Fold {fold} ====\")\n",
    "\n",
    "    train_files = [all_files[i] for i in train_idx]\n",
    "    val_files   = [all_files[i] for i in val_idx]\n",
    "    train_labels= all_labels[train_idx]\n",
    "    val_labels  = all_labels[val_idx]\n",
    "\n",
    "    train_dataset = ECGDataset(train_files, train_labels)\n",
    "    val_dataset   = ECGDataset(val_files, val_labels)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_MAP))\n",
    "    output_dir = os.path.join(MODEL_BASE, f\"fold{fold}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        no_cuda=False,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    "    )\n",
    "\n",
    "    # === Logging Manual ==================================================================\n",
    "    history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch {epoch}/{EPOCHS} ---\")\n",
    "        trainer.train()\n",
    "\n",
    "        train_pred = trainer.predict(train_dataset)\n",
    "        train_preds = np.argmax(train_pred.predictions, axis=1)\n",
    "        train_loss = train_pred.metrics['test_loss']\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "\n",
    "        val_pred = trainer.predict(val_dataset)\n",
    "        val_preds = np.argmax(val_pred.predictions, axis=1)\n",
    "        val_loss = val_pred.metrics['test_loss']\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "        val_recall = recall_score(val_labels, val_preds, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        val_spec = specificity_score(val_labels, val_preds)\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f} | Val Specificity: {val_spec:.4f}\")\n",
    "\n",
    "    # === Simpan Visual dan Evaluasi Fold =================================================\n",
    "    pd.DataFrame(history).to_csv(os.path.join(output_dir, f'history_fold{fold}.csv'), index=False)\n",
    "\n",
    "    plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    plt.title(f'Loss per Epoch Fold {fold}')\n",
    "    plt.savefig(os.path.join(output_dir, f'loss_curve_fold{fold}.png')); plt.close()\n",
    "\n",
    "    plt.plot(history['epoch'], history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['epoch'], history['val_acc'], label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "    plt.title(f'Accuracy per Epoch Fold {fold}')\n",
    "    plt.savefig(os.path.join(output_dir, f'acc_curve_fold{fold}.png')); plt.close()\n",
    "\n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cls_names, yticklabels=cls_names)\n",
    "    plt.title(f'Confusion Matrix Fold {fold}'); plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_fold{fold}.png')); plt.close()\n",
    "\n",
    "    report = classification_report(val_labels, val_preds, target_names=cls_names, output_dict=True)\n",
    "    pd.DataFrame(report).T.to_csv(os.path.join(output_dir, f'classification_report_fold{fold}.csv'))\n",
    "\n",
    "    final_summary.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': val_acc,\n",
    "        'recall': val_recall,\n",
    "        'f1_score': val_f1,\n",
    "        'specificity': val_spec\n",
    "    })\n",
    "\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\" Fold {fold} selesai. Model disimpan di {output_dir}\")\n",
    "\n",
    "# === Ringkasan Akhir ================================================================\n",
    "pd.DataFrame(final_summary).to_csv(os.path.join(MODEL_BASE, 'final_summary.csv'), index=False)\n",
    "print(f\"\\n Semua fold selesai. Ringkasan akhir disimpan di {MODEL_BASE}/final_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5887d0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0804 13:26:00.961000 17140 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluasi Model BERT Base (Best Model Fold 5) ===\n",
      "Akurasi     : 0.6042\n",
      "Recall      : 0.6042\n",
      "F1-score    : 0.5496\n",
      "Spesifisitas: 0.8021\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAJOCAYAAABBWYj1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASrRJREFUeJzt3XmcjfX///HnmX3MjBmMyT6WqbFECgnZd1FZsuVrbKUIFRVJtlA+KlEohpEoSyVLdtlKZWkshYw1uyyDwazX7w+/OTlmhvdMM3NoHvfbbW51rut9rut1nbmceZ739b7ex2ZZliUAAADckYuzCwAAALhXEJwAAAAMEZwAAAAMEZwAAAAMEZwAAAAMEZwAAAAMEZwAAAAMEZwAAAAMEZwAAAAMEZyQZfbv369GjRrJ399fNptNCxcuzNTtHz58WDabTREREZm63XtZnTp1VKdOHWeXgbuUzWbTSy+9lOX7Sf63OW7cuCzf1+389ddf8vLy0o8//ujUOu42y5cvl6+vr86ePevsUu5JBKf/uAMHDqhnz54qWbKkvLy8lDt3btWoUUMfffSRrl27lqX7DgsL065duzRq1CjNmjVLlStXztL9ZacuXbrIZrMpd+7cqb6O+/fvl81my/AfjxMnTmjYsGGKjIzMhGqzR/Hixe3HbLPZ5OXlpfvvv1+vvfaazp8/79B22LBhDm1v/Tl16pSkf/4AJ/+4uLgob968atq0qTZv3ixJioiIuO22kn+KFy+eZu3r1q1L0T5v3rx67LHHNHv27Cx7zf6tW19Hd3d3FS9eXH379tXFixezfP/ff/+9hg0bluX7yagRI0aoatWqqlGjRqq/47R+MsMff/yhYcOG6fDhw8bP2bRpk5o2barChQvLy8tLxYoVU4sWLTRnzpwM1TBp0qRUP1g2adJEISEhGjNmTIa2m9O5ObsAZJ2lS5fqmWeekaenpzp37qwHH3xQcXFx2rRpk1577TX9/vvv+uyzz7Jk39euXdPmzZs1ePDgLPuEGxwcrGvXrsnd3T1Ltn8nbm5uunr1qhYvXqy2bds6rJs9e7a8vLx0/fr1DG37xIkTGj58uIoXL66KFSsaP2/lypUZ2l9mqVixovr37y9Jun79urZt26bx48dr/fr1+vXXX1O0nzx5snx9fVMsDwgIcHjcoUMHNWvWTImJifrzzz81adIk1a1bV1u2bFGtWrU0a9Ysh/Y9evTQo48+queff96+LLX93Kpv376qUqWKJOncuXOaO3euOnXqpIsXL6p37953fL6zJL+OMTExWrNmjSZOnKjt27dr06ZNWbrf77//Xp988sldGZ7Onj2rmTNnaubMmZKkMmXKpDhPBg0aJF9fXw0ePDjT9//HH39o+PDhqlOnzm1De7L58+erXbt2qlixovr166c8efLo0KFD2rBhg6ZOnaqOHTumu4ZJkyYpMDBQXbp0SbGuZ8+eGjBggIYPHy4/P790bztHs/CfdPDgQcvX19cqXbq0deLEiRTr9+/fb40fPz7L9n/kyBFLkvW///0vy/bhTGFhYZaPj4/VqFEj6+mnn06x/v7777dat26d4ddgy5YtliRrxowZRu1jYmLSvY/MFhwcbD3xxBMplg8YMMCSZP3555/2ZUOHDrUkWWfPnr3tNg8dOpTqa7hs2TJLkvXiiy+m+jwfHx8rLCzMuPYffvjBkmTNnz/fYXlsbKxVuHBhq3r16sbbyk5pvY7t2rWzJFm//PKLw3JJVu/evTNt/71797ZS+zOS1u8tO33wwQeWt7e3dfny5TTblCtXzqpdu3aW7H/+/PmWJOuHH34wal+2bFmrXLlyVmxsbIp1p0+fzlANtzu+06dPW66urlZ4eHiGtp2TcanuP2rs2LG6cuWKwsPDVbBgwRTrQ0JC1K9fP/vjhIQEjRw5UqVKlZKnp6eKFy+uN998U7GxsQ7PK168uJo3b65Nmzbp0UcflZeXl0qWLKnPP//c3mbYsGEKDg6WJL322msOl0m6dOmS6qev5EsON1u1apUef/xxBQQEyNfXV6GhoXrzzTft69Ma47R27VrVrFlTPj4+CggI0FNPPaU9e/akur+oqCh16dJFAQEB8vf3V9euXXX16tW0X9hbdOzYUcuWLXO4LLJlyxbt378/1U+I58+f14ABA1S+fHn5+voqd+7catq0qXbs2GFvs27dOnuvR9euXe2XD5KPs06dOnrwwQe1bds21apVS7ly5bK/LreOcQoLC5OXl1eK42/cuLHy5MmjEydOGB9rRhUoUEDSjR66zFKzZk1JNy5FZyUPDw/lyZMnRe0zZsxQvXr1FBQUJE9PT5UtW1aTJ09O8fytW7eqcePGCgwMlLe3t0qUKKFu3bo5tElKStL48eNVrlw5eXl56b777lPPnj114cKFDNd9p9dn4cKFevDBB+Xp6aly5cpp+fLl9nU//PCDbDabvv322xTPmzNnjmw2mzZv3qwuXbrok08+kaTbXub67LPP7O8rVapU0ZYtWxzWd+nSRb6+vjp69KiaN28uX19fFS5c2L7tXbt2qV69evLx8VFwcLDxZauFCxeqatWqRj2NN7t48aJefvllFS1aVJ6engoJCdF7772npKQkh3ZfffWVKlWqJD8/P+XOnVvly5fXRx99JOnG5eNnnnlGklS3bl37a7Nu3bo093vgwAFVqVJFHh4eKdYFBQU5PDY5Z4oXL67ff/9d69evt+//5veGoKAgVahQQd999126Xh9wqe4/a/HixSpZsqSqV69u1L5Hjx6aOXOm2rRpo/79++uXX37RmDFjtGfPnhRvoFFRUWrTpo26d++usLAwTZ8+XV26dFGlSpVUrlw5tWrVSgEBAXrllVfsl1jS++b1+++/q3nz5qpQoYJGjBghT09PRUVF3XGQ5+rVq9W0aVOVLFlSw4YN07Vr1zRx4kTVqFFD27dvTxHa2rZtqxIlSmjMmDHavn27pk2bpqCgIL333ntGdbZq1UovvPCCvvnmG/sfxDlz5qh06dJ65JFHUrQ/ePCgFi5cqGeeeUYlSpTQ6dOn9emnn6p27dr6448/VKhQIZUpU0YjRozQ22+/reeff97+R/Dm3+W5c+fUtGlTtW/fXp06ddJ9992Xan0fffSR1q5dq7CwMG3evFmurq769NNPtXLlSs2aNUuFChUyOk5T8fHx+vvvvyXduFT322+/6YMPPlCtWrVUokSJFO1vHfsk3QhYt16qu1XyuJE8efL865pvdvnyZXv958+f15w5c7R7926Fh4c7tJs8ebLKlSunJ598Um5ublq8eLF69eqlpKQk+yW9M2fOqFGjRsqfP78GDhyogIAAHT58WN98843Dtnr27KmIiAh17dpVffv21aFDh/Txxx/rt99+048//pihS9G3e302bdqkb775Rr169ZKfn58mTJig1q1b6+jRo8qXL5/q1KmjokWLavbs2WrZsqXDc2fPnq1SpUqpWrVqkm5cUl61alWKS2DJ5syZo8uXL6tnz56y2WwaO3asWrVqpYMHDzocV2Jiopo2bapatWpp7Nixmj17tl566SX5+Pho8ODBevbZZ9WqVStNmTJFnTt3VrVq1VI9n5LFx8dry5YtevHFF9P1ul29elW1a9fW8ePH1bNnTxUrVkw//fSTBg0apJMnT2r8+PGSbnyo69Chg+rXr29/r9izZ49+/PFH9evXT7Vq1VLfvn01YcIEvfnmmypTpowk2f+bmuDgYK1Zs0bHjh1TkSJFblunyTkzfvx49enTx+FS5K3vE5UqVcr0m3ZyBGd3eSHzRUdHW5Ksp556yqh9ZGSkJcnq0aOHw/LkSyxr1661LwsODrYkWRs2bLAvO3PmjOXp6Wn179/fviytrvqwsDArODg4RQ3JlxySffjhh3e8lJO8j5svZ1WsWNEKCgqyzp07Z1+2Y8cOy8XFxercuXOK/XXr1s1hmy1btrTy5cuX5j5vPg4fHx/LsiyrTZs2Vv369S3LsqzExESrQIEC1vDhw1N9Da5fv24lJiamOA5PT09rxIgR9mW3u1RXu3ZtS5I1ZcqUVNfd2jW/YsUKS5L1zjvv2C/hpnZ58d9KPjdu/alRo4b1999/O7RNfv1T+wkNDbW3S34Nhw8fbp09e9Y6deqUtXHjRqtKlSqpXlpLltFLdbf+uLi4WKNGjUrR/urVqymWNW7c2CpZsqT98bfffmtJsrZs2ZLmfjdu3GhJsmbPnu2wfPny5akuv1Xy67hv3z7r7Nmz1uHDh63p06db3t7eVv78+VNcwpVkeXh4WFFRUfZlO3bssCRZEydOtC8bNGiQ5enpaV28eNG+7MyZM5abm5s1dOhQ+7I7XarLly+fdf78efvy7777zpJkLV682L4sLCzMkmSNHj3avuzChQuWt7e3ZbPZrK+++sq+fO/evZYkhxpSExUVleKYUnPrpayRI0daPj4+DpeVLcuyBg4caLm6ulpHjx61LMuy+vXrZ+XOndtKSEhIc9vpvVQXHh5u//3UrVvXGjJkiLVx48YU7xfpOWfudCly9OjRlqQMXwrMqbhU9x906dIlSTIe8Pf9999Lkl599VWH5cmDfJcuXeqwvGzZsvZeEEnKnz+/QkNDdfDgwQzXfKvkHofvvvsuRRd5Wk6ePKnIyEh16dJFefPmtS+vUKGCGjZsaD/Om73wwgsOj2vWrKlz587ZX0MTHTt21Lp163Tq1CmtXbtWp06dSnMgp6enp1xcbvyzS0xM1Llz5+yXIbdv3268T09PT3Xt2tWobaNGjdSzZ0+NGDFCrVq1kpeXlz799FPjfaVH1apVtWrVKq1atUpLlizRqFGj9Pvvv+vJJ59M9e7Dr7/+2t4++WfGjBkp2g0dOlT58+dXgQIFVLNmTe3Zs0fvv/++2rRpk6n1v/322/Y65s6dqw4dOmjw4MH2SzDJvL297f8fHR2tv//+W7Vr19bBgwcVHR0t6Z9zeMmSJYqPj091f/Pnz5e/v78aNmyov//+2/5TqVIl+fr66ocffjCqOzQ0VPnz51fx4sXVrVs3hYSEaNmyZcqVK1eKtg0aNFCpUqXsjytUqKDcuXM7/Pvt3LmzYmNjtWDBAvuyuXPnKiEhQZ06dTKqSZLatWvn0OuV/L6R2ntFjx497P8fEBCg0NBQ+fj4ONx4ERoaqoCAgDu+15w7d05S+nsk58+fr5o1aypPnjwOv48GDRooMTFRGzZssNcXExOjVatWpWv7t9OtWzctX75cderU0aZNmzRy5EjVrFlT999/v3766SeHGjPjnJH+eX2Se1lhhkt1/0G5c+eWdOOyg4kjR47IxcVFISEhDssLFCiggIAAHTlyxGF5sWLFUmwjT548/2pMxq3atWunadOmqUePHho4cKDq16+vVq1aqU2bNvbgkdpxSDfeXG9VpkwZrVixQjExMfLx8bEvv/VYkt9ILly4YH8d76RZs2by8/PT3LlzFRkZqSpVqigkJCTV25CTkpL00UcfadKkSTp06JASExPt6/Lly2e0P0kqXLhwqmMh0jJu3Dh99913ioyM1Jw5c1KMmUjN2bNnHerz9fW94yXXwMBANWjQwP74iSeeUGhoqNq0aaNp06apT58+Du1r1aqlwMDAO9by/PPP65lnntH169e1du1aTZgwwaG2zFK+fHmH+tu2bavo6GgNHDhQHTt2VP78+SVJP/74o4YOHarNmzenGBMXHR0tf39/1a5dW61bt9bw4cP14Ycfqk6dOnr66afVsWNHeXp6SroxbUV0dHSav48zZ84Y1f31118rd+7cOnv2rCZMmKBDhw45hLubmfz7LV26tKpUqaLZs2ere/fukm5cpnvsscdSvE/czu3+fd3My8vL/tom8/f3V5EiRVKMm/L39zd+r7Esy7hW6cbvY+fOnSlqSZb8++jVq5fmzZtnnzqgUaNGatu2rZo0aZKu/d2qcePGaty4sa5evapt27Zp7ty5mjJlipo3b669e/cqKCgo084Z6Z/XJ7OmYMgpCE7/Qblz51ahQoW0e/fudD3P9B+Pq6trqstN3qTS2setfwS9vb21YcMG/fDDD1q6dKmWL1+uuXPnql69elq5cmWaNaTXvzmWZJ6enmrVqpVmzpypgwcP3vbW7NGjR2vIkCHq1q2bRo4cqbx588rFxUUvv/yycc+apDT/KKblt99+s7+h7tq1Sx06dLjjc6pUqeIQmocOHZqh287r168vSdqwYUOK4GTq/vvvtwea5s2by9XVVQMHDlTdunWzfH6w+vXra8mSJfr111/1xBNP6MCBA6pfv75Kly6tDz74QEWLFpWHh4e+//57ffjhh/bfo81m04IFC/Tzzz9r8eLFWrFihbp166b3339fP//8s3x9fZWUlKSgoKA054pK6w/4rW4OoC1atFD58uX17LPPatu2bSk+aJie8507d1a/fv107NgxxcbG6ueff9bHH39sVE9695VWu4z++0z+EJLeD3NJSUlq2LChXn/99VTXP/DAA5JuDKyOjIzUihUrtGzZMi1btkwzZsxQ586d7dMf/Bu5cuVSzZo1VbNmTQUGBmr48OFatmyZwsLCMu2ckf55fUw+vOAfBKf/qObNm+uzzz7T5s2b7QM50xIcHKykpCTt37/fYfDi6dOndfHiRfsdcpkhT548qU7Md2uvliS5uLiofv36ql+/vj744AONHj1agwcP1g8//ODQK3DzcUjSvn37Uqzbu3evAgMDHXqbMlPHjh01ffp0ubi4qH379mm2W7BggerWrZtisPHFixcd3rwy8xNgTEyMunbtqrJly6p69eoaO3asWrZsab9zLy2zZ892uLxWsmTJDO0/ISFBknTlypUMPT81gwcP1tSpU/XWW2853BGWFW6tf/HixYqNjdWiRYscelTSukTy2GOP6bHHHtOoUaM0Z84cPfvss/rqq6/Uo0cPlSpVSqtXr1aNGjXSHYbT4uvrq6FDh6pr166aN2/ebc/H22nfvr1effVVffnll/b50tq1a+fQ5m7tqShWrJi8vb116NChdD2vVKlSunLlSqrvL7fy8PBQixYt1KJFCyUlJalXr1769NNPNWTIEIWEhGTaa5P8weDkyZP2Gk3PmTvVcOjQIQUGBqYrbIGZw/+zXn/9dfn4+KhHjx46ffp0ivUHDhywj9to1qyZJNnvGEn2wQcfSLpxuSWzlCpVStHR0dq5c6d92cmTJ1PcuZfa3VbJE0HeOkVCsoIFC6pixYqaOXOmQzjbvXu3Vq5caT/OrFC3bl2NHDlSH3/8sf32+9S4urqm+LQ8f/58HT9+3GFZcsDLjNmf33jjDR09elQzZ87UBx98oOLFiyssLCzN1zFZjRo11KBBA/tPRoPT4sWLJUkPPfRQhp6fmoCAAPXs2VMrVqzI8tnVlyxZIumf+pN7QW7+PUZHR6cYm3XhwoUUv+tbz+G2bdsqMTFRI0eOTLHfhISEDP/+n332WRUpUsT47tDUBAYGqmnTpvriiy80e/ZsNWnSJEXPRGaep5nJ3d1dlStX1tatW9P1vLZt22rz5s1asWJFinUXL160h+jkMVTJXFxcVKFCBUn//G7T+9qsWbMm1eXJYzOThyCk55zx8fG57f63bdt2xw/WSIkep/+oUqVKac6cOWrXrp3KlCnjMHP4Tz/9pPnz59tnk33ooYcUFhamzz77TBcvXlTt2rX166+/aubMmXr66adVt27dTKurffv2euONN9SyZUv17dtXV69e1eTJk/XAAw84DI4eMWKENmzYoCeeeELBwcE6c+aMJk2apCJFiujxxx9Pc/v/+9//1LRpU1WrVk3du3e3T0fg7++fpbMbu7i46K233rpju+bNm2vEiBHq2rWrqlevrl27dmn27NkpQkmpUqUUEBCgKVOmyM/PTz4+Pqpateptb8FOzdq1azVp0iQNHTrUPj3CjBkzVKdOHQ0ZMkRjx45N1/bu5Pjx4/riiy8kSXFxcdqxY4c+/fRTBQYGpnqZbsGCBamOm2rYsGGaUywk69evn8aPH693331XX331VabUv3HjRvts7+fPn9eiRYu0fv16tW/fXqVLl5Z0Y7B9cm9Dz549deXKFU2dOlVBQUH2XgFJmjlzpiZNmqSWLVuqVKlSunz5sqZOnarcuXPbQ3zt2rXVs2dPjRkzRpGRkWrUqJHc3d21f/9+zZ8/Xx999FGGBsC7u7urX79+eu2117R8+fIMj73p3Lmzff+p/aGuVKmSpBszrjdu3Fiurq4Z7uHKbE899ZQGDx6sS5cuGY9XfO2117Ro0SI1b97cPsVKTEyMdu3apQULFujw4cMKDAxUjx49dP78edWrV09FihTRkSNHNHHiRFWsWNHea1+xYkW5urrqvffeU3R0tDw9Pe1zf6VVb4kSJdSiRQuVKlVKMTExWr16tRYvXqwqVaqoRYsWktJ3zlSqVEmTJ0/WO++8o5CQEAUFBalevXqSboyF2rlz5109I/5dy1m38yF7/Pnnn9Zzzz1nFS9e3PLw8LD8/PysGjVqWBMnTrSuX79ubxcfH28NHz7cKlGihOXu7m4VLVrUGjRokEMby0p7duhbb4O/3czBK1eutB588EHLw8PDCg0Ntb744osU0xGsWbPGeuqpp6xChQpZHh4eVqFChawOHTo43Cac2nQElmVZq1evtmrUqGF5e3tbuXPntlq0aGH98ccfDm3SmnF5xowZliTr0KFDab6mluU4HUFa0pqOoH///lbBggUtb29vq0aNGtbmzZtTnUbgu+++s8qWLWu5ubk5HGft2rWtcuXKpbrPm7dz6dIlKzg42HrkkUes+Ph4h3avvPKK5eLiYm3evPm2x5Aet05H4OLiYgUFBVkdOnRwuP3dsm4/HYFuuoX7TjNQd+nSxXJ1dU2x/cyYjsDDw8MqXbq0NWrUKCsuLs6h/aJFi6wKFSpYXl5eVvHixa333nvPmj59usO5s337dqtDhw5WsWLFLE9PTysoKMhq3ry5tXXr1hT7/+yzz6xKlSpZ3t7elp+fn1W+fHnr9ddfT3XW/5vdbgb26Ohoy9/f3+G8UhozhwcHB6f6esXGxlp58uSx/P39rWvXrqVYn5CQYPXp08fKnz+/ZbPZ7P+Gb/d70y3TCaT1bymt8zyt96BbnT592nJzc7NmzZqVZpvUbte/fPmyNWjQICskJMTy8PCwAgMDrerVq1vjxo2znwcLFiywGjVqZAUFBVkeHh5WsWLFrJ49e1onT5502NbUqVOtkiVLWq6urnecmuDLL7+02rdvb5UqVcry9va2vLy8rLJly1qDBw+2Ll26lKK9yTlz6tQp64knnrD8/PwsSQ7HOnnyZCtXrlypbhu3Z7OsdN52AADIERISElSoUCG1aNEixbi8e0H37t31559/auPGjc4u5a7z8MMPq06dOvrwww+dXco9h0t1AIBULVy4UGfPnlXnzp2dXUqGDB06VA888IB+/PFH1ahRw9nl3DWWL1+u/fv3pzqWC3dGjxMAwMEvv/yinTt3auTIkQoMDEzX5KzAfx131QEAHEyePFkvvviigoKCHL7AGwA9TgAAAMbocQIAADBEcAIAADBEcAIAADD0n5yOwPvhl5xdApBhUT984OwSgAzJ5+vh7BKADPMyTET0OAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABgiOAEAABi664JTXFycrly54uwyAAAAUnBqcJoxY4b69Omj2bNnS5IGDRokPz8/+fv7q2HDhjp37pwzywMAAHDgtOA0atQo9e7dW3v37lXfvn314osvKiIiQiNGjNC7776rvXv36q233nJWeQAAACm4OWvHERERCg8PV4cOHbR161ZVrVpV8+bNU+vWrSVJDz74oF544QVnlQcAAJCC03qcjh49qscff1ySVLlyZbm5uenBBx+0r69QoYJOnjzprPIAAABScFpwio+Pl6enp/2xh4eH3N3d7Y/d3NyUmJjojNIAAABS5bRLdZL0xx9/6NSpU5Iky7K0d+9e+x11f//9tzNLAwAASMGpwal+/fqyLMv+uHnz5pIkm80my7Jks9mcVVqOMnNMF7Wo+5C8PG6cDleuxuqDmav17tTlkqSTG8YqwC+Xw3P2HDipR9qMkiRNGtJBXVvVSHXbD7caqb2HTmdh9YCj/i89r6WLvtHVmBhJkn9AgHr1G6AX+/aXJP259w+90KWjDh2MUkJCovz8/NTt+V4aMHiYE6sGUvdU86ZauWK5w7JcPj46d5Fpe5zFacHp0KFDzto1bnHg6FmNm75Cm3cckovNpjd7NtXbLz6hXX8e09L1uyVJew+d0v+9Hm5/zrnoq/b/f/Oj7zTpy3UO21z2WT+5ubkQmpDtgkuUVN9X31DFSo/KspL08YdjNWromwotU071GjZRx1bNdP3adX3wyVQVKVpMkz56X+PHjVHZ8g+p2ZMtnV0+kIKvr6/Wb9psf+xx0zAXZD+nBafg4GBn7Rq3GDF5qcPjNT/vVcy2iXqyXkV7cLoeG6/dUakP1r946aouXvonSD1Q/D7lC/DRtK83ZV3RQBr69h/o8LhGrboKDsyl1cuXql7DJjpz+rS6Pd9bLZ/pIEma8VgNBQfm0sZ1awhOuCu5uLiobLkH79wQ2cJpwWnnzp1G7SpUqJDFleBmbm4ueveVVrLZpO/WRNqXV3igsGK2TVRcfIIi9/6l1v2m6PxNvU43G/3yU5Kktycuzo6SgTTFxcVp3KhhSkpKUsOmN4YCBN13n5YtWahuL7ykQoWLaOzIt5WUZOmp1u2cXC2QustXrih3Lg+5uroquHhxTZv+uSpXqeLssnIspwWnihUr2scypcVms3FnXTZ5st5D+mpcD/vjd6Ys1fcbbvQ2fb9+t/YeOqV9h0+r3qOh6tHmcW2Z96ZKNU59gtIG1crojwMnHXqhgOy0avkSdX/2GSUlJcnV1VWD3h6pug0aS5IWrdyoJxvVUvWKoZJufJp/e+S7eqxGTWeWDKSqdp26qvjww6pSpaqioqI0buwYNahbU3ujDqlAgYLOLi9Hslm3Sy5Z6MiRI0bt7nRJLzY2VrGxsQ7Lgmq+IZuLa4Zry4lyeXno0QrFVSAwt3q2raVHyxfXM698Zg9PN+vbqZ7e699KjZ/7SBu27ndY17VldU16u6O6Do7QV99vza7y/1OifvjA2SXc82JiYrQrcrv+PnNKsyKm6ZefNmnGl9+oboPGerLh4zp4IEr9Bw5RoUJFNfvzcK1fu0rTvpinhk2aO7v0e1o+Xw9nl/Cf99dfR1U6pIS693heEz6Z7Oxy/lO8DLuSnBacMsuwYcM0fPhwh2Wu91WRe8FHnVTRf8Pxde/p9LlLeqT1qBTrAgN89dcP7+rtiYv0v+krHdbtWDhERe7Lo3zVXs2uUv9zCE6Zr1KZYAXdV0DDxryv1s3q6/N536lewyYO6+8rUFDf//CzE6u89xGcskfB/HlU8eFHtGzlGmeX8p9iGpycNgFm586ddfnyZfvjHTt2KD4+Pt3bGTRokKKjox1+3O6rlJml5kg2m+ThlvpZ9HT9ipKk/Ucc75gLyuur+4sFaVkqvVSAM1lJluLj43Up+qIkyfWWHmmbzUVJSff0Z0jkEGdOn9bly5dVoCCX6ZzFaWOcZs+erXHjxsnPz0+SVLNmTUVGRqpkyZLp2o6np6fDDOSSuEyXTj9EvKq5y7dqx55jyp/PT690rq8Av1waN2O1alW+XwN7NNHnizbryPHzalSjrPp3bajoy1e1cM0Oh+2M7Pe0JGnQh9864SiAG9o91VhPtnxGZcqV17lzZzVt8kSdOXNavV4eoMdr15O3t7f6vdBVQ0f/T4WKFNXMqZN16uQJdXu+l7NLB1JoULeW2nXoqAoVHtK+fXs1bMhg2Ww2vTn4bWeXlmM5LTjdeoXwHr9ieE/LG+Cj//VvLVdXF1mWpYuXr+mtCYv0QcQqVS4XrEfKFlXtKg/IZpPiExIVuecvtes/NcV2WtavqCMnzuuvUxeccBTADRfOn9PbA19VXFycXF1dlS8wUMNG/089XuwrSZo59zu93vcFvdLrOSUmJsrX10c9XuijXi+/5uTKgZTOnDmt/i/3VUJCgtzd3RVcvLhmzZ6r+x94wNml5VhOG+Pk4uKiU6dOKSgoSJLk5+enHTt2pLvHKTXeD7/0r7cBOAtjnHCvYowT7mWmY5zu2u+qS8Y8TgAA4G5xV35XXTLmcQIAAHeTu/q76m6+6w4AAMDZ7rrvqrt8+bK+/PJLhYeHa+vWrfQ4AQCAu4bT5nG61YYNGxQWFqaCBQtq3Lhxqlu3rn7+mcnoAADA3cOpY5xOnTqliIgIhYeH69KlS2rbtq1iY2O1cOFClS1b1pmlAQAApOC0HqcWLVooNDRUO3fu1Pjx43XixAlNnDjRWeUAAADckdN6nJYtW6a+ffvqxRdf1P333++sMgAAAIw5rcdp06ZNunz5sipVqqSqVavq448/1t9//+2scgAAAO7IacHpscce09SpU3Xy5En17NlTX331lQoVKqSkpCStWrWKqQgAAMBdx2lfuZKaffv2KTw8XLNmzdLFixfVsGFDLVq0KN3b4StXcC/jK1dwr+IrV3AvM/3KlbtmOgJJCg0N1dixY3Xs2DF9+eWXzi4HAADAwV3V45RZ6HHCvYweJ9yr6HHCveye7HECAAC4mxGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADBGcAAAADNksy7KcXURm23fqqrNLADKscr/5zi4ByJD9n3V0dglAhhXwdzdqR48TAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAIYITAACAoQwFp40bN6pTp06qVq2ajh8/LkmaNWuWNm3alKnFAQAA3E3SHZy+/vprNW7cWN7e3vrtt98UGxsrSYqOjtbo0aMzvUAAAIC7RbqD0zvvvKMpU6Zo6tSpcnd3ty+vUaOGtm/fnmmFJSYm6sSJE5m2PQAAgH8r3cFp3759qlWrVorl/v7+unjxYmbUJEnavXu3ihYtmmnbAwAA+LfSHZwKFCigqKioFMs3bdqkkiVLZkpRAAAAd6N0B6fnnntO/fr10y+//CKbzaYTJ05o9uzZGjBggF588cWsqBEAAOCu4JbeJwwcOFBJSUmqX7++rl69qlq1asnT01MDBgxQnz59sqJGAACAu0K6g5PNZtPgwYP12muvKSoqSleuXFHZsmXl6+ubru3s3Lnztuv37duX3tIAAACyVLqDUzIPDw+VLVs2wzuuWLGibDabLMtKsS55uc1my/D28e/s3b1TA3p31aED+5WUlCQfHx+NGPexmj31jCRp4v9GasGcmfr77BklJiZqYviXatjsSSdXjZxoet9aalGlqDzdXSVJMdcT9OGi3Rr7zY0PZ7lzuWnl8GYqUzRANkknL1zV06NWac+xaPs2Ls8NS7Hdycv+0OsRW7LlGIBkr/R+TksWfaOrMTGSJP+AAPXuN0C9+w1waJeUlKQq5UvpxPHjGjBwiPoPHOKMcnOkdAenunXr3jbQrF271mg7hw4dSu+ukU1OHDuqds3rqHjJEL03YaoKFwvWzu1bFFSgkL1NzOXLKvNgBRUoVERffR7uxGqR0x08eUnvL9ylX/48I5vNRQNbV9BbbStq95EL+n7bX1o94gndXyi3Bs/aqjPR1/RRj2pa804zFerypcN2Zq75U5OW7bE/PnbuSnYfCqDiJUvq5f4D9XClR5WUlKSJH76nd4a+qdJlyql+o6b2dr2f6yyJzgVnSHdwqlixosPj+Ph4RUZGavfu3QoLS/mpLS3BwcHp3TWyyZsv95Svr5++W/urfdnDlR9zaDNwxFhJUuS2XwlOcKp35kc6PF6784Siv+qsFo8W1Y5D51S6iL8mLPlDHy/9Q5J08Uqsvh7UUJ3rhujzH/65Q/jvS9f1x18Xs7FyIKV+/Qc5PH68dl0VyeetVcuX2oPT90sW6vvFC7Vy/a+qU62iE6rM2dIdnD788MNUlw8bNkxXrph/QuvcubM++eQT+fn5SZJ27NihsmXLOkyqCefYGblNZcqVV+PqFXT8ryPyzpVLzZ5qo+FjJzq7NOC23FxcNLpzJdkkfffLUbWsVlw2m02fLvvD3mZl5AklJCapaaWiDsHp5ace1CtPlde1uAR9s/mwek35yQlHAPwjLi5OY0cNVVJSkho3ayFJunD+nPr27KqXBwxSaJmMD5dBxmXal/x26tRJ06dPN24/e/ZsXbt2zf64Zs2a+uuvvzKrHPwL165e1fYtv6hwkaL6YPJMNW3RWvO+mKFRb/V3dmlAqlpUKapLX3XW+Tmd9EKTMho9P1LLtx9T8SBfWZalv85ddWgfG5+o+wK87Y/X7Tqh/tN/UbeJG/TjntPqVCdEs16pnd2HAUiSVi5fosJ5vRQc5KspH4/X4KHvqG6DxpKk9i2bqUTJUnrl9cFOrjLnyvDg8Ftt3rxZXl5exu1vHRSe2iBxE7Gxsfbvy0sWF5soD0/PDG0PN34XefLm1fR5SyVJjZo/rT/3/q4l38zT4Hfed3J1QEprdpxUi5ErdV8ebz3fuLQGtamoyEPnjZ/f4p1V9v//+qfDWj60sZpXKZYVpQJ3VKNmXS1YvFJnz5zWrBlT9e47Q1X2wQra8ds27f9zr37Zud/ZJeZo6Q5OrVq1cnhsWZZOnjyprVu3asiQ7B/VP2bMGA0fPtxhWe/+b6rPANJ4Rnl6eqpAocIOy0rdH6rfd0U6pyDgDq7GJWj976ckSfM2HdKR8PYa+WwlzVy7XzabTUXz5XLodfJ0d9Xpi9fS2pzW/35SNcoWkK+Xm65cT8jy+oGb+fj4qFqNG19t9mTLZ1SxdDG9O/JteXp56tq1a6pwfxGH9uPeHamZ4Z9q5/5jzig3x0l3cPL393d47OLiotDQUI0YMUKNGjVK17b++OMPnTp1483Osizt3bs3xTipChUq3HYbgwYN0quvvuqw7MiFxHTVAUdFigbrzMmTDssOH9yf7rm6AGexSXJ3c9G3mw9r9P9V1vNNymjI7G2SpAYPFZKbq4uWbUt7aMBjofcpybIITbgrWEmW4uPjNGHKDP119LDDuv9r97Rat+ugLt1ecE5xOVC6glNiYqK6du2q8uXLK0+ePP965/Xq1XN43Lx5c0mO8zglJt4+BHl6esrzlstyHlevptEaJnq9+qb69+qi3l3bqcvzL2ndymXavuUXden5z8zwx44e1p7dO3Xk0I0u412/3ZjvpkSpEIWEMmAR2Wf1yKaa/+Mh7Th0Tvlze6nfkw8qwMdDH363S8fPX9XeY9F66YmyOnXxms5GX9P4HtV0+Vq8fWD4W89UVJH8Plq85aiuXI1XWP0HVLd8Qf2457STjww50TNPNtJTLZ9RmQcr6Ny5s5o6aYLOnDmt3i+/ptAyZVMdEF6iRIgqV63mhGpzpnQFJ1dXVzVq1Eh79uz518Fpx44dyp0797/aBrLGEy2f0fG/DuuzieO0dsVSeXvnUttOXfXG0DH2NhGfTtQX06fYH3/28QeSpFr1Gumz2d9me83IufL6eurdzlXk6mKTZUkXr8Zp6Jxt+nDR75KkBm8v1crhzTT6/yrLJunUhWt6evQ/Y5piExLV6rHi6lirlCTpenyi5m46qJ6TNjnjcJDDXTh/Tm8NfFVxcXFydXVVvsBADR89Ts/36uvs0vD/2ax0jsquXLmy3nvvPdWvX/9f7djFxUWPPvqounfvrvbt29unJcgM+07R44R7V+V+851dApAh+z/r6OwSgAwr4G82HVK6pyN45513NGDAAC1ZskQnT57UpUuXHH5MrV+/XmXLllX//v1VsGBBhYWFaePGjektBwAAINsY9ziNGDFC/fv3d+gZuvmrV0zHJN0qJiZG8+bNU0REhDZu3KiQkBB1795dYWFhKlCgQLq2lYweJ9zL6HHCvYoeJ9zLTHucjIOTq6urTp48qT179ty2Xe3aGZ80LioqSjNmzNCsWbN06tQpNWnSRIsWLUr3dghOuJcRnHCvIjjhXmYanIwHhyfnq38TjO4kJCREb775poKDgzVo0CAtXbo0y/YFAACQXum6q+7mS3OZbcOGDZo+fbq+/vprubi4qG3bturevXuW7Q8AACC90hWcHnjggTuGp/Pnzb/m4MSJE4qIiFBERISioqJUvXp1TZgwQW3btpWPj096SgMAAMhy6QpOw4cPTzFzeEY1bdpUq1evVmBgoDp37qxu3bopNDQ0U7YNAACQFdIVnNq3b6+goKBM2bG7u7sWLFig5s2by9XVNVO2CQAAkJWMg1Nmj2/KyN1yAAAAzmQ8AWY6JxgHAAD4zzHucUpKSsrKOgAAAO566f7KFQAAgJyK4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGDIzdkFZIWov684uwQgwx59/AFnlwBkyIWrcc4uAciwAv7uRu3ocQIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADBEcAIAADDktODk6uqqM2fOOGv3AAAA6ebmrB1bluWsXeMWKxbM0ryp43X+7GnFx8erU+/X1P6FAfb1cyaN1cpvZuvC32eVmJiot8bP0GP1mzls45V2DXX04J+Ki42Vq6urAgsU0ktvj1PFarWz+3CQw/j//q32/7xGhw/8KS8vb5WpWFl5Gz6ny94FJEmeCTHSr3O0b+smnTj2l/IFBuqhmo2U9OizinP1TrE9r4Qr+m1cV508cVxNPlitOLdc2X1IyMEuXzinpCtnVPqB++Xl5amTJ0/pr3Mxyl+4uCTpVNQOlQouqvz5AxUfH6/DR47q4nUpf5ESzi08B+FSHXT18iUVKV5KbZ/rl+r6a1cuq1TpB9WkzbNpbiOk7EN6/o13NO6LJXp97GTJsjTipf9TfFxcVpUNSJJO7tmuMg3bqsFbEarZ/2MlJiZo68f95JYYK0nyir2omPNnVan9K2o04itVe2649vy6QdeWj091e5e//1DFHyiTjUcA3HAt5rKK5vFQkmVpx/6/FBl1QodOnZe7p5e9TaJcdeD434qMOqmdB07qcsxVPVy6uGIuRzux8pzFaT1OkjRt2jT5+vretk3fvn2zqZqcq2XX3mrZtbckafakcSnWd399pCRp386tWjr381S30Xuo4/Pc3Dw0om+Y9u/errKPPJbJFQP/yNVmpM7d9Dig+QD91L+xql0+rPMBoYr2KSy35m/+08YrSJXbvqTv3n9djzd9TXL5520w74HVOnTlkko07SatW52dhwHo0qlD8soXoDzFytqX+ee7z6FN4ZAHHR5fv5pHfn6+urx7v3z8/LOlzpzOqcFpypQpcnV1TXO9zWYjON2Doi+c0zcRn8jT00slSpd3djnIYdwTr0mSEt1v86EsNkZ+uXM7hCb/qye04atJqjpgmnSV8ZfIfiWKFtKRYyflfXWfSpUorvMXLurw8dMqfH+FVNsnxMfr/PEoXQ7yVu5bAhayjlOD09atWxUUFOTMEpCJJrzdT6u/m6ekpCR5e+fSyM/myjuXj7PLQk6SlKSzyybp4SqPKdqncKpNPOOv6Kf5U1SjeXtd+f/LXBPjtWfWMNUO669znnnlSXCCExQoUEAFChTQ5i3b9PuhU4qNidbjjz6sTVsiVTjknw+hJw7uUbWHy8rLK7cKeN+v3/YdtY+BQtZzanDKDLGxsYqNjXVYFh8XK3cPTydVlHN17D1QdVu01YkjBzV/2kca2aezpi3fQvcxso3Hj5/pzwN7VbHPZF1NZb17wjUd+2KQipV8QDEPt/vneVu/UMHgEJ0rUiP7igVu4eLioqgDB3VfqYfsy7Zsj1SRoDy6+XaqwELFtSPqhOKuX5N70jWFBBfV8UsX5Zs7INtrzomcNjjcZrPJZrP96+2MGTNG/v7+Dj/zpk3IhAqRXoH3FVT5KjXUuM3/aeI363Xl8iXNn/qRs8tCDuH541Tt/GmNHunzsa565kmx3j3xmk7NHiTvXL7yaTlElu2fYQIHIn/W+uWLtP6Vmlr/Sk19M7yHJGnVa43lt2Nuth0DcrZz587p/EXHQd6xCZby5c3rsMzDy1sB+QsqqGhJ5Qkup6TERF0+fSQ7S83RnDodQZMmTfTCCy+offv28vPzy9B2Bg0apFdffdVh2doo7i5wOitJkhQXe93JheC/zrIsef00Tb9tWKGqL0/SZc/AFG3cE67p1OyBcnP3UMAzw5Xg4uGw/oGwkSqTGG9/7Pp3lBa8P0hPD4/QNe/8WX4MgCQd+eu48vg7/i30cLXp73PnJf+0p8Ww2VyUCf0QMOS04LRhwwZNnz5d/fv31yuvvKLWrVurR48eqlmzZrq24+npKU9Px8ty7h78sU6Pi+fOam/kFvvj44ei9POa75XvvkK6/8GKOn38qA7t3a3jRw5Ikv7cvV2SVCi4pIqFlNae337VtzMnqVbTVrqvcDEdPbBXX04eJxcXFzXr0M0px4Scw/PHz/TTioVq+Op4XXPzlHf8jQ9OcS7eSnT1kHvCNZ384g3FXr+m0HaDlZh4zT6A/Lqbryybq654OY61zBt3WZIUk6sg8zgh28TavPRISIg2bt4i74AgxUSfU43KD+mnrTtVyL+I4q5f04Xjf8rm4SfPXL6Ku35VbokxKlnxIR06c9nZ5ecYNsvJM1HGxMRo3rx5ioiI0MaNGxUSEqLu3bsrLCxMBQoUyNA2l+5mYGd6LJsXoU9GvpFieckHymjC1+v02Zg3tWhOeIr1VWrW19BJc3T4zz804qVOOn/2tBISEuTh4aGCRYP1/MDReuixWtlxCP8pH/xwwNkl3FPW9q2e6vJnBryrc8VqKe+FvVowNPUA3+J/ixXjmS/F8uTnMAFm+nzcJvW7v2DuxME9KlkoUIUKFdSp06e1//Bx+1118XGxijt3RMFFi8jf31/R0Zd09NgxxSS6Mzg8E5QpaHYzk9OD082ioqI0Y8YMzZo1S6dOnVKTJk20aNGidG+H4IR7GcEJ9yqCE+5lpsHprpo5PCQkRG+++abeeust+fn5aenSpc4uCQAAwO6umY4geczT119/LRcXF7Vt21bdu3d3dlkAAAB2Tg1OJ06cUEREhCIiIhQVFaXq1atrwoQJatu2rXx8mDgRAADcXZwWnJo2barVq1crMDBQnTt3Vrdu3RQaGuqscgAAAO7IacHJ3d1dCxYsUPPmzW/7fXUAAAB3C6cFp4zcLQcAAOBMd9VddQAAAHczghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhghMAAIAhm2VZlrOLwL0jNjZWY8aM0aBBg+Tp6enscoB04fzFvYpz9+5BcEK6XLp0Sf7+/oqOjlbu3LmdXQ6QLpy/uFdx7t49uFQHAABgiOAEAABgiOAEAABgiOCEdPH09NTQoUMZnIh7Eucv7lWcu3cPBocDAAAYoscJAADAEMEJAADAEMEJAADAEMEJ2rx5s1xdXfXEE084LD98+LBsNluKn06dOjmsj4yMTLW9h4eHQkJC9M4774ihdMgKLVq0UJMmTVJdt3HjRtlsNu3cuTNd5zHgDCbn8p1+kD3cnF0AnC88PFx9+vRReHi4Tpw4oUKFCjmsX716tcqVK2d/7O3tfdvtJbePjY3Vpk2b1KNHDxUsWFDdu3fPkvqRc3Xv3l2tW7fWsWPHVKRIEYd1M2bMUOXKle2zLKf3PAay053O5YoVK2rZsmX2ZVWqVNHzzz+v5557LrtLzfHoccrhrly5orlz5+rFF1/UE088oYiIiBRt8uXLpwIFCth//P39b7vN5PbBwcF69tlnVaNGDW3fvj2LjgA5WfPmzZU/f/4U5+2VK1c0f/58h7Ce3vMYyE53Opd79uzpcP66urrKz8/PYRmyB8Eph5s3b55Kly6t0NBQderUSdOnT8/Uy2pbt27Vtm3bVLVq1UzbJpDMzc1NnTt3VkREhMN5O3/+fCUmJqpDhw5OrA4wx7l87yA45XDh4eH2sR5NmjRRdHS01q9f79CmevXq8vX1tf/89ttvt91mcnsPDw9VqVJFbdu2VefOnbPsGJCzdevWTQcOHHA4b2fMmKHWrVs79Cql9zwGspvpuQznYoxTDrZv3z79+uuv+vbbbyXd+MTTrl07hYeHq06dOvZ2c+fOVZkyZeyPixYtetvtJrePj4/X7t271adPH+XJk0fvvvtulhwHcrbSpUurevXqmj59uurUqaOoqCht3LhRI0aMcGiX3vMYyG6m5zKci+CUg4WHhyshIcFhMLhlWfL09NTHH39sX1a0aFGFhIQYb/fm9mXKlNGBAwc0ZMgQDRs2TF5eXpl3AMD/1717d/Xp00effPKJZsyYoVKlSql27doObdJ7HgPOYHIuw7m4VJdDJSQk6PPPP9f777+vyMhI+8+OHTtUqFAhffnll5m2L1dXVyUkJCguLi7TtgncrG3btnJxcdGcOXP0+eefq1u3btyejXsS5/Ldjx6nHGrJkiW6cOGCunfvnuLaeevWrRUeHp7mnCJ3cu7cOZ06dUoJCQnatWuXPvroI9WtW9d+WziQ2Xx9fdWuXTsNGjRIly5dUpcuXdK9jX379qVYVq5cObm7u2dChYCZzDiXkbUITjlUeHi4GjRokOqAw9atW2vs2LG6dOlShrbdoEEDSTd6mgoWLKhmzZpp1KhR/6pe4E66d++u8PBwNWvWLMVcZCbat2+fYtlff/2VYk4dIKv923MZWctmMaUzAACAEcY4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AQAAGCI4AfhP6tKli55++mn74zp16ujll1/O9jrWrVsnm82mixcvZvu+AWQ+ghOAbNWlSxfZbDbZbDZ5eHgoJCREI0aMUEJCQpbu95tvvtHIkSON2hJ2AKSF76oDkO2aNGmiGTNmKDY2Vt9//7169+4td3d3DRo0yKFdXFycPDw8MmWfefPmzZTtAMjZ6HECkO08PT1VoEABBQcH68UXX1SDBg20aNEi++W1UaNGqVChQgoNDZV048t227Ztq4CAAOXNm1dPPfWUDh8+bN9eYmKiXn31VQUEBChfvnx6/fXXdevXcN56qS42NlZvvPGGihYtKk9PT4WEhCg8PFyHDx9W3bp1JUl58uSRzWazf0N9UlKSxowZoxIlSsjb21sPPfSQFixY4LCf77//Xg888IC8vb1Vt25dhzoB3PsITgCcztvbW3FxcZKkNWvWaN++fVq1apWWLFmi+Ph4NW7cWH5+ftq4caN+/PFH+fr6qkmTJvbnvP/++4qIiND06dO1adMmnT9/Xt9+++1t99m5c2d9+eWXmjBhgvbs2aNPP/1Uvr6+Klq0qL7++mtJ0r59+3Ty5El99NFHkqQxY8bo888/15QpU/T777/rlVdeUadOnbR+/XpJNwJeq1at1KJFC0VGRqpHjx4aOHBgVr1sAJyAS3UAnMayLK1Zs0YrVqxQnz59dPbsWfn4+GjatGn2S3RffPGFkpKSNG3aNNlsNknSjBkzFBAQoHXr1qlRo0YaP368Bg0apFatWkmSpkyZohUrVqS53z///FPz5s3TqlWr1KBBA0lSyZIl7euTL+sFBQUpICBA0o0eqtGjR2v16tWqVq2a/TmbNm3Sp59+qtq1a2vy5MkqVaqU3n//fUlSaGiodu3apffeey8TXzUAzkRwApDtlixZIl9fX8XHxyspKUkdO3bUsGHD1Lt3b5UvX95hXNOOHTsUFRUlPz8/h21cv35dBw4cUHR0tE6ePKmqVava17m5ualy5copLtcli4yMlKurq2rXrm1cc1RUlK5evaqGDRs6LI+Li9PDDz8sSdqzZ49DHZLsIQvAfwPBCUC2q1u3riZPniwPDw8VKlRIbm7/vBX5+Pg4tL1y5YoqVaqk2bNnp9hO/vz5M7R/b2/vdD/nypUrkqSlS5eqcOHCDus8PT0zVAeAew/BCUC28/HxUUhIiFHbRx55RHPnzlVQUJBy586dapuCBQvql19+Ua1atSRJCQkJ2rZtmx555JFU25cvX15JSUlav369/VLdzZJ7vBITE+3LypYtK09PTx09ejTNnqoyZcpo0aJFDst+/vnnOx8kgHsGg8MB3NWeffZZBQYG6qmnntLGjRt16NAhrVu3Tn379tWxY8ckSf369dO7776rhQsXau/everVq9dt52AqXry4wsLC1K1bNy1cuNC+zXnz5kmSgoODZbPZtGTJEp09e1ZXrlyRn5+fBgwYoFdeeUUzZ87UgQMHtH37dk2cOFEzZ86UJL3wwgvav3+/XnvtNe3bt09z5sxRREREVr9EALIRwQnAXS1XrlzasGGDihUrplatWqlMmTLq3r27rl+/bu+B6t+/v/7v//5PYWFhqlatmvz8/NSyZcvbbnfy5Mlq06aNevXqpdKlS+u5555TTEyMJKlw4cIaPny4Bg4cqPvuu08vvfSSJGnkyJEaMmSIxowZozJlyqhJkyZaunSpSpQoIUkqVqyYvv76ay1cuFAPPfSQpkyZotGjR2fhqwMgu9mstEZPAgAAwAE9TgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIYITgAAAIb+H3ewjMxOnUPxAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrik Per Kelas ===\n",
      "      Accuracy  Recall  Specificity  F1-Score\n",
      "AFIB    0.8192  0.8925       0.7825    0.7669\n",
      "VFL     0.7025  0.7625       0.6725    0.6308\n",
      "VT      0.6867  0.1575       0.9512    0.2510\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "MODEL_PATH = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\RYTHM\\HASIL_BERT_RHYTHM_TUNED\\HASIL_BERT_RHYTHM_TUNED\\fold5\"\n",
    "TEST_DIR   = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\DATA_UJI_INFERENCE\\RYTHMtest\"\n",
    "LABEL_MAP  = {'AFIB': 0, 'VFL': 1, 'VT': 2}\n",
    "IDX2LABEL  = {v: k for k, v in LABEL_MAP.items()}\n",
    "MAX_LEN    = 512\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === LOAD TOKENIZER DAN MODEL ===\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === UTILS ===\n",
    "def signal_to_text(sig, target_len=512):\n",
    "    if len(sig) < target_len:\n",
    "        pad = np.full(target_len - len(sig), sig[-1])\n",
    "        sig = np.concatenate([sig, pad])\n",
    "    else:\n",
    "        idx = np.linspace(0, len(sig) - 1, target_len).astype(int)\n",
    "        sig = sig[idx]\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm))\n",
    "\n",
    "def load_test_data(test_dir):\n",
    "    data, labels = [], []\n",
    "    for label_name in os.listdir(test_dir):\n",
    "        folder_path = os.path.join(test_dir, label_name)\n",
    "        if label_name not in LABEL_MAP:\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                sig = np.load(os.path.join(folder_path, file), allow_pickle=True)\n",
    "                if isinstance(sig, np.ndarray) and sig.ndim == 1:\n",
    "                    data.append(signal_to_text(sig))\n",
    "                    labels.append(LABEL_MAP[label_name])\n",
    "    return data, np.array(labels)\n",
    "\n",
    "# === LOAD DATA TEST ===\n",
    "texts, y_true = load_test_data(TEST_DIR)\n",
    "\n",
    "# === INFERENSI ===\n",
    "preds = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "# === EVALUASI GLOBAL ===\n",
    "acc  = accuracy_score(y_true, preds)\n",
    "rec  = recall_score(y_true, preds, average='macro', zero_division=0)\n",
    "f1   = f1_score(y_true, preds, average='macro', zero_division=0)\n",
    "cm   = confusion_matrix(y_true, preds)\n",
    "\n",
    "def specificity_per_class(true, pred, label, num_classes):\n",
    "    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n",
    "    TN = cm.sum() - (cm[label, :].sum() + cm[:, label].sum() - cm[label, label])\n",
    "    FP = cm[:, label].sum() - cm[label, label]\n",
    "    return TN / (TN + FP + 1e-8)\n",
    "\n",
    "spec = np.mean([specificity_per_class(y_true, preds, i, len(LABEL_MAP)) for i in range(len(LABEL_MAP))])\n",
    "\n",
    "# === CETAK METRIK GLOBAL ===\n",
    "print(f\"\\n=== Evaluasi Model BERT Base (Best Model Fold 5) ===\")\n",
    "print(f\"Akurasi     : {acc:.4f}\")\n",
    "print(f\"Recall      : {rec:.4f}\")\n",
    "print(f\"F1-score    : {f1:.4f}\")\n",
    "print(f\"Spesifisitas: {spec:.4f}\")\n",
    "\n",
    "# === CONFUSION MATRIX ===\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "                 xticklabels=LABEL_MAP.keys(),\n",
    "                 yticklabels=LABEL_MAP.keys(), cbar=False)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j + 0.5, i + 0.5, format(cm[i, j], 'd'),\n",
    "                ha='center', va='center',\n",
    "                color='white' if i == j else 'black',\n",
    "                fontsize=10)\n",
    "\n",
    "plt.title(\"Confusion Matrix - BERT Base Rhythm (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_PATH, \"confmat_test.png\"))\n",
    "plt.show()\n",
    "\n",
    "# === METRIK PER KELAS ===\n",
    "cls_names = list(LABEL_MAP.keys())\n",
    "cm = confusion_matrix(y_true, preds, labels=list(range(len(cls_names))))\n",
    "per_class_metrics = defaultdict(dict)\n",
    "\n",
    "for i, cls in enumerate(cls_names):\n",
    "    TP = cm[i, i]\n",
    "    FN = cm[i].sum() - TP\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    TN = cm.sum() - (TP + FN + FP)\n",
    "\n",
    "    acc_cls  = (TP + TN) / cm.sum()\n",
    "    rec_cls  = TP / (TP + FN + 1e-8)\n",
    "    spec_cls = TN / (TN + FP + 1e-8)\n",
    "    f1_cls   = f1_score(y_true, preds, labels=[i], average='macro', zero_division=0)\n",
    "\n",
    "    per_class_metrics[cls][\"Accuracy\"]     = round(acc_cls, 4)\n",
    "    per_class_metrics[cls][\"Recall\"]       = round(rec_cls, 4)\n",
    "    per_class_metrics[cls][\"Specificity\"]  = round(spec_cls, 4)\n",
    "    per_class_metrics[cls][\"F1-Score\"]     = round(f1_cls, 4)\n",
    "\n",
    "# === TABEL PER KELAS ===\n",
    "df_per_class = pd.DataFrame(per_class_metrics).T\n",
    "print(\"\\n=== Metrik Per Kelas ===\")\n",
    "print(df_per_class)\n",
    "\n",
    "# === SIMPAN ===\n",
    "df_per_class.to_csv(os.path.join(MODEL_PATH, \"test_metrics_perclass.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
