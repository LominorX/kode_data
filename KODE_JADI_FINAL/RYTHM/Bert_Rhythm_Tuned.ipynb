{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13b160b7-3492-4812-b27b-36fce1d29922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
      "Requirement already satisfied: transformers==4.48.2 in /usr/local/lib/python3.10/dist-packages (4.48.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.7.1)\n",
      "Requirement already satisfied: accelerate==0.26.0 in /usr/local/lib/python3.10/dist-packages (0.26.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.10.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.3.1)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.34.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2025.7.31)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (0.5.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.8)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2) (1.1.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 torch transformers==4.48.2 scikit-learn accelerate==0.26.0 matplotlib tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2c2f31-7e88-406b-bdc6-48b8d0ef1ef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\nOnly a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/peft.py:36\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch_model\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_balanced_memory, infer_auto_device_map\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.26.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SAFE_MODEL_NAME,\n\u001b[1;32m     29\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[1;32m     30\u001b[0m     SAMPLER_NAME,\n\u001b[1;32m     31\u001b[0m     SCALER_NAME,\n\u001b[1;32m     32\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     33\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m     34\u001b[0m     get_pretty_name,\n\u001b[1;32m     35\u001b[0m     is_tpu_available,\n\u001b[1;32m     36\u001b[0m     is_xpu_available,\n\u001b[1;32m     37\u001b[0m     save,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py:152\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[0;32m--> 152\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlaunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    154\u001b[0m     PrepareForLaunch,\n\u001b[1;32m    155\u001b[0m     _filter_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m     prepare_tpu,\n\u001b[1;32m    161\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/fsdp_utils.py:25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>=\u001b[39m\u001b[38;5;124m\"\u001b[39m, FSDP_PYTORCH_VERSION) \u001b[38;5;129;01mand\u001b[39;00m is_torch_distributed_available():\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist_cp\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_planner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultLoadPlanner, DefaultSavePlanner\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/__init__.py:7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     TensorStorageMetadata,\n\u001b[1;32m      3\u001b[0m     BytesStorageMetadata,\n\u001b[1;32m      4\u001b[0m     ChunkStorageMetadata,\n\u001b[1;32m      5\u001b[0m     Metadata,\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_dict_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_state_dict, load\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_dict_saver\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_state_dict, save\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/state_dict_loader.py:12\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplanner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoadPlanner\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_planner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultLoadPlanner\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _DistWrapper, _all_gather_keys\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/default_planner.py:14\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_shard\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m narrow_tensor_by_index\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTensor\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplanner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     18\u001b[0m     SavePlanner,\n\u001b[1;32m     19\u001b[0m     LoadPlanner,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     WriteItemType,\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/__init__.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrandom\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/ops/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Meta Platforms, Inc. and affiliates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membedding_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrix_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/ops/embedding_ops.py:5\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mop_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpSchema, OutputSharding\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_prop_rule\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/op_schema.py:6\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpOverload\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplacement_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DTensorSpec\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdevice_mesh\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceMesh\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_tensor/placement_types.py:7\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functional_collectives\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mfuncol\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed_c10d\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mc10d\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/distributed/_functional_collectives.py:647\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_running_with_deploy():\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;66;03m# Library MUST be defined at module scope or it doesn't work\u001b[39;00m\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# Creating a \"DEF\" Library always crashes torch::deploy so we create our Library instances here\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m#   guarded against running inside it\u001b[39;00m\n\u001b[0;32m--> 647\u001b[0m     c10_lib \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibrary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc10d_functional\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDEF\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     c10_lib_impl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlibrary\u001b[38;5;241m.\u001b[39mLibrary(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc10d_functional\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIMPL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/library.py:63\u001b[0m, in \u001b[0;36mLibrary.__init__\u001b[0;34m(self, ns, kind, dispatch_key)\u001b[0m\n\u001b[1;32m     62\u001b[0m filename, lineno \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mfilename, frame\u001b[38;5;241m.\u001b[39mlineno\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdispatch_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineno\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mns \u001b[38;5;241m=\u001b[39m ns\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Only a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py:47\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     37\u001b[0m     BaseModelOutputWithPastAndCrossAttentions,\n\u001b[1;32m     38\u001b[0m     BaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m     TokenClassifierOutput,\n\u001b[1;32m     46\u001b[0m )\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:47\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgeneration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CompileConfig, GenerationConfig, GenerationMixin\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflash_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flash_attention_forward\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.peft because of the following error (look up to see its traceback):\nOnly a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1806\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m-> 1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n\u001b[1;32m   1808\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.bert.modeling_bert because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.peft because of the following error (look up to see its traceback):\nOnly a single TORCH_LIBRARY can be used to register the namespace c10d_functional; please put all of your definitions in a single TORCH_LIBRARY block.  If you were trying to specify implementations, consider using TORCH_LIBRARY_IMPL (which can be duplicated).  If you really intended to define operators for a single namespace in a distributed way, you can use TORCH_LIBRARY_FRAGMENT to explicitly indicate this.  Previous registration of TORCH_LIBRARY was registered at /dev/null:241; latest registration was registered at /dev/null:241"
     ]
    }
   ],
   "source": [
    "import os, torch, numpy as np, glob\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === KONFIGURASI ====================================================================\n",
    "DATA_DIR    = \"/workspace/SPLIT_RHYTHM_NPY/train\"\n",
    "MODEL_BASE  = \"/workspace/HASIL_BERT_RHYTHM_Default/HASIL_UjiCoba\"\n",
    "LABEL_MAP   = {'N': 0, 'AFIB': 1, 'VFL': 2}\n",
    "MODEL_NAME  = \"bert-base-uncased\"\n",
    "MAX_LEN     = 512\n",
    "EPOCHS      = 2\n",
    "BATCH_SIZE  = 16\n",
    "SEED        = 42\n",
    "\n",
    "os.makedirs(MODEL_BASE, exist_ok=True)\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "cls_names = list(LABEL_MAP.keys())\n",
    "\n",
    "# === Dataset Custom ==================================================================\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, files, labels):\n",
    "        self.texts = [self.signal_to_text(np.load(f)) for f in files]\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = tokenizer(self.texts[idx], truncation=True, padding='max_length', max_length=MAX_LEN, return_tensors='pt')\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    @staticmethod\n",
    "    def signal_to_text(sig):\n",
    "        norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "        return \" \".join(map(str, norm.tolist()))\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm.sum(axis=1) - np.diag(cm)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    spec = tn / (tn + fp + 1e-8)\n",
    "    return spec.mean()\n",
    "\n",
    "# === Load Data =========================================================================\n",
    "all_files, all_labels = [], []\n",
    "for cls, idx in LABEL_MAP.items():\n",
    "    files = glob.glob(os.path.join(DATA_DIR, cls, \"*.npy\"))\n",
    "    all_files.extend(files)\n",
    "    all_labels.extend([idx] * len(files))\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "final_summary = []\n",
    "\n",
    "# === Training per Fold ==================================================================\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(all_files, all_labels), 1):\n",
    "    print(f\"\\n==== Training Fold {fold} ====\")\n",
    "\n",
    "    train_files = [all_files[i] for i in train_idx]\n",
    "    val_files   = [all_files[i] for i in val_idx]\n",
    "    train_labels= all_labels[train_idx]\n",
    "    val_labels  = all_labels[val_idx]\n",
    "\n",
    "    train_dataset = ECGDataset(train_files, train_labels)\n",
    "    val_dataset   = ECGDataset(val_files, val_labels)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(LABEL_MAP))\n",
    "    output_dir = os.path.join(MODEL_BASE, f\"fold{fold}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        no_cuda=False,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=lambda p: {'accuracy': accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))}\n",
    "    )\n",
    "\n",
    "    # === Logging Manual ==================================================================\n",
    "    history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        print(f\"\\n--- Epoch {epoch}/{EPOCHS} ---\")\n",
    "        trainer.train()\n",
    "\n",
    "        train_pred = trainer.predict(train_dataset)\n",
    "        train_preds = np.argmax(train_pred.predictions, axis=1)\n",
    "        train_loss = train_pred.metrics['test_loss']\n",
    "        train_acc = accuracy_score(train_labels, train_preds)\n",
    "\n",
    "        val_pred = trainer.predict(val_dataset)\n",
    "        val_preds = np.argmax(val_pred.predictions, axis=1)\n",
    "        val_loss = val_pred.metrics['test_loss']\n",
    "        val_acc = accuracy_score(val_labels, val_preds)\n",
    "\n",
    "        val_recall = recall_score(val_labels, val_preds, average='macro')\n",
    "        val_f1 = f1_score(val_labels, val_preds, average='macro')\n",
    "        val_spec = specificity_score(val_labels, val_preds)\n",
    "\n",
    "        history['epoch'].append(epoch)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Val Recall: {val_recall:.4f} | Val F1: {val_f1:.4f} | Val Specificity: {val_spec:.4f}\")\n",
    "\n",
    "    # === Simpan Visual dan Evaluasi Fold =================================================\n",
    "    pd.DataFrame(history).to_csv(os.path.join(output_dir, f'history_fold{fold}.csv'), index=False)\n",
    "\n",
    "    plt.plot(history['epoch'], history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['epoch'], history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    plt.title(f'Loss per Epoch Fold {fold}')\n",
    "    plt.savefig(os.path.join(output_dir, f'loss_curve_fold{fold}.png')); plt.close()\n",
    "\n",
    "    plt.plot(history['epoch'], history['train_acc'], label='Train Acc')\n",
    "    plt.plot(history['epoch'], history['val_acc'], label='Val Acc')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "    plt.title(f'Accuracy per Epoch Fold {fold}')\n",
    "    plt.savefig(os.path.join(output_dir, f'acc_curve_fold{fold}.png')); plt.close()\n",
    "\n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=cls_names, yticklabels=cls_names)\n",
    "    plt.title(f'Confusion Matrix Fold {fold}'); plt.xlabel('Predicted'); plt.ylabel('True')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f'confusion_matrix_fold{fold}.png')); plt.close()\n",
    "\n",
    "    report = classification_report(val_labels, val_preds, target_names=cls_names, output_dict=True)\n",
    "    pd.DataFrame(report).T.to_csv(os.path.join(output_dir, f'classification_report_fold{fold}.csv'))\n",
    "\n",
    "    final_summary.append({\n",
    "        'fold': fold,\n",
    "        'accuracy': val_acc,\n",
    "        'recall': val_recall,\n",
    "        'f1_score': val_f1,\n",
    "        'specificity': val_spec\n",
    "    })\n",
    "\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    print(f\" Fold {fold} selesai. Model disimpan di {output_dir}\")\n",
    "\n",
    "# === Ringkasan Akhir ================================================================\n",
    "pd.DataFrame(final_summary).to_csv(os.path.join(MODEL_BASE, 'final_summary.csv'), index=False)\n",
    "print(f\"\\n Semua fold selesai. Ringkasan akhir disimpan di {MODEL_BASE}/final_summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b823f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Akurasi  Sensitivitas  Spesifisitas  F1-Score\n",
      "0   0.9760        0.9680        0.9835    0.9644\n",
      "  Kelas  Akurasi  Sensitivitas  Spesifisitas  F1-Score\n",
      "0     N   0.9752        0.9640        0.9797    0.9624\n",
      "1  AFIB   0.9785        0.9750        0.9822    0.9621\n",
      "2   VFL   0.9743        0.9650        0.9885    0.9683\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 620x560 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "MODEL_PATH = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\RYTHM\\HASIL_BERT_RHYTHM_TUNED\\HASIL_BERT_RHYTHM_TUNED\\fold5\"\n",
    "TEST_DIR   = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\DATA_UJI_INFERENCE\\RYTHMtest\"\n",
    "LABEL_MAP  = {'N': 0, 'AFIB': 1, 'VFL': 2}\n",
    "IDX2LABEL  = {v: k for k, v in LABEL_MAP.items()}\n",
    "MAX_LEN    = 512\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === LOAD TOKENIZER DAN MODEL ===\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === UTILS ===\n",
    "def signal_to_text(sig, target_len=512):\n",
    "    if len(sig) < target_len:\n",
    "        pad = np.full(target_len - len(sig), sig[-1])\n",
    "        sig = np.concatenate([sig, pad])\n",
    "    else:\n",
    "        idx = np.linspace(0, len(sig) - 1, target_len).astype(int)\n",
    "        sig = sig[idx]\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm))\n",
    "\n",
    "def load_test_data(test_dir):\n",
    "    data, labels = [], []\n",
    "    for label_name in os.listdir(test_dir):\n",
    "        folder_path = os.path.join(test_dir, label_name)\n",
    "        if label_name not in LABEL_MAP:\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                sig = np.load(os.path.join(folder_path, file), allow_pickle=True)\n",
    "                if isinstance(sig, np.ndarray) and sig.ndim == 1:\n",
    "                    data.append(signal_to_text(sig))\n",
    "                    labels.append(LABEL_MAP[label_name])\n",
    "    return data, np.array(labels)\n",
    "\n",
    "# === LOAD DATA TEST ===\n",
    "texts, y_true = load_test_data(TEST_DIR)\n",
    "\n",
    "# === INFERENSI ===\n",
    "preds = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "# === EVALUASI GLOBAL ===\n",
    "acc  = accuracy_score(y_true, preds)\n",
    "rec  = recall_score(y_true, preds, average='macro', zero_division=0)\n",
    "f1   = f1_score(y_true, preds, average='macro', zero_division=0)\n",
    "cm   = confusion_matrix(y_true, preds)\n",
    "\n",
    "def specificity_per_class(true, pred, label, num_classes):\n",
    "    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n",
    "    TN = cm.sum() - (cm[label, :].sum() + cm[:, label].sum() - cm[label, label])\n",
    "    FP = cm[:, label].sum() - cm[label, label]\n",
    "    return TN / (TN + FP + 1e-8)\n",
    "\n",
    "spec = np.mean([specificity_per_class(y_true, preds, i, len(LABEL_MAP)) for i in range(len(LABEL_MAP))])\n",
    "\n",
    "# === CETAK METRIK GLOBAL ===\n",
    "print(f\"\\n=== Evaluasi Model BERT Base (Best Model Fold 5) ===\")\n",
    "print(f\"Akurasi     : {acc:.4f}\")\n",
    "print(f\"Recall      : {rec:.4f}\")\n",
    "print(f\"F1-score    : {f1:.4f}\")\n",
    "print(f\"Spesifisitas: {spec:.4f}\")\n",
    "\n",
    "# === CONFUSION MATRIX ===\n",
    "plt.figure(figsize=(6,6))\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "                 xticklabels=LABEL_MAP.keys(),\n",
    "                 yticklabels=LABEL_MAP.keys(), cbar=False)\n",
    "\n",
    "for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "        ax.text(j + 0.5, i + 0.5, format(cm[i, j], 'd'),\n",
    "                ha='center', va='center',\n",
    "                color='white' if i == j else 'black',\n",
    "                fontsize=10)\n",
    "\n",
    "plt.title(\"Confusion Matrix - BERT Base Rhythm (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_PATH, \"confmat_test.png\"))\n",
    "plt.show()\n",
    "\n",
    "# === METRIK PER KELAS ===\n",
    "cls_names = list(LABEL_MAP.keys())\n",
    "cm = confusion_matrix(y_true, preds, labels=list(range(len(cls_names))))\n",
    "per_class_metrics = defaultdict(dict)\n",
    "\n",
    "for i, cls in enumerate(cls_names):\n",
    "    TP = cm[i, i]\n",
    "    FN = cm[i].sum() - TP\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    TN = cm.sum() - (TP + FN + FP)\n",
    "\n",
    "    acc_cls  = (TP + TN) / cm.sum()\n",
    "    rec_cls  = TP / (TP + FN + 1e-8)\n",
    "    spec_cls = TN / (TN + FP + 1e-8)\n",
    "    f1_cls   = f1_score(y_true, preds, labels=[i], average='macro', zero_division=0)\n",
    "\n",
    "    per_class_metrics[cls][\"Accuracy\"]     = round(acc_cls, 4)\n",
    "    per_class_metrics[cls][\"Recall\"]       = round(rec_cls, 4)\n",
    "    per_class_metrics[cls][\"Specificity\"]  = round(spec_cls, 4)\n",
    "    per_class_metrics[cls][\"F1-Score\"]     = round(f1_cls, 4)\n",
    "\n",
    "# === TABEL PER KELAS ===\n",
    "df_per_class = pd.DataFrame(per_class_metrics).T\n",
    "print(\"\\n=== Metrik Per Kelas ===\")\n",
    "print(df_per_class)\n",
    "\n",
    "# === SIMPAN ===\n",
    "df_per_class.to_csv(os.path.join(MODEL_PATH, \"test_metrics_perclass.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
