{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376dcb0d-4ac4-410d-bc73-e7def8e23c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Collecting transformers==4.48.2\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Collecting accelerate==0.26.0\n",
      "  Downloading accelerate-0.26.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.2)\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.48.2)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.2) (2.31.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.2)\n",
      "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.48.2)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.26.0) (5.9.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.58.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.6/106.6 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.2)\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.2) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m96.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.26.0-py3-none-any.whl (270 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.7/270.7 kB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m110.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.0/325.0 kB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.58.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.15.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pytz, tzdata, tqdm, threadpoolctl, safetensors, regex, numpy, kiwisolver, joblib, hf-xet, fsspec, fonttools, cycler, scipy, pandas, huggingface-hub, contourpy, tokenizers, scikit-learn, matplotlib, accelerate, transformers, seaborn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.1\n",
      "    Uninstalling numpy-1.24.1:\n",
      "      Successfully uninstalled numpy-1.24.1\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.4.0\n",
      "    Uninstalling fsspec-2023.4.0:\n",
      "      Successfully uninstalled fsspec-2023.4.0\n",
      "Successfully installed accelerate-0.26.0 contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 fsspec-2025.5.1 hf-xet-1.1.5 huggingface-hub-0.33.1 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.10.3 numpy-1.26.4 pandas-2.3.0 pytz-2025.2 regex-2024.11.6 safetensors-0.5.3 scikit-learn-1.7.0 scipy-1.15.3 seaborn-0.13.2 threadpoolctl-3.6.0 tokenizers-0.21.2 tqdm-4.67.1 transformers-4.48.2 tzdata-2025.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.26.4 torch transformers==4.48.2 scikit-learn accelerate==0.26.0 matplotlib tqdm pandas seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f819f3-de44-44e1-9e50-c1b8d58d9bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Distribusi Kelas (TRAIN SET):\n",
      "========================================\n",
      "  - AFIB   | Jumlah file:  366\n",
      "  - N      | Jumlah file:  390\n",
      "  - VFL    | Jumlah file:   53\n",
      "  - VT     | Jumlah file:   56\n",
      "========================================\n",
      "🔢 Jumlah Total Semua File: 865 file\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "DATA_DIR = \"/workspace/SEMUA_DATA_FILTERED\"  # Ganti jika lokasimu berbeda\n",
    "LABEL_MAP = {'AFIB': 0, 'N': 1, 'VFL': 2, 'VT': 3}\n",
    "\n",
    "def get_class_distribution(data_dir):\n",
    "    class_counts = {}\n",
    "    total = 0\n",
    "    for class_name in LABEL_MAP:\n",
    "        class_path = os.path.join(data_dir, class_name)\n",
    "        if os.path.exists(class_path):\n",
    "            num_files = len([f for f in os.listdir(class_path) if f.endswith(\".npy\")])\n",
    "            class_counts[class_name] = num_files\n",
    "            total += num_files\n",
    "        else:\n",
    "            class_counts[class_name] = 0\n",
    "    return class_counts, total\n",
    "\n",
    "class_dist, total_files = get_class_distribution(DATA_DIR)\n",
    "\n",
    "print(\"📊 Distribusi Kelas (TRAIN SET):\")\n",
    "print(\"========================================\")\n",
    "for cls, count in class_dist.items():\n",
    "    print(f\"  - {cls:<6} | Jumlah file: {count:4}\")\n",
    "print(\"========================================\")\n",
    "print(f\"🔢 Jumlah Total Semua File: {total_files} file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d711d1-b830-4493-a483-ce6be9971e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Total disimpan: 865\n",
      "[INFO] Total dilewati (pendek): 321\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "SOURCE_DIR = \"/workspace/SEMUA_DATA\"             # Folder data asli\n",
    "TARGET_DIR = \"/workspace/SEMUA_DATA_FILTERED\"    # Folder output hasil filter\n",
    "MIN_LEN = 3600  # 8 detik @360Hz\n",
    "\n",
    "# === BUAT FOLDER BARU UNTUK SIMPAN HASIL FILTER ===\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "total_kept = 0\n",
    "total_skipped = 0\n",
    "\n",
    "# === ITERASI SETIAP KELAS ===\n",
    "for class_name in os.listdir(SOURCE_DIR):\n",
    "    class_path = os.path.join(SOURCE_DIR, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    out_class_path = os.path.join(TARGET_DIR, class_name)\n",
    "    os.makedirs(out_class_path, exist_ok=True)\n",
    "\n",
    "    for file in os.listdir(class_path):\n",
    "        if not file.endswith(\".npy\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(class_path, file)\n",
    "        try:\n",
    "            signal = np.load(file_path)\n",
    "\n",
    "            if signal.ndim != 1 or len(signal) < MIN_LEN:\n",
    "                total_skipped += 1\n",
    "                continue\n",
    "\n",
    "            # Simpan ke folder baru\n",
    "            shutil.copy(file_path, os.path.join(out_class_path, file))\n",
    "            total_kept += 1\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Gagal memuat {file}: {e}\")\n",
    "            total_skipped += 1\n",
    "\n",
    "print(f\"[INFO] Total disimpan: {total_kept}\")\n",
    "print(f\"[INFO] Total dilewati (pendek): {total_skipped}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "955d8162-465f-4a87-bfc6-d86d2559af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4800' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4800/4800 30:49, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.728600</td>\n",
       "      <td>0.470535</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>0.847917</td>\n",
       "      <td>0.847526</td>\n",
       "      <td>0.923958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399200</td>\n",
       "      <td>0.375539</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.890625</td>\n",
       "      <td>0.890741</td>\n",
       "      <td>0.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.362528</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.870833</td>\n",
       "      <td>0.871571</td>\n",
       "      <td>0.935417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.380744</td>\n",
       "      <td>0.903125</td>\n",
       "      <td>0.903125</td>\n",
       "      <td>0.903381</td>\n",
       "      <td>0.951562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.162900</td>\n",
       "      <td>0.383562</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.912500</td>\n",
       "      <td>0.912029</td>\n",
       "      <td>0.956250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.304787</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933324</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>0.341182</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.938537</td>\n",
       "      <td>0.969271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.295640</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>0.945833</td>\n",
       "      <td>0.945774</td>\n",
       "      <td>0.972917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.349119</td>\n",
       "      <td>0.943750</td>\n",
       "      <td>0.943750</td>\n",
       "      <td>0.943773</td>\n",
       "      <td>0.971875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.059600</td>\n",
       "      <td>0.330765</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944789</td>\n",
       "      <td>0.972396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.402088</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941651</td>\n",
       "      <td>0.970833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.039400</td>\n",
       "      <td>0.452093</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933338</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.380220</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944830</td>\n",
       "      <td>0.972396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.024600</td>\n",
       "      <td>0.398145</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.942655</td>\n",
       "      <td>0.971354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015300</td>\n",
       "      <td>0.338541</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.953115</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.401735</td>\n",
       "      <td>0.940625</td>\n",
       "      <td>0.940625</td>\n",
       "      <td>0.940576</td>\n",
       "      <td>0.970312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.415479</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.942684</td>\n",
       "      <td>0.971354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.381514</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946876</td>\n",
       "      <td>0.973437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.368389</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952098</td>\n",
       "      <td>0.976042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.361018</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952083</td>\n",
       "      <td>0.952095</td>\n",
       "      <td>0.976042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Loss] Epoch 1.00 | Step 240 | Loss: 0.7286\n",
      "[Metrics] Accuracy: 0.8479 | Recall: 0.8479 | F1: 0.8475 | Specificity: 0.9240\n",
      "[Training Loss] Epoch 2.00 | Step 480 | Loss: 0.3992\n",
      "[Metrics] Accuracy: 0.8906 | Recall: 0.8906 | F1: 0.8907 | Specificity: 0.9453\n",
      "[Training Loss] Epoch 3.00 | Step 720 | Loss: 0.2859\n",
      "[Metrics] Accuracy: 0.8708 | Recall: 0.8708 | F1: 0.8716 | Specificity: 0.9354\n",
      "[Training Loss] Epoch 4.00 | Step 960 | Loss: 0.2134\n",
      "[Metrics] Accuracy: 0.9031 | Recall: 0.9031 | F1: 0.9034 | Specificity: 0.9516\n",
      "[Training Loss] Epoch 5.00 | Step 1200 | Loss: 0.1629\n",
      "[Metrics] Accuracy: 0.9125 | Recall: 0.9125 | F1: 0.9120 | Specificity: 0.9562\n",
      "[Training Loss] Epoch 6.00 | Step 1440 | Loss: 0.1371\n",
      "[Metrics] Accuracy: 0.9333 | Recall: 0.9333 | F1: 0.9333 | Specificity: 0.9667\n",
      "[Training Loss] Epoch 7.00 | Step 1680 | Loss: 0.1051\n",
      "[Metrics] Accuracy: 0.9385 | Recall: 0.9385 | F1: 0.9385 | Specificity: 0.9693\n",
      "[Training Loss] Epoch 8.00 | Step 1920 | Loss: 0.0884\n",
      "[Metrics] Accuracy: 0.9458 | Recall: 0.9458 | F1: 0.9458 | Specificity: 0.9729\n",
      "[Training Loss] Epoch 9.00 | Step 2160 | Loss: 0.0697\n",
      "[Metrics] Accuracy: 0.9437 | Recall: 0.9437 | F1: 0.9438 | Specificity: 0.9719\n",
      "[Training Loss] Epoch 10.00 | Step 2400 | Loss: 0.0596\n",
      "[Metrics] Accuracy: 0.9448 | Recall: 0.9448 | F1: 0.9448 | Specificity: 0.9724\n",
      "[Training Loss] Epoch 11.00 | Step 2640 | Loss: 0.0466\n",
      "[Metrics] Accuracy: 0.9417 | Recall: 0.9417 | F1: 0.9417 | Specificity: 0.9708\n",
      "[Training Loss] Epoch 12.00 | Step 2880 | Loss: 0.0394\n",
      "[Metrics] Accuracy: 0.9333 | Recall: 0.9333 | F1: 0.9333 | Specificity: 0.9667\n",
      "[Training Loss] Epoch 13.00 | Step 3120 | Loss: 0.0294\n",
      "[Metrics] Accuracy: 0.9448 | Recall: 0.9448 | F1: 0.9448 | Specificity: 0.9724\n",
      "[Training Loss] Epoch 14.00 | Step 3360 | Loss: 0.0246\n",
      "[Metrics] Accuracy: 0.9427 | Recall: 0.9427 | F1: 0.9427 | Specificity: 0.9714\n",
      "[Training Loss] Epoch 15.00 | Step 3600 | Loss: 0.0153\n",
      "[Metrics] Accuracy: 0.9531 | Recall: 0.9531 | F1: 0.9531 | Specificity: 0.9766\n",
      "[Training Loss] Epoch 16.00 | Step 3840 | Loss: 0.0067\n",
      "[Metrics] Accuracy: 0.9406 | Recall: 0.9406 | F1: 0.9406 | Specificity: 0.9703\n",
      "[Training Loss] Epoch 17.00 | Step 4080 | Loss: 0.0075\n",
      "[Metrics] Accuracy: 0.9427 | Recall: 0.9427 | F1: 0.9427 | Specificity: 0.9714\n",
      "[Training Loss] Epoch 18.00 | Step 4320 | Loss: 0.0067\n",
      "[Metrics] Accuracy: 0.9469 | Recall: 0.9469 | F1: 0.9469 | Specificity: 0.9734\n",
      "[Training Loss] Epoch 19.00 | Step 4560 | Loss: 0.0051\n",
      "[Metrics] Accuracy: 0.9521 | Recall: 0.9521 | F1: 0.9521 | Specificity: 0.9760\n",
      "[Training Loss] Epoch 20.00 | Step 4800 | Loss: 0.0061\n",
      "[Metrics] Accuracy: 0.9521 | Recall: 0.9521 | F1: 0.9521 | Specificity: 0.9760\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] Accuracy: 0.9521 | Recall: 0.9521 | F1: 0.9521 | Specificity: 0.9760\n",
      "[Metrics] Accuracy: 0.9521 | Recall: 0.9521 | F1: 0.9521 | Specificity: 0.9760\n",
      "\n",
      "[INFO] Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4800' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4800/4800 30:53, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.790600</td>\n",
       "      <td>0.489757</td>\n",
       "      <td>0.832292</td>\n",
       "      <td>0.832292</td>\n",
       "      <td>0.832999</td>\n",
       "      <td>0.916146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.394800</td>\n",
       "      <td>0.352817</td>\n",
       "      <td>0.902083</td>\n",
       "      <td>0.902083</td>\n",
       "      <td>0.901766</td>\n",
       "      <td>0.951042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.285900</td>\n",
       "      <td>0.299756</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.932292</td>\n",
       "      <td>0.932167</td>\n",
       "      <td>0.966146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.229200</td>\n",
       "      <td>0.263116</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933103</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.161000</td>\n",
       "      <td>0.250790</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.949858</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.262941</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946779</td>\n",
       "      <td>0.973437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.236477</td>\n",
       "      <td>0.955208</td>\n",
       "      <td>0.955208</td>\n",
       "      <td>0.955053</td>\n",
       "      <td>0.977604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.091800</td>\n",
       "      <td>0.206127</td>\n",
       "      <td>0.959375</td>\n",
       "      <td>0.959375</td>\n",
       "      <td>0.959349</td>\n",
       "      <td>0.979687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.267905</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958316</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.065900</td>\n",
       "      <td>0.251837</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962444</td>\n",
       "      <td>0.981250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.326748</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.952997</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.049700</td>\n",
       "      <td>0.349564</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.950762</td>\n",
       "      <td>0.975521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.386637</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946534</td>\n",
       "      <td>0.973437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.033400</td>\n",
       "      <td>0.348560</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.950692</td>\n",
       "      <td>0.975521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.419998</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944378</td>\n",
       "      <td>0.972396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.461433</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941147</td>\n",
       "      <td>0.970833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.356320</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.953938</td>\n",
       "      <td>0.977083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.383242</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.950777</td>\n",
       "      <td>0.975521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.008800</td>\n",
       "      <td>0.393210</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.949658</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.391838</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.951042</td>\n",
       "      <td>0.950729</td>\n",
       "      <td>0.975521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Loss] Epoch 1.00 | Step 240 | Loss: 0.7906\n",
      "[Metrics] Accuracy: 0.8323 | Recall: 0.8323 | F1: 0.8330 | Specificity: 0.9161\n",
      "[Training Loss] Epoch 2.00 | Step 480 | Loss: 0.3948\n",
      "[Metrics] Accuracy: 0.9021 | Recall: 0.9021 | F1: 0.9018 | Specificity: 0.9510\n",
      "[Training Loss] Epoch 3.00 | Step 720 | Loss: 0.2859\n",
      "[Metrics] Accuracy: 0.9323 | Recall: 0.9323 | F1: 0.9322 | Specificity: 0.9661\n",
      "[Training Loss] Epoch 4.00 | Step 960 | Loss: 0.2292\n",
      "[Metrics] Accuracy: 0.9333 | Recall: 0.9333 | F1: 0.9331 | Specificity: 0.9667\n",
      "[Training Loss] Epoch 5.00 | Step 1200 | Loss: 0.1610\n",
      "[Metrics] Accuracy: 0.9500 | Recall: 0.9500 | F1: 0.9499 | Specificity: 0.9750\n",
      "[Training Loss] Epoch 6.00 | Step 1440 | Loss: 0.1404\n",
      "[Metrics] Accuracy: 0.9469 | Recall: 0.9469 | F1: 0.9468 | Specificity: 0.9734\n",
      "[Training Loss] Epoch 7.00 | Step 1680 | Loss: 0.1194\n",
      "[Metrics] Accuracy: 0.9552 | Recall: 0.9552 | F1: 0.9551 | Specificity: 0.9776\n",
      "[Training Loss] Epoch 8.00 | Step 1920 | Loss: 0.0918\n",
      "[Metrics] Accuracy: 0.9594 | Recall: 0.9594 | F1: 0.9593 | Specificity: 0.9797\n",
      "[Training Loss] Epoch 9.00 | Step 2160 | Loss: 0.0700\n",
      "[Metrics] Accuracy: 0.9583 | Recall: 0.9583 | F1: 0.9583 | Specificity: 0.9792\n",
      "[Training Loss] Epoch 10.00 | Step 2400 | Loss: 0.0659\n",
      "[Metrics] Accuracy: 0.9625 | Recall: 0.9625 | F1: 0.9624 | Specificity: 0.9812\n",
      "[Training Loss] Epoch 11.00 | Step 2640 | Loss: 0.0630\n",
      "[Metrics] Accuracy: 0.9531 | Recall: 0.9531 | F1: 0.9530 | Specificity: 0.9766\n",
      "[Training Loss] Epoch 12.00 | Step 2880 | Loss: 0.0497\n",
      "[Metrics] Accuracy: 0.9510 | Recall: 0.9510 | F1: 0.9508 | Specificity: 0.9755\n",
      "[Training Loss] Epoch 13.00 | Step 3120 | Loss: 0.0400\n",
      "[Metrics] Accuracy: 0.9469 | Recall: 0.9469 | F1: 0.9465 | Specificity: 0.9734\n",
      "[Training Loss] Epoch 14.00 | Step 3360 | Loss: 0.0334\n",
      "[Metrics] Accuracy: 0.9510 | Recall: 0.9510 | F1: 0.9507 | Specificity: 0.9755\n",
      "[Training Loss] Epoch 15.00 | Step 3600 | Loss: 0.0289\n",
      "[Metrics] Accuracy: 0.9448 | Recall: 0.9448 | F1: 0.9444 | Specificity: 0.9724\n",
      "[Training Loss] Epoch 16.00 | Step 3840 | Loss: 0.0190\n",
      "[Metrics] Accuracy: 0.9417 | Recall: 0.9417 | F1: 0.9411 | Specificity: 0.9708\n",
      "[Training Loss] Epoch 17.00 | Step 4080 | Loss: 0.0104\n",
      "[Metrics] Accuracy: 0.9542 | Recall: 0.9542 | F1: 0.9539 | Specificity: 0.9771\n",
      "[Training Loss] Epoch 18.00 | Step 4320 | Loss: 0.0063\n",
      "[Metrics] Accuracy: 0.9510 | Recall: 0.9510 | F1: 0.9508 | Specificity: 0.9755\n",
      "[Training Loss] Epoch 19.00 | Step 4560 | Loss: 0.0088\n",
      "[Metrics] Accuracy: 0.9500 | Recall: 0.9500 | F1: 0.9497 | Specificity: 0.9750\n",
      "[Training Loss] Epoch 20.00 | Step 4800 | Loss: 0.0055\n",
      "[Metrics] Accuracy: 0.9510 | Recall: 0.9510 | F1: 0.9507 | Specificity: 0.9755\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] Accuracy: 0.9510 | Recall: 0.9510 | F1: 0.9507 | Specificity: 0.9755\n",
      "[Metrics] Accuracy: 0.9510 | Recall: 0.9510 | F1: 0.9507 | Specificity: 0.9755\n",
      "\n",
      "[INFO] Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4800' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4800/4800 30:47, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.764100</td>\n",
       "      <td>0.440321</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.856250</td>\n",
       "      <td>0.856232</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.399400</td>\n",
       "      <td>0.285757</td>\n",
       "      <td>0.909375</td>\n",
       "      <td>0.909375</td>\n",
       "      <td>0.909237</td>\n",
       "      <td>0.954687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.286300</td>\n",
       "      <td>0.213656</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933238</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.198600</td>\n",
       "      <td>0.280823</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.929748</td>\n",
       "      <td>0.965104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.166000</td>\n",
       "      <td>0.179165</td>\n",
       "      <td>0.955208</td>\n",
       "      <td>0.955208</td>\n",
       "      <td>0.955168</td>\n",
       "      <td>0.977604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.150700</td>\n",
       "      <td>0.206749</td>\n",
       "      <td>0.948958</td>\n",
       "      <td>0.948958</td>\n",
       "      <td>0.948802</td>\n",
       "      <td>0.974479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.174714</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.960286</td>\n",
       "      <td>0.980208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.090100</td>\n",
       "      <td>0.233611</td>\n",
       "      <td>0.959375</td>\n",
       "      <td>0.959375</td>\n",
       "      <td>0.959448</td>\n",
       "      <td>0.979687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.078100</td>\n",
       "      <td>0.187267</td>\n",
       "      <td>0.965625</td>\n",
       "      <td>0.965625</td>\n",
       "      <td>0.965669</td>\n",
       "      <td>0.982812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>0.264710</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.956304</td>\n",
       "      <td>0.978125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.278504</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.954232</td>\n",
       "      <td>0.977083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.257234</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962442</td>\n",
       "      <td>0.981250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.283723</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.956180</td>\n",
       "      <td>0.978125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.266083</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.958261</td>\n",
       "      <td>0.979167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.015400</td>\n",
       "      <td>0.309943</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.952855</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.274033</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.963517</td>\n",
       "      <td>0.981771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.314474</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.954167</td>\n",
       "      <td>0.953977</td>\n",
       "      <td>0.977083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>0.271032</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962436</td>\n",
       "      <td>0.981250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.292411</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.956250</td>\n",
       "      <td>0.956117</td>\n",
       "      <td>0.978125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.296542</td>\n",
       "      <td>0.957292</td>\n",
       "      <td>0.957292</td>\n",
       "      <td>0.957158</td>\n",
       "      <td>0.978646</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Loss] Epoch 1.00 | Step 240 | Loss: 0.7641\n",
      "[Metrics] Accuracy: 0.8562 | Recall: 0.8562 | F1: 0.8562 | Specificity: 0.9281\n",
      "[Training Loss] Epoch 2.00 | Step 480 | Loss: 0.3994\n",
      "[Metrics] Accuracy: 0.9094 | Recall: 0.9094 | F1: 0.9092 | Specificity: 0.9547\n",
      "[Training Loss] Epoch 3.00 | Step 720 | Loss: 0.2863\n",
      "[Metrics] Accuracy: 0.9333 | Recall: 0.9333 | F1: 0.9332 | Specificity: 0.9667\n",
      "[Training Loss] Epoch 4.00 | Step 960 | Loss: 0.1986\n",
      "[Metrics] Accuracy: 0.9302 | Recall: 0.9302 | F1: 0.9297 | Specificity: 0.9651\n",
      "[Training Loss] Epoch 5.00 | Step 1200 | Loss: 0.1660\n",
      "[Metrics] Accuracy: 0.9552 | Recall: 0.9552 | F1: 0.9552 | Specificity: 0.9776\n",
      "[Training Loss] Epoch 6.00 | Step 1440 | Loss: 0.1507\n",
      "[Metrics] Accuracy: 0.9490 | Recall: 0.9490 | F1: 0.9488 | Specificity: 0.9745\n",
      "[Training Loss] Epoch 7.00 | Step 1680 | Loss: 0.1052\n",
      "[Metrics] Accuracy: 0.9604 | Recall: 0.9604 | F1: 0.9603 | Specificity: 0.9802\n",
      "[Training Loss] Epoch 8.00 | Step 1920 | Loss: 0.0901\n",
      "[Metrics] Accuracy: 0.9594 | Recall: 0.9594 | F1: 0.9594 | Specificity: 0.9797\n",
      "[Training Loss] Epoch 9.00 | Step 2160 | Loss: 0.0781\n",
      "[Metrics] Accuracy: 0.9656 | Recall: 0.9656 | F1: 0.9657 | Specificity: 0.9828\n",
      "[Training Loss] Epoch 10.00 | Step 2400 | Loss: 0.0601\n",
      "[Metrics] Accuracy: 0.9563 | Recall: 0.9562 | F1: 0.9563 | Specificity: 0.9781\n",
      "[Training Loss] Epoch 11.00 | Step 2640 | Loss: 0.0447\n",
      "[Metrics] Accuracy: 0.9542 | Recall: 0.9542 | F1: 0.9542 | Specificity: 0.9771\n",
      "[Training Loss] Epoch 12.00 | Step 2880 | Loss: 0.0355\n",
      "[Metrics] Accuracy: 0.9625 | Recall: 0.9625 | F1: 0.9624 | Specificity: 0.9812\n",
      "[Training Loss] Epoch 13.00 | Step 3120 | Loss: 0.0350\n",
      "[Metrics] Accuracy: 0.9563 | Recall: 0.9562 | F1: 0.9562 | Specificity: 0.9781\n",
      "[Training Loss] Epoch 14.00 | Step 3360 | Loss: 0.0181\n",
      "[Metrics] Accuracy: 0.9583 | Recall: 0.9583 | F1: 0.9583 | Specificity: 0.9792\n",
      "[Training Loss] Epoch 15.00 | Step 3600 | Loss: 0.0154\n",
      "[Metrics] Accuracy: 0.9531 | Recall: 0.9531 | F1: 0.9529 | Specificity: 0.9766\n",
      "[Training Loss] Epoch 16.00 | Step 3840 | Loss: 0.0066\n",
      "[Metrics] Accuracy: 0.9635 | Recall: 0.9635 | F1: 0.9635 | Specificity: 0.9818\n",
      "[Training Loss] Epoch 17.00 | Step 4080 | Loss: 0.0044\n",
      "[Metrics] Accuracy: 0.9542 | Recall: 0.9542 | F1: 0.9540 | Specificity: 0.9771\n",
      "[Training Loss] Epoch 18.00 | Step 4320 | Loss: 0.0066\n",
      "[Metrics] Accuracy: 0.9625 | Recall: 0.9625 | F1: 0.9624 | Specificity: 0.9812\n",
      "[Training Loss] Epoch 19.00 | Step 4560 | Loss: 0.0018\n",
      "[Metrics] Accuracy: 0.9563 | Recall: 0.9563 | F1: 0.9561 | Specificity: 0.9781\n",
      "[Training Loss] Epoch 20.00 | Step 4800 | Loss: 0.0022\n",
      "[Metrics] Accuracy: 0.9573 | Recall: 0.9573 | F1: 0.9572 | Specificity: 0.9786\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] Accuracy: 0.9573 | Recall: 0.9573 | F1: 0.9572 | Specificity: 0.9786\n",
      "[Metrics] Accuracy: 0.9573 | Recall: 0.9573 | F1: 0.9572 | Specificity: 0.9786\n",
      "\n",
      "[INFO] Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4800' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4800/4800 30:51, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.823200</td>\n",
       "      <td>0.652255</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.758333</td>\n",
       "      <td>0.756130</td>\n",
       "      <td>0.879167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.499129</td>\n",
       "      <td>0.855208</td>\n",
       "      <td>0.855208</td>\n",
       "      <td>0.854905</td>\n",
       "      <td>0.927604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.345600</td>\n",
       "      <td>0.476069</td>\n",
       "      <td>0.878125</td>\n",
       "      <td>0.878125</td>\n",
       "      <td>0.878074</td>\n",
       "      <td>0.939062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258000</td>\n",
       "      <td>0.408530</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899789</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.193800</td>\n",
       "      <td>0.416576</td>\n",
       "      <td>0.905208</td>\n",
       "      <td>0.905208</td>\n",
       "      <td>0.905340</td>\n",
       "      <td>0.952604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.171500</td>\n",
       "      <td>0.334724</td>\n",
       "      <td>0.926042</td>\n",
       "      <td>0.926042</td>\n",
       "      <td>0.925860</td>\n",
       "      <td>0.963021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.143600</td>\n",
       "      <td>0.289842</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.938513</td>\n",
       "      <td>0.969271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.330981</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.939681</td>\n",
       "      <td>0.969792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.093800</td>\n",
       "      <td>0.341865</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941667</td>\n",
       "      <td>0.941508</td>\n",
       "      <td>0.970833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.079500</td>\n",
       "      <td>0.316260</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944792</td>\n",
       "      <td>0.944643</td>\n",
       "      <td>0.972396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.434069</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.929167</td>\n",
       "      <td>0.928570</td>\n",
       "      <td>0.964583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.059000</td>\n",
       "      <td>0.481049</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.929725</td>\n",
       "      <td>0.965104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.049300</td>\n",
       "      <td>0.409566</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.939583</td>\n",
       "      <td>0.939294</td>\n",
       "      <td>0.969792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.048500</td>\n",
       "      <td>0.485186</td>\n",
       "      <td>0.931250</td>\n",
       "      <td>0.931250</td>\n",
       "      <td>0.930641</td>\n",
       "      <td>0.965625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.497320</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.930208</td>\n",
       "      <td>0.929712</td>\n",
       "      <td>0.965104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.026600</td>\n",
       "      <td>0.405695</td>\n",
       "      <td>0.940625</td>\n",
       "      <td>0.940625</td>\n",
       "      <td>0.940253</td>\n",
       "      <td>0.970312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.479649</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.938542</td>\n",
       "      <td>0.938216</td>\n",
       "      <td>0.969271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.470357</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.942708</td>\n",
       "      <td>0.942445</td>\n",
       "      <td>0.971354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.010100</td>\n",
       "      <td>0.488337</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937092</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.009900</td>\n",
       "      <td>0.490198</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937025</td>\n",
       "      <td>0.968750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Loss] Epoch 1.00 | Step 240 | Loss: 0.8232\n",
      "[Metrics] Accuracy: 0.7583 | Recall: 0.7583 | F1: 0.7561 | Specificity: 0.8792\n",
      "[Training Loss] Epoch 2.00 | Step 480 | Loss: 0.4517\n",
      "[Metrics] Accuracy: 0.8552 | Recall: 0.8552 | F1: 0.8549 | Specificity: 0.9276\n",
      "[Training Loss] Epoch 3.00 | Step 720 | Loss: 0.3456\n",
      "[Metrics] Accuracy: 0.8781 | Recall: 0.8781 | F1: 0.8781 | Specificity: 0.9391\n",
      "[Training Loss] Epoch 4.00 | Step 960 | Loss: 0.2580\n",
      "[Metrics] Accuracy: 0.9000 | Recall: 0.9000 | F1: 0.8998 | Specificity: 0.9500\n",
      "[Training Loss] Epoch 5.00 | Step 1200 | Loss: 0.1938\n",
      "[Metrics] Accuracy: 0.9052 | Recall: 0.9052 | F1: 0.9053 | Specificity: 0.9526\n",
      "[Training Loss] Epoch 6.00 | Step 1440 | Loss: 0.1715\n",
      "[Metrics] Accuracy: 0.9260 | Recall: 0.9260 | F1: 0.9259 | Specificity: 0.9630\n",
      "[Training Loss] Epoch 7.00 | Step 1680 | Loss: 0.1436\n",
      "[Metrics] Accuracy: 0.9385 | Recall: 0.9385 | F1: 0.9385 | Specificity: 0.9693\n",
      "[Training Loss] Epoch 8.00 | Step 1920 | Loss: 0.1163\n",
      "[Metrics] Accuracy: 0.9396 | Recall: 0.9396 | F1: 0.9397 | Specificity: 0.9698\n",
      "[Training Loss] Epoch 9.00 | Step 2160 | Loss: 0.0938\n",
      "[Metrics] Accuracy: 0.9417 | Recall: 0.9417 | F1: 0.9415 | Specificity: 0.9708\n",
      "[Training Loss] Epoch 10.00 | Step 2400 | Loss: 0.0795\n",
      "[Metrics] Accuracy: 0.9448 | Recall: 0.9448 | F1: 0.9446 | Specificity: 0.9724\n",
      "[Training Loss] Epoch 11.00 | Step 2640 | Loss: 0.0757\n",
      "[Metrics] Accuracy: 0.9292 | Recall: 0.9292 | F1: 0.9286 | Specificity: 0.9646\n",
      "[Training Loss] Epoch 12.00 | Step 2880 | Loss: 0.0590\n",
      "[Metrics] Accuracy: 0.9302 | Recall: 0.9302 | F1: 0.9297 | Specificity: 0.9651\n",
      "[Training Loss] Epoch 13.00 | Step 3120 | Loss: 0.0493\n",
      "[Metrics] Accuracy: 0.9396 | Recall: 0.9396 | F1: 0.9393 | Specificity: 0.9698\n",
      "[Training Loss] Epoch 14.00 | Step 3360 | Loss: 0.0485\n",
      "[Metrics] Accuracy: 0.9313 | Recall: 0.9313 | F1: 0.9306 | Specificity: 0.9656\n",
      "[Training Loss] Epoch 15.00 | Step 3600 | Loss: 0.0300\n",
      "[Metrics] Accuracy: 0.9302 | Recall: 0.9302 | F1: 0.9297 | Specificity: 0.9651\n",
      "[Training Loss] Epoch 16.00 | Step 3840 | Loss: 0.0266\n",
      "[Metrics] Accuracy: 0.9406 | Recall: 0.9406 | F1: 0.9403 | Specificity: 0.9703\n",
      "[Training Loss] Epoch 17.00 | Step 4080 | Loss: 0.0165\n",
      "[Metrics] Accuracy: 0.9385 | Recall: 0.9385 | F1: 0.9382 | Specificity: 0.9693\n",
      "[Training Loss] Epoch 18.00 | Step 4320 | Loss: 0.0177\n",
      "[Metrics] Accuracy: 0.9427 | Recall: 0.9427 | F1: 0.9424 | Specificity: 0.9714\n",
      "[Training Loss] Epoch 19.00 | Step 4560 | Loss: 0.0101\n",
      "[Metrics] Accuracy: 0.9375 | Recall: 0.9375 | F1: 0.9371 | Specificity: 0.9687\n",
      "[Training Loss] Epoch 20.00 | Step 4800 | Loss: 0.0099\n",
      "[Metrics] Accuracy: 0.9375 | Recall: 0.9375 | F1: 0.9370 | Specificity: 0.9687\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] Accuracy: 0.9375 | Recall: 0.9375 | F1: 0.9370 | Specificity: 0.9687\n",
      "[Metrics] Accuracy: 0.9375 | Recall: 0.9375 | F1: 0.9370 | Specificity: 0.9687\n",
      "\n",
      "[INFO] Fold 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4800' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4800/4800 30:49, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.838100</td>\n",
       "      <td>0.514930</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.823037</td>\n",
       "      <td>0.911458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.498900</td>\n",
       "      <td>0.296047</td>\n",
       "      <td>0.902083</td>\n",
       "      <td>0.902083</td>\n",
       "      <td>0.902137</td>\n",
       "      <td>0.951042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.334700</td>\n",
       "      <td>0.183256</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946875</td>\n",
       "      <td>0.946851</td>\n",
       "      <td>0.973437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.245700</td>\n",
       "      <td>0.269961</td>\n",
       "      <td>0.926042</td>\n",
       "      <td>0.926042</td>\n",
       "      <td>0.926302</td>\n",
       "      <td>0.963021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.189800</td>\n",
       "      <td>0.209887</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.953138</td>\n",
       "      <td>0.976562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.154800</td>\n",
       "      <td>0.239242</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.949968</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.144400</td>\n",
       "      <td>0.129096</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.968752</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.155082</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>0.962603</td>\n",
       "      <td>0.981250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.093700</td>\n",
       "      <td>0.171520</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.963542</td>\n",
       "      <td>0.963533</td>\n",
       "      <td>0.981771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.082200</td>\n",
       "      <td>0.175273</td>\n",
       "      <td>0.965625</td>\n",
       "      <td>0.965625</td>\n",
       "      <td>0.965598</td>\n",
       "      <td>0.982812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.059500</td>\n",
       "      <td>0.136227</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.968722</td>\n",
       "      <td>0.984375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.067000</td>\n",
       "      <td>0.179161</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.971875</td>\n",
       "      <td>0.971889</td>\n",
       "      <td>0.985937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.052500</td>\n",
       "      <td>0.172114</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966669</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.045400</td>\n",
       "      <td>0.220251</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.960417</td>\n",
       "      <td>0.960456</td>\n",
       "      <td>0.980208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.036800</td>\n",
       "      <td>0.200205</td>\n",
       "      <td>0.965625</td>\n",
       "      <td>0.965625</td>\n",
       "      <td>0.965597</td>\n",
       "      <td>0.982812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>0.178381</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>0.967705</td>\n",
       "      <td>0.983854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.028700</td>\n",
       "      <td>0.211057</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966663</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.174374</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966667</td>\n",
       "      <td>0.966686</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>0.188127</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>0.967711</td>\n",
       "      <td>0.983854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.017400</td>\n",
       "      <td>0.180482</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>0.967708</td>\n",
       "      <td>0.967707</td>\n",
       "      <td>0.983854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training Loss] Epoch 1.00 | Step 240 | Loss: 0.8381\n",
      "[Metrics] Accuracy: 0.8229 | Recall: 0.8229 | F1: 0.8230 | Specificity: 0.9115\n",
      "[Training Loss] Epoch 2.00 | Step 480 | Loss: 0.4989\n",
      "[Metrics] Accuracy: 0.9021 | Recall: 0.9021 | F1: 0.9021 | Specificity: 0.9510\n",
      "[Training Loss] Epoch 3.00 | Step 720 | Loss: 0.3347\n",
      "[Metrics] Accuracy: 0.9469 | Recall: 0.9469 | F1: 0.9469 | Specificity: 0.9734\n",
      "[Training Loss] Epoch 4.00 | Step 960 | Loss: 0.2457\n",
      "[Metrics] Accuracy: 0.9260 | Recall: 0.9260 | F1: 0.9263 | Specificity: 0.9630\n",
      "[Training Loss] Epoch 5.00 | Step 1200 | Loss: 0.1898\n",
      "[Metrics] Accuracy: 0.9531 | Recall: 0.9531 | F1: 0.9531 | Specificity: 0.9766\n",
      "[Training Loss] Epoch 6.00 | Step 1440 | Loss: 0.1548\n",
      "[Metrics] Accuracy: 0.9500 | Recall: 0.9500 | F1: 0.9500 | Specificity: 0.9750\n",
      "[Training Loss] Epoch 7.00 | Step 1680 | Loss: 0.1444\n",
      "[Metrics] Accuracy: 0.9688 | Recall: 0.9688 | F1: 0.9688 | Specificity: 0.9844\n",
      "[Training Loss] Epoch 8.00 | Step 1920 | Loss: 0.1180\n",
      "[Metrics] Accuracy: 0.9625 | Recall: 0.9625 | F1: 0.9626 | Specificity: 0.9812\n",
      "[Training Loss] Epoch 9.00 | Step 2160 | Loss: 0.0937\n",
      "[Metrics] Accuracy: 0.9635 | Recall: 0.9635 | F1: 0.9635 | Specificity: 0.9818\n",
      "[Training Loss] Epoch 10.00 | Step 2400 | Loss: 0.0822\n",
      "[Metrics] Accuracy: 0.9656 | Recall: 0.9656 | F1: 0.9656 | Specificity: 0.9828\n",
      "[Training Loss] Epoch 11.00 | Step 2640 | Loss: 0.0595\n",
      "[Metrics] Accuracy: 0.9688 | Recall: 0.9688 | F1: 0.9687 | Specificity: 0.9844\n",
      "[Training Loss] Epoch 12.00 | Step 2880 | Loss: 0.0670\n",
      "[Metrics] Accuracy: 0.9719 | Recall: 0.9719 | F1: 0.9719 | Specificity: 0.9859\n",
      "[Training Loss] Epoch 13.00 | Step 3120 | Loss: 0.0525\n",
      "[Metrics] Accuracy: 0.9667 | Recall: 0.9667 | F1: 0.9667 | Specificity: 0.9833\n",
      "[Training Loss] Epoch 14.00 | Step 3360 | Loss: 0.0454\n",
      "[Metrics] Accuracy: 0.9604 | Recall: 0.9604 | F1: 0.9605 | Specificity: 0.9802\n",
      "[Training Loss] Epoch 15.00 | Step 3600 | Loss: 0.0368\n",
      "[Metrics] Accuracy: 0.9656 | Recall: 0.9656 | F1: 0.9656 | Specificity: 0.9828\n",
      "[Training Loss] Epoch 16.00 | Step 3840 | Loss: 0.0393\n",
      "[Metrics] Accuracy: 0.9677 | Recall: 0.9677 | F1: 0.9677 | Specificity: 0.9839\n",
      "[Training Loss] Epoch 17.00 | Step 4080 | Loss: 0.0287\n",
      "[Metrics] Accuracy: 0.9667 | Recall: 0.9667 | F1: 0.9667 | Specificity: 0.9833\n",
      "[Training Loss] Epoch 18.00 | Step 4320 | Loss: 0.0285\n",
      "[Metrics] Accuracy: 0.9667 | Recall: 0.9667 | F1: 0.9667 | Specificity: 0.9833\n",
      "[Training Loss] Epoch 19.00 | Step 4560 | Loss: 0.0195\n",
      "[Metrics] Accuracy: 0.9677 | Recall: 0.9677 | F1: 0.9677 | Specificity: 0.9839\n",
      "[Training Loss] Epoch 20.00 | Step 4800 | Loss: 0.0174\n",
      "[Metrics] Accuracy: 0.9677 | Recall: 0.9677 | F1: 0.9677 | Specificity: 0.9839\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Metrics] Accuracy: 0.9677 | Recall: 0.9677 | F1: 0.9677 | Specificity: 0.9839\n",
      "[Metrics] Accuracy: 0.9677 | Recall: 0.9677 | F1: 0.9677 | Specificity: 0.9839\n",
      "[INFO] Fold terbaik berdasarkan F1-score: Fold 5\n"
     ]
    }
   ],
   "source": [
    "# =================== IMPORT ===================\n",
    "import os, numpy as np, pandas as pd, random, shutil\n",
    "import torch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (\n",
    "    BertTokenizer, BertConfig, BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments, set_seed, TrainerCallback\n",
    ")\n",
    "\n",
    "# =================== CALLBACK LOG ===================\n",
    "class LossLoggerCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and \"loss\" in logs:\n",
    "            print(f\"[Training Loss] Epoch {state.epoch:.2f} | Step {state.global_step} | Loss: {logs['loss']:.4f}\")\n",
    "\n",
    "# =================== SPESIFISITAS PER KELAS ===================\n",
    "def specificity_per_class(true, pred, label, num_classes):\n",
    "    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n",
    "    TN = cm.sum() - (cm[label, :].sum() + cm[:, label].sum() - cm[label, label])\n",
    "    FP = cm[:, label].sum() - cm[label, label]\n",
    "    return TN / (TN + FP + 1e-8)\n",
    "\n",
    "# =================== KONFIGURASI ===================\n",
    "DATA_DIR   = \"/workspace/SPLIT_SLIDING_FINAL/train\"\n",
    "OUTPUT_DIR = \"/workspace/HASIL_BERT_KFOLD/HASIL_13\"\n",
    "LABEL_MAP  = {'AFIB': 0, 'VFL': 1, 'VT': 2}\n",
    "N_SPLITS   = 5\n",
    "SEED       = 42\n",
    "EPOCHS     = 20\n",
    "BATCH_SIZE = 16\n",
    "LR         = 2e-5\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# =================== LOAD DATA ===================\n",
    "def load_data(data_dir):\n",
    "    data, labels = [], []\n",
    "    for label_name in os.listdir(data_dir):\n",
    "        label_path = os.path.join(data_dir, label_name)\n",
    "        if label_name not in LABEL_MAP:\n",
    "            continue\n",
    "        for file in os.listdir(label_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                path = os.path.join(label_path, file)\n",
    "                try:\n",
    "                    signal = np.load(path)\n",
    "                    if signal.ndim != 1:\n",
    "                        continue\n",
    "                    data.append(signal)\n",
    "                    labels.append(LABEL_MAP[label_name])\n",
    "                except:\n",
    "                    continue\n",
    "    return np.array(data, dtype=object), np.array(labels)\n",
    "\n",
    "X, y = load_data(DATA_DIR)\n",
    "\n",
    "# =================== SIGNAL TO TEXT ===================\n",
    "def signal_to_text(sig, target_len=512):\n",
    "    if len(sig) < target_len:\n",
    "        pad = np.full(target_len - len(sig), sig[-1])\n",
    "        sig = np.concatenate([sig, pad])\n",
    "    else:\n",
    "        idx = np.linspace(0, len(sig)-1, target_len).astype(int)\n",
    "        sig = sig[idx]\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm))\n",
    "\n",
    "texts = [signal_to_text(sig) for sig in X]\n",
    "\n",
    "# =================== TOKENIZER ===================\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "def tokenize_batch(texts):\n",
    "    return tokenizer(texts, padding='max_length', truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# =================== DATASET ===================\n",
    "class ECGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# =================== METRIK GLOBAL ===================\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    recall = recall_score(labels, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(labels, preds, average='macro', zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    specs = []\n",
    "    for class_idx in range(len(LABEL_MAP)):\n",
    "        spec = specificity_per_class(labels, preds, class_idx, num_classes=len(LABEL_MAP))\n",
    "        specs.append(spec)\n",
    "    specificity = np.mean(specs)\n",
    "\n",
    "    print(f\"[Metrics] Accuracy: {acc:.4f} | Recall: {recall:.4f} | F1: {f1:.4f} | Specificity: {specificity:.4f}\")\n",
    "    return { \"accuracy\": acc, \"recall\": recall, \"f1\": f1, \"specificity\": specificity }\n",
    "\n",
    "# =================== TRAINING LOOP ===================\n",
    "results_all = []\n",
    "best_f1 = 0\n",
    "best_fold = None\n",
    "best_model_path = \"\"\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(texts, y), 1):\n",
    "    print(f\"\\n[INFO] Fold {fold}\")\n",
    "    X_train, X_val = [texts[i] for i in train_idx], [texts[i] for i in val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    train_enc = tokenize_batch(X_train)\n",
    "    val_enc = tokenize_batch(X_val)\n",
    "    train_dataset = ECGDataset(train_enc, y_train)\n",
    "    val_dataset = ECGDataset(val_enc, y_val)\n",
    "\n",
    "    config = BertConfig.from_pretrained(\"bert-base-uncased\",\n",
    "        num_labels=len(LABEL_MAP),\n",
    "        hidden_dropout_prob=0.2,\n",
    "        attention_probs_dropout_prob=0.2)\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config).to(device)\n",
    "\n",
    "    fold_dir = os.path.join(OUTPUT_DIR, f\"fold{fold}\")\n",
    "    os.makedirs(fold_dir, exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(fold_dir, \"model\"),\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=LR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        weight_decay=0.01,\n",
    "        report_to=\"none\",\n",
    "        logging_dir=os.path.join(fold_dir, \"logs\"),\n",
    "        logging_steps=100,\n",
    "        save_total_limit=1\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[LossLoggerCallback()]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    trainer.save_model(os.path.join(fold_dir, \"model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(fold_dir, \"model\"))\n",
    "\n",
    "    pred_output = trainer.predict(val_dataset)\n",
    "    eval_result = compute_metrics(pred_output)\n",
    "    preds = np.argmax(pred_output.predictions, axis=1)\n",
    "    labels = y_val\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", cbar=True,\n",
    "                xticklabels=LABEL_MAP.keys(), yticklabels=LABEL_MAP.keys())\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix Fold {fold}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(fold_dir, f\"confmat_fold{fold}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    df_class = []\n",
    "    for class_idx, class_name in enumerate(LABEL_MAP.keys()):\n",
    "        true_binary = (labels == class_idx).astype(int)\n",
    "        pred_binary = (preds == class_idx).astype(int)\n",
    "        acc = accuracy_score(true_binary, pred_binary)\n",
    "        rec = recall_score(true_binary, pred_binary, zero_division=0)\n",
    "        f1s = f1_score(true_binary, pred_binary, zero_division=0)\n",
    "        spec = specificity_per_class(labels, preds, class_idx, num_classes=len(LABEL_MAP))\n",
    "        df_class.append([fold, class_name, acc, rec, f1s, spec])\n",
    "\n",
    "    df_fold = pd.DataFrame(df_class, columns=[\"fold\", \"kelas\", \"akurasi\", \"recall\", \"f1 score\", \"spesifisitas\"])\n",
    "    df_fold.to_csv(os.path.join(fold_dir, f\"hasil_fold{fold}.csv\"), index=False)\n",
    "\n",
    "    results_all.append(eval_result)\n",
    "\n",
    "    if eval_result[\"f1\"] > best_f1:\n",
    "        best_f1 = eval_result[\"f1\"]\n",
    "        best_fold = fold\n",
    "        best_model_path = os.path.join(fold_dir, \"model\")\n",
    "        shutil.copyfile(os.path.join(fold_dir, f\"confmat_fold{fold}.png\"), os.path.join(OUTPUT_DIR, \"confmat_final.png\"))\n",
    "\n",
    "# Copy best model ke OUTPUT_DIR\n",
    "if best_model_path:\n",
    "    shutil.copytree(best_model_path, os.path.join(OUTPUT_DIR, \"bert_best\"), dirs_exist_ok=True)\n",
    "\n",
    "# Gabungkan semua hasil CSV per fold\n",
    "all_df = []\n",
    "for fold in range(1, N_SPLITS+1):\n",
    "    csv_path = os.path.join(OUTPUT_DIR, f\"fold{fold}\", f\"hasil_fold{fold}.csv\")\n",
    "    if os.path.exists(csv_path):\n",
    "        all_df.append(pd.read_csv(csv_path))\n",
    "\n",
    "final_df = pd.concat(all_df, ignore_index=True)\n",
    "final_df.to_csv(os.path.join(OUTPUT_DIR, \"hasil_kfold_bert.csv\"), index=False)\n",
    "print(f\"[INFO] Fold terbaik berdasarkan F1-score: Fold {best_fold}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6731906-a2c3-411e-ac53-bb5ff67a33e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BERT TEST] Akurasi: 0.4800 | Recall: 0.4800 | F1-score: 0.4351\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHqCAYAAAD4YG/CAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAU81JREFUeJzt3XlcVNX7B/DPDMuwg4gwEIoIiuCemiKuiQrilpjhEqCoaWjllmGaigtm5ppLC4KZWqmphbvimmhuuIvikhuDqAGCMmz398f8mK8joIyOjFw+71739WruPXPuucMoj89zzr0SQRAEEBEREYmEVN8DICIiItIlBjdEREQkKgxuiIiISFQY3BAREZGoMLghIiIiUWFwQ0RERKLC4IaIiIhEhcENERERiQqDGyIiIhIVBjckGleuXEHnzp1hbW0NiUSCTZs26bT/GzduQCKRIDY2Vqf9VmTt27dH+/bt9T0MIiINDG5Ip65evYqPPvoItWrVgomJCaysrODj44OFCxfiyZMnr/XcISEhOHv2LGbOnIlVq1ahWbNmr/V85Sk0NBQSiQRWVlYlfo5XrlyBRCKBRCLB3Llzte7/7t27mDp1KhITE3Uw2vJRs2ZN9TVLJBKYmJigdu3aGD9+PB4+fKjRdurUqRptn90UCgWA/wWwRZtUKoWtrS38/f2RkJAAAIiNjX1uX0VbzZo1yzTu0jZdBdGzZs3SeaBP9KYz1PcASDy2bNmC999/HzKZDMHBwahfvz5yc3Nx6NAhjB8/HufPn8cPP/zwWs795MkTJCQk4Msvv8TIkSNfyzlcXFzw5MkTGBkZvZb+X8TQ0BCPHz/GX3/9hb59+2ocW716NUxMTJCTk/NSfd+9exfTpk1DzZo10bhx4zK/b+fOnS91Pl1p3Lgxxo4dCwDIycnBiRMnsGDBAuzfvx///PNPsfbLli2DhYVFsf02NjYar/v164euXbuioKAAly9fxtKlS9GhQwccO3YMbdu2xapVqzTaDxkyBO+88w6GDRum3lfSeQBgwYIFyMrKUr/eunUr1q5di/nz58POzk69v1WrVi/+AMpg1qxZ6NOnD3r16qWT/ogqAgY3pBPXr19HUFAQXFxcEB8fD0dHR/Wx8PBwJCcnY8uWLa/t/GlpaQCK/5LSpaLsgL7IZDL4+Phg7dq1xYKbNWvWICAgABs2bCiXsTx+/BhmZmYwNjYul/OV5q233sLAgQPVr4cMGQILCwvMnTsXV65cQe3atTXa9+nTRyOAKM3bb7+t0W+bNm3g7++PZcuWYenSpahVq5ZG++HDh6NWrVoa7ynNs0GGQqHA2rVr0atXr1KzPUSkHZalSCfmzJmDrKwsREdHawQ2Rdzd3fHpp5+qX+fn52P69Olwc3ODTCZDzZo1MXHiRCiVSo331axZE926dcOhQ4fwzjvvwMTEBLVq1cLPP/+sbjN16lS4uLgAAMaPH69REggNDS3xF0ZRmeJpu3btQuvWrWFjYwMLCwt4eHhg4sSJ6uOlzbmJj49HmzZtYG5uDhsbG/Ts2RMXL14s8XzJyckIDQ2FjY0NrK2tMWjQIDx+/Lj0D/YZ/fv3x7Zt25Cenq7ed+zYMVy5cgX9+/cv1v7hw4cYN24cGjRoAAsLC1hZWcHf3x+nT59Wt9m3bx+aN28OABg0aFCxskj79u1Rv359nDhxAm3btoWZmZn6c3l2zk1ISAhMTEyKXX+XLl1QpUoV3L17t8zX+rLkcjkAVaZLV9q0aQNAVXYtL7/88guaNm0KU1NT2NraIigoCLdu3dJoc+XKFQQGBkIul8PExATOzs4ICgpCRkYGAFVAnp2djZUrV6p/rqGhoeV2DUT6wswN6cRff/2FWrVqlTmVPmTIEKxcuRJ9+vTB2LFjcfToUURFReHixYvYuHGjRtvk5GT06dMHYWFhCAkJwYoVKxAaGoqmTZuiXr166N27N2xsbDB69Gh1OaG0kkBpzp8/j27duqFhw4aIjIyETCZDcnIy/v777+e+b/fu3fD390etWrUwdepUPHnyBIsXL4aPjw9OnjxZLLDq27cvXF1dERUVhZMnT+Knn36Cvb09vv766zKNs3fv3hg+fDj++OMPDB48GIAqa1O3bl28/fbbxdpfu3YNmzZtwvvvvw9XV1ekpqbi+++/R7t27XDhwgU4OTnB09MTkZGR+OqrrzBs2DD1L/Knf5YPHjyAv78/goKCMHDgQDg4OJQ4voULFyI+Ph4hISFISEiAgYEBvv/+e+zcuROrVq2Ck5NTma6zrPLy8nD//n0AqrLUqVOnMG/ePLRt2xaurq7F2j87FwdQBUEvyvjduHEDAFClSpVXHnNZzJw5E5MnT0bfvn0xZMgQpKWlYfHixWjbti1OnToFGxsb5ObmokuXLlAqlRg1ahTkcjnu3LmDuLg4pKenw9raGqtWrSpWMnNzcyuXayDSK4HoFWVkZAgAhJ49e5apfWJiogBAGDJkiMb+cePGCQCE+Ph49T4XFxcBgHDgwAH1vnv37gkymUwYO3aset/169cFAMI333yj0WdISIjg4uJSbAxTpkwRnv76z58/XwAgpKWllTruonPExMSo9zVu3Fiwt7cXHjx4oN53+vRpQSqVCsHBwcXON3jwYI0+33vvPaFq1aqlnvPp6zA3NxcEQRD69OkjdOzYURAEQSgoKBDkcrkwbdq0Ej+DnJwcoaCgoNh1yGQyITIyUr3v2LFjxa6tSLt27QQAwvLly0s81q5dO419O3bsEAAIM2bMEK5duyZYWFgIvXr1euE1aqvou/Hs5uPjI9y/f1+jbdHnX9Lm4eGhblf0GU6bNk1IS0sTFAqFcPDgQaF58+YCAGHdunUljsXc3FwICQl5qev45ptvBADC9evXBUEQhBs3bggGBgbCzJkzNdqdPXtWMDQ0VO8/derUc8eki7ERVVQsS9Ery8zMBABYWlqWqf3WrVsBAGPGjNHYXzQx9Nm5OV5eXupsAgBUq1YNHh4euHbt2kuP+VlF/3LfvHkzCgsLy/SelJQUJCYmIjQ0FLa2tur9DRs2RKdOndTX+bThw4drvG7Tpg0ePHig/gzLon///ti3bx8UCgXi4+OhUChKLEkBqnk6Uqnqj3lBQQEePHigLrmdPHmyzOeUyWQYNGhQmdp27twZH330ESIjI9G7d2+YmJjg+++/L/O5tNGiRQvs2rULu3btQlxcHGbOnInz58+jR48eJa4q27Bhg7p90RYTE1Os3ZQpU1CtWjXI5XK0adMGFy9exLfffos+ffq8lut42h9//IHCwkL07dsX9+/fV29yuRy1a9fG3r17AQDW1tYAgB07dmhV2iSqDFiWoldmZWUFAHj06FGZ2v/777+QSqVwd3fX2C+Xy2FjY4N///1XY3+NGjWK9VGlShX8999/Lzni4j744AP89NNPGDJkCL744gt07NgRvXv3Rp8+fdTBQUnXAQAeHh7Fjnl6emLHjh3Izs6Gubm5ev+z11JU5vjvv//Un+OLdO3aFZaWlvjtt9+QmJiI5s2bw93dXV06eVphYSEWLlyIpUuX4vr16ygoKFAfq1q1apnOB6gm7mozeXju3LnYvHkzEhMTsWbNGtjb27/wPWlpaRrjs7CweGF50c7ODr6+vurXAQEB8PDwQJ8+ffDTTz9h1KhRGu3btm1bpgnFw4YNw/vvv4+cnBzEx8dj0aJFGmN7na5cuQJBEIpNhi5StFrP1dUVY8aMwbx587B69Wq0adMGPXr0wMCBA9WBD1FlxeCGXpmVlRWcnJxw7tw5rd737ITe0hgYGJS4XxCElz7Hs7+oTE1NceDAAezduxdbtmzB9u3b8dtvv+Hdd9/Fzp07Sx2Dtl7lWorIZDL07t0bK1euxLVr1zB16tRS286aNQuTJ0/G4MGDMX36dNja2kIqleKzzz4rc4YKUH0+2jh16hTu3bsHADh79iz69ev3wvc0b95cI7CdMmXKc6+tNB07dgQAHDhwoFhwU1a1a9dWB03dunWDgYEBvvjiC3To0OG13z+psLAQEokE27ZtK/H78nTA9+233yI0NBSbN2/Gzp078cknnyAqKgpHjhyBs7Pzax0n0ZuMwQ3pRLdu3fDDDz8gISEB3t7ez23r4uKCwsJCXLlyBZ6enur9qampSE9PV6980oUqVaporCwq8mx2CACkUik6duyIjh07Yt68eZg1axa+/PJL7N27VyM78PR1AEBSUlKxY5cuXYKdnZ1G1kaX+vfvjxUrVkAqlSIoKKjUduvXr0eHDh0QHR2tsT89PV0jg1HWQLMssrOzMWjQIHh5eaFVq1aYM2cO3nvvPfWKrNKsXr1ao5T07HLrssrPzwcAjXvJvKovv/wSP/74IyZNmoTt27frrN+SuLm5QRAEuLq6ok6dOi9s36BBAzRo0ACTJk3C4cOH4ePjg+XLl2PGjBkAdPuzJaooOOeGdOLzzz+Hubk5hgwZgtTU1GLHr169ioULFwJQlVUA1c3MnjZv3jwAqtKCrri5uSEjIwNnzpxR70tJSSm2IqukVTRFN7N7dnl6EUdHRzRu3BgrV67UCKDOnTuHnTt3qq/zdejQoQOmT5+O7777Tr30uSQGBgbFskLr1q3DnTt3NPYVBWElBYLamjBhAm7evImVK1di3rx5qFmzJkJCQkr9HIv4+PjA19dXvb1scPPXX38BABo1avRS7y+JjY0NPvroI+zYseO138W5d+/eMDAwwLRp04r97ARBwIMHDwCo5roVBXJFGjRoAKlUqvFZm5ub6+TnSlSRMHNDOuHm5oY1a9bggw8+gKenp8Ydig8fPox169ap76/RqFEjhISE4IcffkB6ejratWuHf/75BytXrkSvXr3QoUMHnY0rKCgIEyZMwHvvvYdPPvkEjx8/xrJly1CnTh2NCbWRkZE4cOAAAgIC4OLignv37mHp0qVwdnZG69atS+3/m2++gb+/P7y9vREWFqZeCm5tbf1SJZWykkqlmDRp0gvbdevWDZGRkRg0aBBatWqFs2fPYvXq1cUCBzc3N9jY2GD58uWwtLSEubk5WrRoUeJy6ueJj4/H0qVLMWXKFPXS9JiYGLRv3x6TJ0/GnDlztOrvRe7cuYNffvkFAJCbm4vTp0/j+++/h52dXYklqfXr15c4j6dTp06lLm8v8umnn2LBggWYPXs2fv31V91cQAnc3NwwY8YMRERE4MaNG+jVqxcsLS1x/fp1bNy4EcOGDcO4ceMQHx+PkSNH4v3330edOnWQn5+PVatWwcDAAIGBger+mjZtit27d2PevHlwcnKCq6srWrRo8drGT/RG0ONKLRKhy5cvC0OHDhVq1qwpGBsbC5aWloKPj4+wePFiIScnR90uLy9PmDZtmuDq6ioYGRkJ1atXFyIiIjTaCIJquW9AQECx8zy7BLm0peCCIAg7d+4U6tevLxgbGwseHh7CL7/8Umwp+J49e4SePXsKTk5OgrGxseDk5CT069dPuHz5crFzPLtcevfu3YKPj49gamoqWFlZCd27dxcuXLig0abofM8uNY+JidFYBlyap5eCl6a0peBjx44VHB0dBVNTU8HHx0dISEgocQn35s2bBS8vL8HQ0FDjOtu1ayfUq1evxHM+3U9mZqbg4uIivP3220JeXp5Gu9GjRwtSqVRISEh47jVo49ml4FKpVLC3txf69esnJCcna7R93lJwAMLevXsFQXj+90gQBCE0NFQwMDAo1r8ul4IX2bBhg9C6dWvB3NxcMDc3F+rWrSuEh4cLSUlJgiAIwrVr14TBgwcLbm5ugomJiWBrayt06NBB2L17t0Y/ly5dEtq2bSuYmpoKALgsnCoFiSBoMZORiIiI6A3HOTdEREQkKgxuiIiISFQY3BAREZGoMLghIiIiUWFwQ0RERKLC4IaIiIhEhcENERERiYoo71Bs2mSkvodAFcz9o4v1PQSqQAykfF4TaceknH7b6vr335NT3+m0v/LCzA0RERGJiigzN0RERJWShDkLgMENERGReEhYMgVYliIiIiKRYeaGiIhILFiWAsDMDREREYkMMzdERERiwTk3ABjcEBERiQfLUgBYliIiIiKRYeaGiIhILFiWAsDghoiISDxYlgLAshQRERGJDDM3REREYsGyFABmboiIiEhkmLkhIiISC865AcDghoiISDxYlgLAshQRERGJDDM3REREYsGyFABmboiIiMRDItHtpoVly5ahYcOGsLKygpWVFby9vbFt2zb18fbt20MikWhsw4cP1+jj5s2bCAgIgJmZGezt7TF+/Hjk5+dr/TEwc0NERESvzNnZGbNnz0bt2rUhCAJWrlyJnj174tSpU6hXrx4AYOjQoYiMjFS/x8zMTP3/BQUFCAgIgFwux+HDh5GSkoLg4GAYGRlh1qxZWo2FwQ0REZFY6LEs1b17d43XM2fOxLJly3DkyBF1cGNmZga5XF7i+3fu3IkLFy5g9+7dcHBwQOPGjTF9+nRMmDABU6dOhbGxcZnHwrIUERGRWEikut1eUkFBAX799VdkZ2fD29tbvX/16tWws7ND/fr1ERERgcePH6uPJSQkoEGDBnBwcFDv69KlCzIzM3H+/Hmtzs/MDREREZVIqVRCqVRq7JPJZJDJZCW2P3v2LLy9vZGTkwMLCwts3LgRXl5eAID+/fvDxcUFTk5OOHPmDCZMmICkpCT88ccfAACFQqER2ABQv1YoFFqNm8ENERGRWEh1e5+bqKgoTJs2TWPflClTMHXq1BLbe3h4IDExERkZGVi/fj1CQkKwf/9+eHl5YdiwYep2DRo0gKOjIzp27IirV6/Czc1Np+NmcENEREQlioiIwJgxYzT2lZa1AQBjY2O4u7sDAJo2bYpjx45h4cKF+P7774u1bdGiBQAgOTkZbm5ukMvl+OeffzTapKamAkCp83RKwzk3REREYqHjOTcymUy9tLtoe15w86zCwsJiZa0iiYmJAABHR0cAgLe3N86ePYt79+6p2+zatQtWVlbq0lZZMXNDREQkFnp8/EJERAT8/f1Ro0YNPHr0CGvWrMG+ffuwY8cOXL16FWvWrEHXrl1RtWpVnDlzBqNHj0bbtm3RsGFDAEDnzp3h5eWFDz/8EHPmzIFCocCkSZMQHh6uVUAFMLghIiIiHbh37x6Cg4ORkpICa2trNGzYEDt27ECnTp1w69Yt7N69GwsWLEB2djaqV6+OwMBATJo0Sf1+AwMDxMXFYcSIEfD29oa5uTlCQkI07otTVhJBEARdXtybwLTJSH0PgSqY+0cX63sIVIEY6HjSJomfSTmlEkx9Z+u0vye7v9Bpf+WFmRsiIiKx4FPBAXBCMREREYkMMzdERERiwaeCA2DmhoiIiESGmRsiIiKx4JwbAAxuiIiIxINlKQAsSxEREZHIMHNDREQkFixLAWBwQ0REJB4sSwFgWYqIiIhEhpkbIiIisWBZCgCDGyIiIvFgWQoAy1JEREQkMszcEBERiQUzNwCYuSEiIiKRYeaGiIhILDihGACDGyIiIvFgWQoAy1JEREQkMszcEBERiQXLUgAY3BAREYkHy1IAWJYiIiIikWHmhoiISCxYlgLA4IaIiEg0JAxuALAsRURERCLzxmVucnNzkZubCwsLC30PhYiIqEJh5kZFr5mbmJgYjBo1CqtXrwYAREREwNLSEtbW1ujUqRMePHigz+ERERFRBaS34GbmzJkIDw/HpUuX8Mknn2DEiBGIjY1FZGQkZs+ejUuXLmHSpEn6Gh4REVHFI9HxVkHprSwVGxuL6Oho9OvXD8ePH0eLFi3w+++/IzAwEABQv359DB8+XF/DIyIiqnBYllLRW+bm5s2baN26NQCgWbNmMDQ0RP369dXHGzZsiJSUFH0Nj4iIiCoovWVu8vLyIJPJ1K+NjY1hZGSkfm1oaIiCggJ9DI2IiKhCYuZGRa+rpS5cuACFQgEAEAQBly5dQlZWFgDg/v37+hwaERFRhcPgRkWvwU3Hjh0hCIL6dbdu3QCofjiCIPCHRERERFrTW3Bz/fp1fZ1aNIa+3xpD+7SBi5MtAODiNQVm/bANO/++gBqOtkjaGlni+waMj8Yfu08BAL79vA9aNqqFeu6OuHQ9FS2DZpfb+OnNcOL4MfwcG42LF87jfloavl3wHTp09FUff/w4G4vmf4t98XuQkZEOp7ec0W/Ah+jTN0iPoyZ9OXH8GGJXROPihXNIS0vD/EVL8O5T35fdu3Zi3e+/4uL588jISMdv6zehrqenHkdcuTApoKK34MbFxUVfpxaNO6npmLx4M5JvpkECCQZ2b4F184ehZdBsJN1IRU3fCI32gwN9MDrYFzv+Pq+x/+fNR9C8gQvq136rPIdPb4icJ09Qp05d9HwvEOM+G1Xs+LdzZuPYP0cxY/YcODm9hYTDf2P2zEhUq2aPdh3e1cOISZ+ePHkMDw8P9OodiDGfjizxeJMmb6NLF39Mm8LbeZB+6C24OXPmTJnaNWzY8DWPpOLaeuCcxuupS/7C0Pdb452Grrh4TYHUB480jvfo0Agbdp1E9pNc9b6xc9YDAOyqdGVwU0n5tGkLnzZtSz1+5nQiuvfohWbNWwAAAt//ABvW/YZzZ88wuKmEWrdph9Zt2pV6vHuPXgCAO3dul9OISAMTNwD0GNw0btxYPbemNBKJhCumykgqlSCw09swNzXG0TPFS35NPKujcd3qGD37dz2Mjiqyho0aY/++ePR8LxDV7O1x/NhR3Pz3BsZ+HvHiNxNRuWJZSoVzbiq4eu5O2LdyLEyMDZH1RIkPxv6IS9cUxdqF9PLGxWspOHKanztpZ8LEyZgxbTL8fNvB0NAQEokEk6dOR9NmzfU9NCKiElX4OTdKpRJKpVJjn1BYAInUQCf9v+ku30hFi6AoWFuY4j3fJvgx8kN0HrJQI8AxkRnhA/9mmP3jdj2OlCqqX9eswtkzpzF/8VI4Or6FkyeOqefctPBupe/hEdFTmLlR0dsdioODg/Ho0f/mhJw+fRp5eXla9xMVFQVra2uNLT/1hC6H+kbLyy/AtVv3ceriLXy1+E+cvXwH4f3aa7R5z7cxzEyMsTruH/0MkiqsnJwcfLdwAcaM/wLt2r+LOh4eCOo/EJ39uuLnlSv0PTwieoZEItHpVlHpLbhZvXo1njx5on7dpk0b3Lp1S+t+IiIikJGRobEZOjTV5VArFKlEApmxZkIutFcrbNl/Fvf/y9LTqKiiys/PR35+HqQSzb8qpFIphMJCPY2KiOj59FaWenYi8fMmFj+PTCbTeIwDgEpTkooc1QM7/j6PWyn/wdLcBB/4N0PbZrXR/eOl6ja1qtuh9dtu6DVqWYl91KpuBwtTGRzsrGAqM0LDOqoVUxevKZCXz8nclcHjx9m4dfOm+vWdO7eRdOkirKyt4ejohKbNmmPBvG8gM5HB0fEtnDj+D7b8tRljxn+hx1GTvjzOzsbNp78vt2/j0sWLsLa2hqOTEzLS05GSkoK0tHsAgBs3VPP87OzsYFetml7GXJlU5GyLLkmEl40qXpFUKoVCoYC9vT0AwNLSEqdPn0atWrVeuW/TJsXvvSBGy6b0R4d3PCC3s0JGVg7OXbmDb2N2I/7oJXWbaSO7o1/X5vAImFJiALnjx0/RtlntYvs9un6FmykPX+v43yT3jy7W9xD05vixoxg2OKTY/u49emHazNm4fz8NixfMw5GEv5GZkQFHRyf07tMXA4JDK+1fpAbSynndAHDsn6MYMii42P4ePd/D9FmzsXnjH/hqUvGVdMM/HokR4cXvo1RZmJRTKqFqyFqd9vdgZT+d9lde9BrcxMfHw9ZWdXfdVq1a4ffff4ezs7NGu5e5z01lCW5IdypzcEPaq8zBDb0cBjfl6418tlQR3ueGiIio7CprNvVZb/R9bp5eTUVERERUFm/cfW4ePXqEtWvXIjo6GsePH2fmhoiIqIyYuVHR21LwZx04cAAhISFwdHTE3Llz0aFDBxw5ckTfwyIiIqow9Hmfm2XLlqFhw4awsrKClZUVvL29sW3bNvXxnJwchIeHo2rVqrCwsEBgYCBSU1M1+rh58yYCAgJgZmYGe3t7jB8/Hvn5+Vp/Dnqdc6NQKBAbG4vo6GhkZmaib9++UCqV2LRpE7y8vPQ5NCIiItKCs7MzZs+ejdq1a0MQBKxcuRI9e/bEqVOnUK9ePYwePRpbtmzBunXrYG1tjZEjR6J37974+++/AQAFBQUICAiAXC7H4cOHkZKSguDgYBgZGWHWrFlajUVvq6W6d++OAwcOICAgAAMGDICfnx8MDAxgZGSE06dPv1Jww9VSpC2uliJtcLUUaau8VkvZh+n24cj3ovu+0vttbW3xzTffoE+fPqhWrRrWrFmDPn36AAAuXboET09PJCQkoGXLlti2bRu6deuGu3fvwsHBAQCwfPlyTJgwAWlpaTA2Ni7zefVWltq2bRvCwsIwbdo0BAQEwMCgctx4j4iI6HXRdVlKqVQiMzNTY3v2eY4lKSgowK+//ors7Gx4e3vjxIkTyMvLg6+vr7pN3bp1UaNGDSQkJAAAEhIS0KBBA3VgAwBdunRBZmYmzp8/r9XnoLfg5tChQ3j06BGaNm2KFi1a4LvvvsP9+/f1NRwiIiJ6RknPb4yKiiq1/dmzZ2FhYQGZTIbhw4dj48aN8PLygkKhgLGxMWxsbDTaOzg4QKFQPehZoVBoBDZFx4uOaUNvwU3Lli3x448/IiUlBR999BF+/fVXODk5obCwELt27eIycCIiIi3pOnNT0vMbIyKK34G6iIeHBxITE3H06FGMGDECISEhuHDhQjl+Aip6Xy1lbm6OwYMH49ChQzh79izGjh2L2bNnw97eHj169ND38IiIiCoMXQc3MplMvfqpaHv2eY5PMzY2hru7O5o2bYqoqCg0atQICxcuhFwuR25uLtLT0zXap6amQi6XAwDkcnmx1VNFr4valJXeg5uneXh4YM6cObh9+zbWrtXtLaSJiIiofBUWFkKpVKJp06YwMjLCnj171MeSkpJw8+ZNeHt7AwC8vb1x9uxZ3Lt3T91m165dsLKy0nqRkV6XgpfGwMAAvXr1Qq9evfQ9FCIiogpDnzfxi4iIgL+/P2rUqIFHjx5hzZo12LdvH3bs2AFra2uEhYVhzJgxsLW1hZWVFUaNGgVvb2+0bNkSANC5c2d4eXnhww8/xJw5c6BQKDBp0iSEh4c/N1tUkjcyuCEiIqKK5d69ewgODkZKSgqsra3RsGFD7NixA506dQIAzJ8/H1KpFIGBgVAqlejSpQuWLl2qfr+BgQHi4uIwYsQIeHt7w9zcHCEhIYiMjNR6LHq7z83rxPvckLZ4nxvSBu9zQ9oqr/vcOA3/Q6f93V3eW6f9lRdmboiIiESCz5ZSeaMmFBMRERG9KmZuiIiIRIKZGxUGN0RERCLB4EaFZSkiIiISFWZuiIiIxIKJGwDM3BAREZHIMHNDREQkEpxzo8LghoiISCQY3KiwLEVERESiwswNERGRSDBzo8LghoiISCQY3KiwLEVERESiwswNERGRWDBxA4DBDRERkWiwLKXCshQRERGJCjM3REREIsHMjQozN0RERCQqzNwQERGJBBM3KgxuiIiIRIJlKRWWpYiIiEhUmLkhIiISCSZuVBjcEBERiQTLUiosSxEREZGoMHNDREQkEkzcqDC4ISIiEgmplNENwLIUERERiQwzN0RERCLBspQKMzdEREQkKszcEBERiQSXgqswuCEiIhIJxjYqLEsRERGRqDBzQ0REJBIsS6kwuCEiIhIJBjcqLEsRERGRqDBzQ0REJBJM3Kgwc0NERESiwswNERGRSHDOjQqDGyIiIpFgbKPCshQRERGJCjM3REREIsGylAqDGyIiIpFgbKPCshQRERGJCjM3REREIsGylAqDGyIiIpFgbKPCshQRERG9sqioKDRv3hyWlpawt7dHr169kJSUpNGmffv2kEgkGtvw4cM12ty8eRMBAQEwMzODvb09xo8fj/z8fK3GwswNERGRSOizLLV//36Eh4ejefPmyM/Px8SJE9G5c2dcuHAB5ubm6nZDhw5FZGSk+rWZmZn6/wsKChAQEAC5XI7Dhw8jJSUFwcHBMDIywqxZs8o8FgY3RERE9Mq2b9+u8To2Nhb29vY4ceIE2rZtq95vZmYGuVxeYh87d+7EhQsXsHv3bjg4OKBx48aYPn06JkyYgKlTp8LY2LhMYxFlcNN0wAf6HgJVMPsup+l7CFSBtKhpq+8hUAVjYlE+v251nbhRKpVQKpUa+2QyGWQy2Qvfm5GRAQCwtdX887J69Wr88ssvkMvl6N69OyZPnqzO3iQkJKBBgwZwcHBQt+/SpQtGjBiB8+fPo0mTJmUaN+fcEBERicSz81ledYuKioK1tbXGFhUV9cJxFBYW4rPPPoOPjw/q16+v3t+/f3/88ssv2Lt3LyIiIrBq1SoMHDhQfVyhUGgENgDUrxUKRZk/B1FmboiIiOjVRUREYMyYMRr7ypK1CQ8Px7lz53Do0CGN/cOGDVP/f4MGDeDo6IiOHTvi6tWrcHNz082gweCGiIhINHRdliprCeppI0eORFxcHA4cOABnZ+fntm3RogUAIDk5GW5ubpDL5fjnn3802qSmpgJAqfN0SsKyFBERkUjouiylDUEQMHLkSGzcuBHx8fFwdXV94XsSExMBAI6OjgAAb29vnD17Fvfu3VO32bVrF6ysrODl5VXmsTBzQ0RERK8sPDwca9aswebNm2FpaameI2NtbQ1TU1NcvXoVa9asQdeuXVG1alWcOXMGo0ePRtu2bdGwYUMAQOfOneHl5YUPP/wQc+bMgUKhwKRJkxAeHq5VBonBDRERkUjo8w7Fy5YtA6C6Ud/TYmJiEBoaCmNjY+zevRsLFixAdnY2qlevjsDAQEyaNEnd1sDAAHFxcRgxYgS8vb1hbm6OkJAQjfvilAWDGyIiInplgiA893j16tWxf//+F/bj4uKCrVu3vtJYGNwQERGJBB+cqcLghoiISCQY3KhwtRQRERGJCjM3REREIsHEjQqDGyIiIpFgWUqFZSkiIiISFWZuiIiIRIKJGxUGN0RERCLBspQKy1JEREQkKszcEBERiQQTNyrM3BAREZGoMHNDREQkElKmbgAwuCEiIhINxjYqLEsRERGRqDBzQ0REJBJcCq7C4IaIiEgkpIxtALAsRURERCLDzA0REZFIsCylwuCGiIhIJBjbqLAsRURERKLCzA0REZFISMDUDcDMDREREYkMMzdEREQiwaXgKgxuiIiIRIKrpVRYliIiIiJRYeaGiIhIJJi4UWFwQ0REJBJSRjcAWJYiIiIikWHmhoiISCSYuFFh5oaIiIhEhZkbIiIikeBScBUGN0RERCLB2EaFZSkiIiISFWZuiIiIRIJLwVUY3BAREYkEQxsVlqWIiIhIVJi5ISIiEgmullJhcENERCQSUsY2AFiWIiIiIpFh5oaIiEgkWJZSYeaGiIiIRIWZGyIiIpFg4kaFwQ0REZFIsCylwrIUERERiQozN0RERCLBpeAqzNwQERGJhEQi0emmjaioKDRv3hyWlpawt7dHr169kJSUpNEmJycH4eHhqFq1KiwsLBAYGIjU1FSNNjdv3kRAQADMzMxgb2+P8ePHIz8/X6uxvFRwc/DgQQwcOBDe3t64c+cOAGDVqlU4dOjQy3RHREREFdz+/fsRHh6OI0eOYNeuXcjLy0Pnzp2RnZ2tbjN69Gj89ddfWLduHfbv34+7d++id+/e6uMFBQUICAhAbm4uDh8+jJUrVyI2NhZfffWVVmPROrjZsGEDunTpAlNTU5w6dQpKpRIAkJGRgVmzZmnbHREREemIRMebNrZv347Q0FDUq1cPjRo1QmxsLG7evIkTJ04AUMUJ0dHRmDdvHt599100bdoUMTExOHz4MI4cOQIA2LlzJy5cuIBffvkFjRs3hr+/P6ZPn44lS5YgNze3zGPROriZMWMGli9fjh9//BFGRkbq/T4+Pjh58qS23ZWqoKAAd+/e1Vl/REREYieVSHS6KZVKZGZmamxFSY0XycjIAADY2toCAE6cOIG8vDz4+vqq29StWxc1atRAQkICACAhIQENGjSAg4ODuk2XLl2QmZmJ8+fPl/1zKHPL/5eUlIS2bdsW229tbY309HRtuyvVuXPnUL16dZ31R0RERNqJioqCtbW1xhYVFfXC9xUWFuKzzz6Dj48P6tevDwBQKBQwNjaGjY2NRlsHBwcoFAp1m6cDm6LjRcfKSuvVUnK5HMnJyahZs6bG/kOHDqFWrVradkdEREQ6ouvb3ERERGDMmDEa+2Qy2QvfFx4ejnPnzultLq7WmZuhQ4fi008/xdGjRyGRSHD37l2sXr0a48aNw4gRI17HGImIiEgPZDIZrKysNLYXBTcjR45EXFwc9u7dC2dnZ/V+uVyO3NzcYlWe1NRUyOVydZtnV08VvS5qUxZaZ26++OILFBYWomPHjnj8+DHatm0LmUyGcePGYdSoUdp2R0RERDqizzsUC4KAUaNGYePGjdi3bx9cXV01jjdt2hRGRkbYs2cPAgMDAaimuty8eRPe3t4AAG9vb8ycORP37t2Dvb09AGDXrl2wsrKCl5dXmceidXAjkUjw5ZdfYvz48UhOTkZWVha8vLxgYWGhVT9nzpx57vFn18YTERHR8+nz6Qvh4eFYs2YNNm/eDEtLS/UcGWtra5iamsLa2hphYWEYM2YMbG1tYWVlhVGjRsHb2xstW7YEAHTu3BleXl748MMPMWfOHCgUCkyaNAnh4eFlKocVeek7FBsbG2sVRT2rcePGkEgkEASh2LGi/XxGxvMNfMcZ7erYwcXWFMr8Qpy9k4llB27g1n9PSmw/N7AeWrraImLTBRxMfqDef2hcm2Jtp/x1CXuS0l7b2Ek/rp5PxN7Na3H7WhIy/3uAQZ/PRIMWqgUCBfn52Lr2R1w8eQQPU+/CxMwcdRo2Q8DA4bC2tQMAPLyXgp3rViL53Elkpj+AdRU7NG3bGb6BwTB8avUkidfGdb9i4/rfkJKiuseZay13DBo6At4+qr9H5sycimNHj+D+/XswMzVD/UaN8fGoMXBx5ZxMsVu2bBkAoH379hr7Y2JiEBoaCgCYP38+pFIpAgMDoVQq0aVLFyxdulTd1sDAAHFxcRgxYgS8vb1hbm6OkJAQREZGajUWrYObDh06PDfoiI+PL1M/169f1/bU9Iwm1a3xx6m7uKTIgoFUgmFtamL++/UxMOYEcvIKNdr2beqEEuJItZnbknD0+n/q11lK7e4GSRVDrjIHTjXd8U7HAMTO+bLYsTvXLqNznxA41XTH4+xH2LRiIaJnf4Exc34CAKTeuQlBKMT7H42DndwZKbeu4fdlc5CrzEGPkHB9XBKVs2oODhg+ajSq13CBIAjYFrcZX4wZiZg1G1DLzR0enl7o7N8NDnJHZGZkIPqHJRgdPhTr/toJAwMDfQ9f9KR6Lku9iImJCZYsWYIlS5aU2sbFxQVbt259pbFoHdw0btxY43VeXh4SExNx7tw5hISElLkfFxcXbU9Nzxi7QXPN/6xtlxEX3hIeDhY4fTtTvd+9mjmCmjljyKpT+PPjliX2laUswMPHea91vKR/nm+3hOfbJX8HTM0tMHzKfI19vYeMxoIJw/BfWiqqVHOAZ5MW8GzSQn28qtwJ9+7cxOEdmxjcVBKt23bQeP1R+KfYuP5XnD97GrXc3NGzd1/1MUentzDs408QEtQbKXfvwLl6jfIebqXDgoeK1sHN/PnzS9w/depUZGVllbmf4OBgLFmyBJaWlgCA06dPw8vLS+PGgKQdc5nqX0WZOf/LusgMpZjSrS7m7U5+bvAypqMbJnSujbsZOdh8OgVbzqWW2pYqj5zsbEgkEpialz6nLudxNswsrcpxVPSmKCgowN7dO5Dz5AnqN2xU7PiTJ4+x5c+NcHrLGQ5arHQhelU6eyr4wIED8c4772Du3Lllar969WrMnTtXHdy0adMGiYmJvFfOS5IA+KRDLZy5nYHr9x+r93/SoRbO3cnEoasPS33vj4du4OTNDOTkF+CdmlUwxtcdpkYGWH+Kd4iuzPJylYj7ZRmatPaFiZl5iW3SUm7j0LYN6B78cTmPjvTp6pXL+GhQf+Tm5sLU1Ayz5i6Cay139fE/fl+LpYu+xZMnT1DDxRXzl/wIIyNjPY648uBcVRWdBTcJCQkwMTEpc/tna3NlqdWVRKlUFrsVdGF+LqSGlesP0hhfd9SyM8fHa0+r9/m42eLtGjYY/PPzH4ux8sgt9f9fuZcNEyMD9GvuzOCmEivIz8fP306BIAjoM2xsiW3SH6Thhxnj0Mi7Pbw79SjnEZI+1ahZE7FrNyArKwt7d+/EzCkT8d2PseoAp7N/NzRv2QoP7qdhzaoYfPXFWCxb8YtWq12IXoXWwc3TT+8EVEFJSkoKjh8/jsmTJ+tsYGUVFRWFadOmaeyr3ikUNToPLvex6Mvojm5oVcsWI387jbSs/z1YrGkNG7xlY4Jto1pptJ/RwxNn7mRg1G9nS+zvQkomBnnXgJGBBHkFLxd0UsVVkJ+Pld9+hYdpCnw8bWGJWZuMh/exbMoncPWoj/eHf66HUZI+GRkZw7m6at5kXc96uHThHNat/QWffzkVAGBhaQkLS0tUr+GCeg0awq99KxzYuxud/AL0OOrKQes784qU1sGNtbW1xmupVAoPDw9ERkaic+fOWvV14cIF9Tp4QRBw6dKlYvN2GjZs+Nw+Sro1tN/SY1qNoyIb3dENbd2rYtRvZ5CSoZnB+uXoLfx1VvNZHKtCm2Lx3mv4+9oDlKZ2NQtkPsljYFMJFQU291Nu4+NpC2FuaV2sTfqDNCyb8gmca3kgKDwCUin/Oq3sCgsLS31isyCo/n7X5onO9PJYllLRKrgpKCjAoEGD0KBBA1SpUuWVT/7uu+9qvO7WrRsAzfvcFBQUPLcPmUxWLNVZWUpSY33d4FvXHhGbLuBxbgFszVSTsbNyC5CbX4iHj/NKnESc+kipDoR8atmiirkRzqc8Qm5+IZq7VMGHLatj7bHb5XotVD6UTx7jvuKO+vXDeym4c/0KzCysYFWlKmLnTsada5cRNvFrFBYWIvM/VRBsZmEFQyMjpD9Iw9KvPkGVag7oHhKOrMx0dV9WVaqW9+WQHixbPB/ePm3gIHfE4+xs7Ny+BadOHMO8737Andu3sGfndrzj3Qo2NlWQdi8Vq2J/gsxEhlatiz9wmeh10Sq4MTAwQOfOnXHx4sVXDm5Onz4NKyuusHgV7zV2AgB8F6SZ3Zq5LQnbzt8rUx/5hQJ6N3bCJx1MAEhwJ/0Jvtt7DX+eKfvTV6niuHU1CUunfKJ+vTn2OwBA8/Z+6PLBYJw/pnrI3bdjB2m87+Npi+Bevwkunz6G+4rbuK+4jchhmiXqeRsOvubR05sg/b+HmP5VBB7cT4O5hSXca9fBvO9+wDstWyEt7R5OJ57A72tX4VFmBmyr2qFRk6ZYvmI1qtgy+C0PUiZuAAASQcuZvM2aNcPXX3+Njh07vtKJpVIp3nnnHYSFhSEoKEi9akoXWs/lX7KknQg/D30PgSqQFjVt9T0EqmDsLHS2fue5xvx5Saf9zetRV6f9lReti+UzZszAuHHjEBcXh5SUFGRmZmpsZbV//354eXlh7NixcHR0REhICA4eZFBCREREr6bMwU1kZCSys7PRtWtXnD59Gj169ICzszOqVKmCKlWqwMbGRqtSVZs2bbBixQqkpKRg8eLFuHHjBtq1a4c6derg66+/Vk80JiIiorKRSCQ63SqqMpelDAwMkJKSgosXLz63Xbt27V56MMnJyYiJicGqVaugUCjg5+eHP//8U+t+WJYibbEsRdpgWYq0VV5lqfFxSTrt75tuFfPvxjJ/2kUx0KsELy/i7u6OiRMnwsXFBREREdiyZctrOxcRERGJk1ah5OtMUR04cAArVqzAhg0bIJVK0bdvX4SFhb228xEREYlNBa4k6ZRWwU2dOnVeGOA8fFj6M4yedffuXcTGxiI2NhbJyclo1aoVFi1ahL59+8LcvORn2RARERE9j1bBzbRp04rdofhl+fv7Y/fu3bCzs0NwcDAGDx4MD4+KWdsjIiJ6E0iZugGgZXATFBQEe3t7nZzYyMgI69evR7du3WBgYKCTPomIiCozPgxFpczBja7n27zMKigiIiKiF9F6tRQRERG9mViVUilzcFNYWPg6x0FERESviHNuVFieIyIiIlEpn1smEhER0WvHxI0KgxsiIiKRkDK4AcCyFBEREYkMMzdEREQiwQnFKszcEBERkagwc0NERCQSTNyoMLghIiISCU4oVmFZioiIiESFmRsiIiKRkICpG4DBDRERkWiwLKXCshQRERGJCjM3REREIsHMjQozN0RERCQqzNwQERGJhIQ3ugHA4IaIiEg0WJZSYVmKiIiIRIWZGyIiIpFgVUqFwQ0REZFI8KngKixLERERkagwc0NERCQSnFCswuCGiIhIJFiVUmFZioiIiESFmRsiIiKRkPKp4ACYuSEiIiKRYXBDREQkEhKJbjdtHDhwAN27d4eTkxMkEgk2bdqkcTw0NBQSiURj8/Pz02jz8OFDDBgwAFZWVrCxsUFYWBiysrK0/hwY3BAREYmEVKLbTRvZ2dlo1KgRlixZUmobPz8/pKSkqLe1a9dqHB8wYADOnz+PXbt2IS4uDgcOHMCwYcO0/hw454aIiIhemb+/P/z9/Z/bRiaTQS6Xl3js4sWL2L59O44dO4ZmzZoBABYvXoyuXbti7ty5cHJyKvNYmLkhIiISCalEotNN1/bt2wd7e3t4eHhgxIgRePDggfpYQkICbGxs1IENAPj6+kIqleLo0aNanYeZGyIiIpHQdTyiVCqhVCo19slkMshkMq378vPzQ+/eveHq6oqrV69i4sSJ8Pf3R0JCAgwMDKBQKGBvb6/xHkNDQ9ja2kKhUGh1LmZuiIiIqERRUVGwtrbW2KKiol6qr6CgIPTo0QMNGjRAr169EBcXh2PHjmHfvn26HTSYuSEiIhINXZeSIiIiMGbMGI19L5O1KUmtWrVgZ2eH5ORkdOzYEXK5HPfu3dNok5+fj4cPH5Y6T6c0DG6IiIhEQtdlqZctQZXF7du38eDBAzg6OgIAvL29kZ6ejhMnTqBp06YAgPj4eBQWFqJFixZa9c3ghoiIiF5ZVlYWkpOT1a+vX7+OxMRE2NrawtbWFtOmTUNgYCDkcjmuXr2Kzz//HO7u7ujSpQsAwNPTE35+fhg6dCiWL1+OvLw8jBw5EkFBQVqtlAI454aIiEg0pDretHH8+HE0adIETZo0AQCMGTMGTZo0wVdffQUDAwOcOXMGPXr0QJ06dRAWFoamTZvi4MGDGpmh1atXo27duujYsSO6du2K1q1b44cfftD6c2DmhoiIiF5Z+/btIQhCqcd37Njxwj5sbW2xZs2aVx4LgxsiIiKRkLyGe9NURAxuiIiIRIKhjQrn3BAREZGoMHNDREQkEq/jkQkVEYMbIiIikWBoo8KyFBEREYkKMzdEREQiwaqUCjM3REREJCrM3BAREYkE73OjwuCGiIhIJFiOUeHnQERERKLCzA0REZFIsCylwuCGiIhIJBjaqLAsRURERKLCzA0REZFIsCylIsrgJjs7T99DoArGS26l7yFQBXLn4RN9D4EqGDsLy3I5D8sxKvwciIiISFREmbkhIiKqjFiWUmHmhoiIiESFmRsiIiKRYN5GhcENERGRSLAqpcKyFBEREYkKMzdEREQiIWVhCgCDGyIiItFgWUqFZSkiIiISFWZuiIiIRELCshQAZm6IiIhIZJi5ISIiEgnOuVFhcENERCQSXC2lwrIUERERiQozN0RERCLBspQKgxsiIiKRYHCjwrIUERERiQozN0RERCLB+9yoMLghIiISCSljGwAsSxEREZHIMHNDREQkEixLqTBzQ0RERKLCzA0REZFIcCm4CoMbIiIikWBZSoVlKSIiIhIVZm6IiIhEgkvBVRjcEBERiQTLUiosSxEREZGoMHNDREQkElwtpcLMDRERkUhIdLxp48CBA+jevTucnJwgkUiwadMmjeOCIOCrr76Co6MjTE1N4evriytXrmi0efjwIQYMGAArKyvY2NggLCwMWVlZWo6EwQ0RERHpQHZ2Nho1aoQlS5aUeHzOnDlYtGgRli9fjqNHj8Lc3BxdunRBTk6Ous2AAQNw/vx57Nq1C3FxcThw4ACGDRum9VhYliIiIhIJqR7rUv7+/vD39y/xmCAIWLBgASZNmoSePXsCAH7++Wc4ODhg06ZNCAoKwsWLF7F9+3YcO3YMzZo1AwAsXrwYXbt2xdy5c+Hk5FTmsTBzQ0RERK/V9evXoVAo4Ovrq95nbW2NFi1aICEhAQCQkJAAGxsbdWADAL6+vpBKpTh69KhW52PmhoiISCR0nbdRKpVQKpUa+2QyGWQymVb9KBQKAICDg4PGfgcHB/UxhUIBe3t7jeOGhoawtbVVtykrZm6IiIjEQscziqOiomBtba2xRUVFlfNFaY+ZGyIiIipRREQExowZo7FP26wNAMjlcgBAamoqHB0d1ftTU1PRuHFjdZt79+5pvC8/Px8PHz5Uv7+smLkhIiISCYmO/5PJZLCystLYXia4cXV1hVwux549e9T7MjMzcfToUXh7ewMAvL29kZ6ejhMnTqjbxMfHo7CwEC1atNDqfMzcEBERiYQ+b+KXlZWF5ORk9evr168jMTERtra2qFGjBj777DPMmDEDtWvXhqurKyZPngwnJyf06tULAODp6Qk/Pz8MHToUy5cvR15eHkaOHImgoCCtVkoBDG6IiIhIB44fP44OHTqoXxeVs0JCQhAbG4vPP/8c2dnZGDZsGNLT09G6dWts374dJiYm6vesXr0aI0eORMeOHSGVShEYGIhFixZpPRaJIAjCq1/Sm6XJtHh9D4EqmD/CW+l7CFSBZD7O0/cQqIJpVMOyXM5z7FqGTvtrXstap/2VF865ISIiIlFhWYqIiEgs+OBMAAxuiIiIREPC6AYAy1JEREQkMszcEBERiYQ+l4K/SRjcEBERiQRjGxWWpYiIiEhUmLkhIiISC6ZuADC4ISIiEg2ullJhWYqIiIhEhZkbIiIikeBqKRVmboiIiEhUmLkhIiISCSZuVBjcEBERiQWjGwAsSxEREZHIMHNDREQkElwKrsLghoiISCS4WkqFZSkiIiISFWZuiIiIRIKJGxW9ZW4MDAxw7949fZ2eiIhIfCQ63ioovWVuBEHQ16lFY3BrF7xbtxpq2plBmV+I07cysHD3Vfz74LFGu4bOVgh/1w0N3rJCgSDgsiILH/+SCGV+IQDAysQQE/zroK2HHQRBwJ6LaZiz7Qqe5BXo47KoHK2KXobVK5Zr7HOuURM/rd0MRcodhPbpWuL7Jk7/Bm3f7VweQ6Q3TGFBAX5f9QMO7tmG9IcPYFvVDu06d0fggDBI/n/Cx+8/f4/D+3biQVoqDA2NUKu2J4IGfYzanvX1PHqqLFiWqsDedrHBb8du4/zdRzCUSjDy3VpYNrAxei89gpw8VeDS0NkK3w1ojJhD/+LrbZdRUCigjoMFCp8KLmf1rgc7S2OMWJUIQ6kE03p6YnJ3D0z844K+Lo3KkYurG6IW/qB+bWBgAACoZi/Hmj/3aLTdtnk91q9ZieYtW5frGOnNsem3ldj113qEfz4Nzi61cO3yBSydGwkzcwt0fS8IAODk7ILBIz+Hg+NbyFUqsWXDGsz4IhyLV26ClU0VPV+BuHG1lIpeg5uffvoJFhYWz23zySeflNNoKp6Rq09rvJ6y+SLix7eBl6MVTt5MBwCM7VIbv/5zCzF//6tu93Rmx9XODD61q2LAD8dwIeURAODrbZexeEAjzN+ZjLSs3Nd/IaRXBgaGsK1qV8J+g2L7Dx+IR5uOnWFqZlZew6M3zOULZ9CsVTu83UIV4NrLnXBo7w4kJ51Xt2n9rp/Ge4KHj0b89s3499oVNHj7nXIdL1VOeg1uli9frv5XYkkkEgmDGy1YyFQ/zowneQCAKmZGaOhsjW1nUxE7uCmcq5jixv1sfBd/DYm3MgAADZ2tkfkkTx3YAMDRa/+hUBBQ39kKey/dL/8LoXJ15/a/6N/DF8YyY3jWa4RBwz+BvdyxWLsrly7g6pUkhI+dqIdR0puijldD7Nm6EXdv/wsnZxfcuHoZSedOI3j46BLb5+flYffWjTAzt4CLW51yHm3lw6XgKnoNbo4fPw57e3t9DkE0JADG+dXGqZvpuJqWDQBwrmIKAPionSvm70pGkuIRujWS4/vgJnh/2VHcfPgEVS2M8TBbMztTIAjIfJIPOwtZeV8GlbO6Xg0w9svpcK5REw8fpGH1iu8x7uNBWL5qA8zMzTXa7ojbiBo1a8GrQWP9DJbeCL2CQvHkcTZGD+4DqVSKwsJCBA36GG06+mu0O3HkIBbMnIhcZQ5sbO0w6eslsLK20c+gKxHGNioVfs6NUqmEUqnU2FeYnwupobGeRqQfEQF14G5vjkErTqr3Sf//W77hxB38mZgCAEhSJOMdV1v0bOKIxXuu6WOo9AZp7v2/uTO13OugrlcDBAf640D8Dvh1760+plTmYO+ubegfOlQfw6Q3SML+XTgUvx2fRMxA9ZpuuJGchNhl81ClajW079xN3a5eo2b4ZvkaZGakY8+2jZg/IwKzFsXCuoqtHkdPlYXeloJLJBL1zPpXERUVBWtra40t9eBaHYyw4pjgXwdtatth6MpTuPfof4Fe0XyZa/+fySlyPS0bcisTAMCDrFzYmmsGggYSCaxMDXE/SzNoJPGzsLTCW9VdcPf2LY39B/fugjLnCTr6ddfTyOhN8cuPi9DzgxD4dOiCGq7uaNspAAGB/bDp1xiNdiamppC/VR11vBpgxNivYCA1QPz2zXoadSXCpeAA9BjcCIIAPz8//Pjjj3j06NGL31CKiIgIZGRkaGwObfrpcKRvtgn+dfBu3Wr46OdTuJueo3HsbnoO7mUqUdNOc/KnS1UzpGSo2p65nQErUyN4Olqqjzd3rQKpRIJztzNf/wXQG+XJ48dIuXMLtnaaE4l3xG1Cy9btYcN/dVd6ypwcSKWavzqkUgMIhc+/vYcgFCIvjwsUXjeJjv+rqPQW3Bw4cACNGjXC2LFj4ejoiJCQEBw8eFDrfmQyGaysrDS2ylKSiuhaBwENHTDxj/PIVhagqrkxqpobQ2b4vx/rysP/Iuid6vD1rIbqVUzxcQdX1LQzw6ZTqjLV9fuP8feVB5jcvS7qOVmiUXVrfNG1DnacS+VKqUrgx+++xZlTx6FIuYMLZxMRGTEaBgYGaO/7v/kTd2/fxLnEExplKqq8mrZsgz/WrMDJo4dwT3EX/xzai7gNq9Hcpz0AIOfJE6yJXoLLF84iLTUF1y5fxNK50/Dwfhq82/rqd/BUaUgEPd9NLzs7G7///jtiY2Nx8OBBuLu7IywsDCEhIZDL5S/VZ5Np8Toe5Zvp1JR3S9z/1aYL+Ou0Qv16kI8L+jZ/C9amRricmoUFu5LVq6UA1U38vuhaB23r2KFQAPZcvFfpbuL3R3grfQ9BL6K++hxnE0/iUWY6rG2qoF7DJggZNgpOztXVbWKWL0L8zi1YuX5bsX+xV1aZj/P0PQS9efI4G7/FLsc/f+9FRvp/sK1qB58OXdBn4FAYGhkhN1eJRbMm4cqlc3iUmQ5LS2u4eXih94AwuHvU0/fw9aZRDcsXN9KBJMXjFzfSgoe8Yt72Qe/BzdOSk5MRExODVatWQaFQwM/PD3/++afW/VSW4IZ0p7IGN/RyKnNwQy+HwU35eqP+Gebu7o6JEydi0qRJsLS0xJYtW/Q9JCIiogqD84lV3pil4AcOHMCKFSuwYcMGSKVS9O3bF2FhYfoeFhERUcVRkSMSHdJrcHP37l3ExsYiNjYWycnJaNWqFRYtWoS+ffvC/JkbiBERERGVhd6CG39/f+zevRt2dnYIDg7G4MGD4eHhoa/hEBERVXgVefm2LuktuDEyMsL69evRrVu35z5fioiIiMqGz5ZS0Vtw8zKroIiIiIhe5I2ZUExERESvhokbFQY3REREYsHoBsAbdp8bIiIiolfFzA0REZFIcLWUCjM3REREJCrM3BAREYkEl4KrMLghIiISCcY2KixLERERkagwuCEiIhILPT4WfOrUqZBIJBpb3bp11cdzcnIQHh6OqlWrwsLCAoGBgUhNTX2Vqy0VgxsiIiKRkOj4P23Vq1cPKSkp6u3QoUPqY6NHj8Zff/2FdevWYf/+/bh79y569+6ty8tX45wbIiIi0glDQ0PI5fJi+zMyMhAdHY01a9bg3XffBQDExMTA09MTR44cQcuWLXU6DmZuiIiIREIi0e2mrStXrsDJyQm1atXCgAEDcPPmTQDAiRMnkJeXB19fX3XbunXrokaNGkhISNDV5asxc0NERCQSul4tpVQqoVQqNfbJZDLIZLJibVu0aIHY2Fh4eHggJSUF06ZNQ5s2bXDu3DkoFAoYGxvDxsZG4z0ODg5QKBQ6HjUzN0RERFSKqKgoWFtba2xRUVEltvX398f777+Phg0bokuXLti6dSvS09Px+++/l/OombkhIiISDV3fxC8iIgJjxozR2FdS1qYkNjY2qFOnDpKTk9GpUyfk5uYiPT1dI3uTmppa4hydV8XMDREREZVIJpPByspKYytrcJOVlYWrV6/C0dERTZs2hZGREfbs2aM+npSUhJs3b8Lb21vn42bmhoiISDT0d4/icePGoXv37nBxccHdu3cxZcoUGBgYoF+/frC2tkZYWBjGjBkDW1tbWFlZYdSoUfD29tb5SimAwQ0REZFo6PPZUrdv30a/fv3w4MEDVKtWDa1bt8aRI0dQrVo1AMD8+fMhlUoRGBgIpVKJLl26YOnSpa9lLBJBEITX0rMeNZkWr+8hUAXzR3grfQ+BKpDMx3n6HgJVMI1qWJbLee6k5+q0v7dsjHXaX3lh5oaIiEgk+OBMFQY3REREIqHPstSbhKuliIiISFSYuSEiIhKJl3nYpRgxc0NERESiwswNERGRWDBxA4DBDRERkWgwtlFhWYqIiIhEhZkbIiIikeBScBUGN0RERCLB1VIqLEsRERGRqDBzQ0REJBZM3ABgcENERCQajG1UWJYiIiIiUWHmhoiISCS4WkqFmRsiIiISFWZuiIiIRIJLwVUY3BAREYkEy1IqLEsRERGRqDC4ISIiIlFhWYqIiEgkWJZSYeaGiIiIRIWZGyIiIpHgaikVZm6IiIhIVJi5ISIiEgnOuVFhcENERCQSjG1UWJYiIiIiUWHmhoiISCyYugHA4IaIiEg0uFpKhWUpIiIiEhVmboiIiESCq6VUGNwQERGJBGMbFZaliIiISFSYuSEiIhILpm4AMHNDREREIsPMDRERkUhwKbgKgxsiIiKR4GopFZaliIiISFQkgiAI+h4ElQ+lUomoqChERERAJpPpezj0huP3hbTB7wu9SRjcVCKZmZmwtrZGRkYGrKys9D0cesPx+0La4PeF3iQsSxEREZGoMLghIiIiUWFwQ0RERKLC4KYSkclkmDJlCif7UZnw+0La4PeF3iScUExERESiwswNERERiQqDGyIiIhIVBjdEREQkKgxuKrCEhAQYGBggICBAY/+NGzcgkUiKbQMHDtQ4npiYWGJ7Y2NjuLu7Y8aMGeCULPHo3r07/Pz8Sjx28OBBSCQSnDlzRqvvDlUuZfkOvWgjKg98cGYFFh0djVGjRiE6Ohp3796Fk5OTxvHdu3ejXr166tempqbP7a+ovVKpxKFDhzBkyBA4OjoiLCzstYyfyldYWBgCAwNx+/ZtODs7axyLiYlBs2bN1HeW1fa7Q5XDi75DjRs3xrZt29T7mjdvjmHDhmHo0KHlPVSq5Ji5qaCysrLw22+/YcSIEQgICEBsbGyxNlWrVoVcLldv1tbWz+2zqL2LiwsGDBgAHx8fnDx58jVdAZW3bt26oVq1asW+K1lZWVi3bp1GEKvtd4cqhxd9hz766CON742BgQEsLS019hGVBwY3FdTvv/+OunXrwsPDAwMHDsSKFSt0WkI6fvw4Tpw4gRYtWuisT9IvQ0NDBAcHIzY2VuO7sm7dOhQUFKBfv356HB1VBPwOUUXB4KaCio6OVs+D8PPzQ0ZGBvbv36/RplWrVrCwsFBvp06dem6fRe2NjY3RvHlz9O3bF8HBwa/tGqj8DR48GFevXtX4rsTExCAwMFAjO6Ptd4cqj7J+h4j0iXNuKqCkpCT8888/2LhxIwDVv6Y++OADREdHo3379up2v/32Gzw9PdWvq1ev/tx+i9rn5eXh3LlzGDVqFKpUqYLZs2e/luug8le3bl20atUKK1asQPv27ZGcnIyDBw8iMjJSo5223x2qPMr6HSLSJwY3FVB0dDTy8/M1JhALggCZTIbvvvtOva969epwd3cvc79Pt/f09MTVq1cxefJkTJ06FSYmJrq7ANKrsLAwjBo1CkuWLEFMTAzc3NzQrl07jTbafneocinLd4hIn1iWqmDy8/Px888/49tvv0ViYqJ6O336NJycnLB27VqdncvAwAD5+fnIzc3VWZ+kf3379oVUKsWaNWvw888/Y/DgwVyiS1rhd4jedMzcVDBxcXH477//EBYWVqy+HRgYiOjo6FLvQ/EiDx48gEKhQH5+Ps6ePYuFCxeiQ4cO6uXBJA4WFhb44IMPEBERgczMTISGhmrdR1JSUrF99erVg5GRkQ5GSG86XXyHiF4nBjcVTHR0NHx9fUucuBcYGIg5c+YgMzPzpfr29fUFoMrYODo6omvXrpg5c+YrjZfeTGFhYYiOjkbXrl2L3R+pLIKCgortu3XrVrF7n5B4vep3iOh14lPBiYiISFQ454aIiIhEhcENERERiQqDGyIiIhIVBjdEREQkKgxuiIiISFQY3BAREZGoMLghIiIiUWFwQ0RERKLC4IaIAAChoaHo1auX+nX79u3x2Weflfs49u3bB4lEgvT09HI/NxGJA4MbojdcaGgoJBIJJBIJjI2N4e7ujsjISOTn57/W8/7xxx+YPn16mdoyICGiNwmfLUVUAfj5+SEmJgZKpRJbt25FeHg4jIyMEBERodEuNzcXxsbGOjmnra2tTvohIipvzNwQVQAymQxyuRwuLi4YMWIEfH198eeff6pLSTNnzoSTkxM8PDwAqB5i2bdvX9jY2MDW1hY9e/bEjRs31P0VFBRgzJgxsLGxQdWqVfH555/j2cfMPVuWUiqVmDBhAqpXrw6ZTAZ3d3dER0fjxo0b6NChAwCgSpUqkEgk6qdEFxYWIioqCq6urjA1NUWjRo2wfv16jfNs3boVderUgampKTp06KAxTiKil8HghqgCMjU1RW5uLgBgz549SEpKwq5duxAXF4e8vDx06dIFlpaWOHjwIP7++29YWFjAz89P/Z5vv/0WsbGxWLFiBQ4dOoSHDx9i48aNzz1ncHAw1q5di0WLFuHixYv4/vvvYWFhgerVq2PDhg0AgKSkJKSkpGDhwoUAgKioKPz8889Yvnw5zp8/j9GjR2PgwIHYv38/AFUQ1rt3b3Tv3h2JiYkYMmQIvvjii9f1sRFRZSEQ0RstJCRE6NmzpyAIglBYWCjs2rVLkMlkwrhx44SQkBDBwcFBUCqV6varVq0SPDw8hMLCQvU+pVIpmJqaCjt27BAEQRAcHR2FOXPmqI/n5eUJzs7O6vMIgiC0a9dO+PTTTwVBEISkpCQBgLBr164Sx7h3714BgPDff/+p9+Xk5AhmZmbC4cOHNdqGhYUJ/fr1EwRBECIiIgQvLy+N4xMmTCjWFxGRNjjnhqgCiIuLg4WFBfLy8lBYWIj+/ftj6tSpCA8PR4MGDTTm2Zw+fRrJycmwtLTU6CMnJwdXr15FRkYGUlJS0KJFC/UxQ0NDNGvWrFhpqkhiYiIMDAzQrl27Mo85OTkZjx8/RqdOnTT25+bmokmTJgCAixcvaowDALy9vct8DiKikjC4IaoAOnTogGXLlsHY2BhOTk4wNPzfH11zc3ONtllZWWjatClWr15drJ9q1aq91PlNTU21fk9WVhYAYMuWLXjrrbc0jslkspcaBxFRWTC4IaoAzM3N4e7uXqa2b7/9Nn777TfY29vDysqqxDaOjo44evQo2rZtCwDIz8/HiRMn8Pbbb5fYvkGDBigsLMT+/fvh6+tb7HhR5qigoEC9z8vLCzKZDDdv3iw14+Pp6Yk///xTY9+RI0defJFERM/BCcVEIjNgwADY2dmhZ8+eOHjwIK5fv459+/bhk08+we3btwEAn376KWbPno1Nmzbh0qVL+Pjjj597j5qaNWsiJCQEgwcPxqZNm9R9/v777wAAFxcXSCQSxMXFIS0tDVlZWbC0tMS4ceMwevRorFy5ElevXsXJkyexePFirFy5EgAwfPhwXLlyBePHj0dSUhLWrFmD2NjY1/0REZHIMbghEhkzMzMcOHAANWrUQO/eveHp6YmwsDDk5OSoMzljx47Fhx9+iJCQEHh7e8PS0hLvvffec/tdtmwZ+vTpg48//hh169bF0KFDkZ2dDQB46623MG3aNHzxxRdwcHDAyJEjAQDTp0/H5MmTERUVBU9PT/j5+WHLli1wdXUFANSoUQMbNmzApk2b0KhRIyxfvhyzZs16jZ8OEVUGEqG0GYREREREFRAzN0RERCQqDG6IiIhIVBjcEBERkagwuCEiIiJRYXBDREREosLghoiIiESFwQ0RERGJCoMbIiIiEhUGN0RERCQqDG6IiIhIVBjcEBERkagwuCEiIiJR+T8QK8JDxYa+rgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "TEST_DIR = \"/workspace/SPLIT_SLIDING_FINAL/test\"\n",
    "MODEL_DIR = \"/workspace/HASIL_BERT_KFOLD/HASIL_11/fold4/model\"\n",
    "LABEL_MAP = {'AFIB': 0, 'VFL': 1, 'VT': 2}\n",
    "IDX2LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === LOAD TOKENIZER & MODEL ===\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === PREPROSES SINYAL KE TEKS ===\n",
    "def signal_to_text(sig: np.ndarray) -> str:\n",
    "    sig = (sig - sig.min()) / (sig.ptp() + 1e-8)\n",
    "    sig = (sig * 255).astype(int)\n",
    "    return \" \".join(map(str, sig.tolist()))\n",
    "\n",
    "# === LOAD DATA ===\n",
    "texts, labels = [], []\n",
    "for label in os.listdir(TEST_DIR):\n",
    "    if label not in LABEL_MAP: continue\n",
    "    folder = os.path.join(TEST_DIR, label)\n",
    "    for file in os.listdir(folder):\n",
    "        if file.endswith(\".npy\"):\n",
    "            sig = np.load(os.path.join(folder, file), allow_pickle=True)\n",
    "            if isinstance(sig, np.ndarray) and sig.ndim == 1:\n",
    "                texts.append(signal_to_text(sig))\n",
    "                labels.append(LABEL_MAP[label])\n",
    "labels = np.array(labels)\n",
    "\n",
    "# === TOKENISASI (tanpa tensors dulu)\n",
    "encodings = tokenizer(texts, padding='max_length', truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "# === Dataset & DataLoader ===\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "    def __len__(self): return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "test_dataset = ECGDataset(encodings, labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# === INFERENSI ===\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "        labels_batch = batch['labels'].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = outputs.logits.argmax(dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "# === EVALUASI ===\n",
    "acc = accuracy_score(all_labels, all_preds)\n",
    "rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "f1s = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "print(f\"[BERT TEST] Akurasi: {acc:.4f} | Recall: {rec:.4f} | F1-score: {f1s:.4f}\")\n",
    "\n",
    "# === VISUAL CONFUSION MATRIX ===\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\", xticklabels=LABEL_MAP.keys(), yticklabels=LABEL_MAP.keys())\n",
    "plt.title(\"Confusion Matrix - BERT Test\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6df6e460-d7c2-4541-b7cf-58362dcab71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluasi Model BERT Base (Best Model) ===\n",
      "Akurasi     : 0.9650\n",
      "Recall      : 0.9650\n",
      "F1-score    : 0.9650\n",
      "Spesifisitas: 0.9825\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "MODEL_PATH = \"/workspace/HASIL_BERT_KFOLD/HASIL_13/bert_best\"\n",
    "TEST_DIR   = \"/workspace/SPLIT_SLIDING_FINAL/test\"\n",
    "LABEL_MAP  = {'AFIB': 0, 'VFL': 1, 'VT': 2}\n",
    "IDX2LABEL  = {v: k for k, v in LABEL_MAP.items()}\n",
    "MAX_LEN    = 512\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === LOAD TOKENIZER DAN MODEL ===\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === UTILS ===\n",
    "def signal_to_text(sig, target_len=512):\n",
    "    if len(sig) < target_len:\n",
    "        pad = np.full(target_len - len(sig), sig[-1])\n",
    "        sig = np.concatenate([sig, pad])\n",
    "    else:\n",
    "        idx = np.linspace(0, len(sig) - 1, target_len).astype(int)\n",
    "        sig = sig[idx]\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm))\n",
    "\n",
    "def load_test_data(test_dir):\n",
    "    data, labels = [], []\n",
    "    for label_name in os.listdir(test_dir):\n",
    "        folder_path = os.path.join(test_dir, label_name)\n",
    "        if label_name not in LABEL_MAP:\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                sig = np.load(os.path.join(folder_path, file), allow_pickle=True)\n",
    "                if isinstance(sig, np.ndarray) and sig.ndim == 1:\n",
    "                    data.append(signal_to_text(sig))\n",
    "                    labels.append(LABEL_MAP[label_name])\n",
    "    return data, np.array(labels)\n",
    "\n",
    "# === LOAD DATA TEST ===\n",
    "texts, y_true = load_test_data(TEST_DIR)\n",
    "\n",
    "# === INFERENSI ===\n",
    "preds = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "# === EVALUASI ===\n",
    "acc  = accuracy_score(y_true, preds)\n",
    "rec  = recall_score(y_true, preds, average='macro', zero_division=0)\n",
    "f1   = f1_score(y_true, preds, average='macro', zero_division=0)\n",
    "cm   = confusion_matrix(y_true, preds)\n",
    "\n",
    "def specificity_per_class(true, pred, label, num_classes):\n",
    "    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n",
    "    TN = cm.sum() - (cm[label, :].sum() + cm[:, label].sum() - cm[label, label])\n",
    "    FP = cm[:, label].sum() - cm[label, label]\n",
    "    return TN / (TN + FP + 1e-8)\n",
    "\n",
    "spec = np.mean([specificity_per_class(y_true, preds, i, len(LABEL_MAP)) for i in range(len(LABEL_MAP))])\n",
    "\n",
    "# === CETAK HASIL ===\n",
    "print(f\"\\n=== Evaluasi Model BERT Base (Best Model) ===\")\n",
    "print(f\"Akurasi     : {acc:.4f}\")\n",
    "print(f\"Recall      : {rec:.4f}\")\n",
    "print(f\"F1-score    : {f1:.4f}\")\n",
    "print(f\"Spesifisitas: {spec:.4f}\")\n",
    "\n",
    "# === CONFUSION MATRIX ===\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=LABEL_MAP.keys(),\n",
    "            yticklabels=LABEL_MAP.keys())\n",
    "plt.title(\"Confusion Matrix - BERT Base (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(MODEL_PATH, \"confmat_test.png\"))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84f9ef28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluasi Model BERT Base (Best Model) ===\n",
      "Akurasi     : 0.9542\n",
      "Recall      : 0.9542\n",
      "F1-score    : 0.9541\n",
      "Spesifisitas: 0.9771\n",
      "📂 Semua hasil disimpan ke: D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_AKHIR_RECORD\\RYTHM\\RHYTHM_EVALUASI_BERT\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === KONFIGURASI ===\n",
    "MODEL_PATH  = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_TRAIN\\RYTHM\\HASIL_Final_BERT_RYTHM\\HASIL_Final_BERT_RYTHM\\fold3\\model\"\n",
    "TEST_DIR    = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\SPLIT_SLIDING_FINAL\\SPLIT_SLIDING_FINAL\\test\"\n",
    "LABEL_MAP   = {'AFIB': 0, 'VFL': 1, 'VT': 2}\n",
    "IDX2LABEL   = {v: k for k, v in LABEL_MAP.items()}\n",
    "MAX_LEN     = 512\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === OUTPUT DIR ===\n",
    "OUTPUT_DIR = r\"D:\\KULIAH\\TELKOM_UNIVERSITY\\SEMESTER_8\\TA\\TA_SKRIPSI_GUE\\HASIL_AKHIR_RECORD\\RYTHM\\RHYTHM_EVALUASI_BERT\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# === LOAD TOKENIZER DAN MODEL ===\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === UTILS ===\n",
    "def signal_to_text(sig, target_len=512):\n",
    "    if len(sig) < target_len:\n",
    "        pad = np.full(target_len - len(sig), sig[-1])\n",
    "        sig = np.concatenate([sig, pad])\n",
    "    else:\n",
    "        idx = np.linspace(0, len(sig) - 1, target_len).astype(int)\n",
    "        sig = sig[idx]\n",
    "    norm = ((sig - sig.min()) / (sig.ptp() + 1e-8) * 255).astype(int)\n",
    "    return \" \".join(map(str, norm))\n",
    "\n",
    "def load_test_data(test_dir):\n",
    "    data, labels = [], []\n",
    "    for label_name in os.listdir(test_dir):\n",
    "        folder_path = os.path.join(test_dir, label_name)\n",
    "        if label_name not in LABEL_MAP:\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\".npy\"):\n",
    "                sig = np.load(os.path.join(folder_path, file), allow_pickle=True)\n",
    "                if isinstance(sig, np.ndarray) and sig.ndim == 1:\n",
    "                    data.append(signal_to_text(sig))\n",
    "                    labels.append(LABEL_MAP[label_name])\n",
    "    return data, np.array(labels)\n",
    "\n",
    "# === LOAD DATA TEST ===\n",
    "texts, y_true = load_test_data(TEST_DIR)\n",
    "\n",
    "# === INFERENSI ===\n",
    "preds = []\n",
    "batch_size = 32\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    batch_texts = texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, padding=\"max_length\", truncation=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings)\n",
    "        logits = outputs.logits\n",
    "        preds.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "\n",
    "# === EVALUASI GLOBAL ===\n",
    "acc  = accuracy_score(y_true, preds)\n",
    "rec  = recall_score(y_true, preds, average='macro', zero_division=0)\n",
    "f1   = f1_score(y_true, preds, average='macro', zero_division=0)\n",
    "cm   = confusion_matrix(y_true, preds)\n",
    "\n",
    "def specificity_per_class(true, pred, label, num_classes):\n",
    "    cm = confusion_matrix(true, pred, labels=list(range(num_classes)))\n",
    "    TN = cm.sum() - (cm[label, :].sum() + cm[:, label].sum() - cm[label, label])\n",
    "    FP = cm[:, label].sum() - cm[label, label]\n",
    "    return TN / (TN + FP + 1e-8)\n",
    "\n",
    "spec = np.mean([specificity_per_class(y_true, preds, i, len(LABEL_MAP)) for i in range(len(LABEL_MAP))])\n",
    "\n",
    "# === SIMPAN CONFUSION MATRIX PNG ===\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\",\n",
    "            xticklabels=LABEL_MAP.keys(),\n",
    "            yticklabels=LABEL_MAP.keys())\n",
    "plt.title(\"Confusion Matrix - BERT Base (Test Set)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confmat_test.png\"))\n",
    "plt.close()\n",
    "\n",
    "# === SIMPAN METRIK PER KELAS CSV ===\n",
    "rows = []\n",
    "for i, label in IDX2LABEL.items():\n",
    "    TP = cm[i,i]\n",
    "    FN = cm[i].sum() - TP\n",
    "    FP = cm[:,i].sum() - TP\n",
    "    TN = cm.sum() - TP - FN - FP\n",
    "    acc_cls  = (TP + TN) / cm.sum()\n",
    "    rec_cls  = TP / (TP + FN + 1e-8)\n",
    "    spec_cls = TN / (TN + FP + 1e-8)\n",
    "    f1_cls   = 2 * TP / (2 * TP + FP + FN + 1e-8)\n",
    "    rows.append({\n",
    "        \"kelas\": label,\n",
    "        \"akurasi\": round(acc_cls, 4),\n",
    "        \"recall\": round(rec_cls, 4),\n",
    "        \"f1\": round(f1_cls, 4),\n",
    "        \"spesifisitas\": round(spec_cls, 4)\n",
    "    })\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, \"summary_test_metrics.csv\"), index=False)\n",
    "\n",
    "# === CETAK HASIL RINGKAS DI TERMINAL ===\n",
    "print(f\"\\n=== Evaluasi Model BERT Base (Best Model) ===\")\n",
    "print(f\"Akurasi     : {acc:.4f}\")\n",
    "print(f\"Recall      : {rec:.4f}\")\n",
    "print(f\"F1-score    : {f1:.4f}\")\n",
    "print(f\"Spesifisitas: {spec:.4f}\")\n",
    "print(f\"📂 Semua hasil disimpan ke: {OUTPUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
